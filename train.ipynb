{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %pip install opencv-python numpy torchvision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KPbCuq5wWL38",
        "outputId": "6f5aad38-9292-4381-a9ed-d9f9cdeab69b"
      },
      "outputs": [],
      "source": [
        "from copy import deepcopy\n",
        "from time import time\n",
        "import os\n",
        "import traceback\n",
        "\n",
        "import paths\n",
        "import process_dataset.common as common"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SzbBwCKrpUQh",
        "outputId": "8f83fd26-9409-4ea9-e167-4a56efa7f260"
      },
      "outputs": [],
      "source": [
        "# %pip install -U openmim\n",
        "# !mim install \"mmcv>=2.0.0rc1,<2.1.0\"\n",
        "\n",
        "# !mim install \"mmdet>=3.0.0rc5,<3.1.0\"\n",
        "\n",
        "# We can install mmdet from repository, to which we can make changes\n",
        "# %cd $paths.mm_parent_dirpath\n",
        "# !git clone https://github.com/open-mmlab/mmdetection.git\n",
        "# %cd mmdetection\n",
        "# %pip install -e .\n",
        "# %cd $paths.proj_path\n",
        "\n",
        "# !mim install \"mmengine>=0.3.1\"\n",
        "\n",
        "# %cd $paths.mm_parent_dirpath\n",
        "# !git clone https://github.com/open-mmlab/mmengine\n",
        "# %cd mmengine\n",
        "# %pip install -e .\n",
        "# %cd $paths.proj_path\n",
        "\n",
        "# MMYOLO\n",
        "# %cd $paths.mm_parent_dirpath\n",
        "# !git clone https://github.com/open-mmlab/mmyolo\n",
        "# %cd mmyolo\n",
        "# %pip install -e .\n",
        "# %cd $paths.proj_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Additional paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hwfR63Ryv1y2",
        "outputId": "2565aac0-e2fd-4e65-e546-2273187453a3"
      },
      "outputs": [],
      "source": [
        "# Print paths.py file\n",
        "with open(\"./paths.py\") as f:\n",
        "    lines = f.readlines()\n",
        "    for line in lines.copy():\n",
        "        if line.startswith(\"#\"):\n",
        "            lines.remove(line)\n",
        "    print(\"\".join(lines).replace(\"\\n\\n\\n\", \"\\n\"))\n",
        "\n",
        "# Assert everything is in the right place\n",
        "assert os.path.exists(paths.proj_path)\n",
        "\n",
        "assert os.path.exists(paths.model_config_filepath)\n",
        "assert os.path.exists(paths.model_checkpoint_filepath)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "egmFK4r_CiHB"
      },
      "outputs": [],
      "source": [
        "# https://colab.research.google.com/github/ZwwWayne/mmdetection/blob/update-colab/demo/MMDet_Tutorial.ipynb#scrollTo=hamZrlnH-YDD\n",
        "# from mmcv import Config\n",
        "from mmengine.config import Config\n",
        "\n",
        "cfg = Config.fromfile(paths.model_config_filepath)\n",
        "\n",
        "if paths.last_checkpoint_filepath:\n",
        "    cfg.load_from = paths.last_checkpoint_filepath\n",
        "    cfg.resume = True\n",
        "else:\n",
        "    cfg.load_from = paths.model_checkpoint_filepath\n",
        "\n",
        "cfg.work_dir = paths.working_dirpath\n",
        "\n",
        "data_root = common.datasets_dirpath\n",
        "cfg.data_root = data_root\n",
        "\n",
        "# Set classes # TODO does this do something?\n",
        "cfg[\"metainfo\"] = dict(\n",
        "    classes = tuple(common.classes_ids.keys())\n",
        ")\n",
        "\n",
        "# Batch size (default 8)\n",
        "# On P52\n",
        "# batch_size = 24 # Cuda out of memory\n",
        "# batch_size = 20 # Cuda out of memory\n",
        "batch_size = 16\n",
        "\n",
        "# Workers per gpu (default 4)\n",
        "# Tested 8, 12 and 16 on P52 and higher numbers actually made the training (ETA) longer\n",
        "# With 12, ETA was about 10% longer than at default. Using 2, speed is slightly improved (~2%)\n",
        "# num_workers = 4, # Default\n",
        "# num_workers = 8, # Doesn't seem to do much\n",
        "num_workers = 1\n",
        "\n",
        "# TODO + some other augs from yolov8?\n",
        "train_pipeline = [\n",
        "    dict(type='LoadImageFromFile',\n",
        "        file_client_args=dict(backend='disk')),\n",
        "    dict(type='LoadAnnotations', with_bbox=True),\n",
        "    dict(type='mmdet.Resize',\n",
        "        scale=cfg.img_scale,\n",
        "        keep_ratio=True),\n",
        "    dict(type='mmdet.Pad',\n",
        "        pad_to_square=True,\n",
        "        pad_val=dict(img=(114.0, 114.0, 114.0))),\n",
        "    dict(type='YOLOv5RandomAffine',\n",
        "        # min_bbox_size=8, # No need. Done in FilterAnnotations\n",
        "        # border=(-cfg.img_scale[0] // 2, -cfg.img_scale[1] // 2), # This was a problem. No idea why I added it. Shouldn't exist\n",
        "        scaling_ratio_range=(0, 0), # Needs to be adjusted per dataset later below\n",
        "        max_rotate_degree=10,\n",
        "        max_shear_degree=5),\n",
        "    dict(type='YOLOv5HSVRandomAug'),\n",
        "    dict(type='mmdet.RandomFlip',\n",
        "         prob=0.5),\n",
        "    dict(type=\"mmdet.PhotoMetricDistortion\"),\n",
        "    dict(type='mmdet.FilterAnnotations',\n",
        "        # min_gt_bbox_wh=(8, 8), # Should be okay, I think 16x16 causes small objects (even 64x64) to be undetected\n",
        "        min_gt_bbox_wh=(1, 1), # But YOLOX originally just uses 1x1, so let's try\n",
        "        keep_empty=False),\n",
        "    dict(type='mmdet.PackDetInputs',\n",
        "        meta_keys=('img_id', 'img_path', 'ori_shape', 'img_shape', 'flip', 'flip_direction')),\n",
        "]\n",
        "\n",
        "train_datasets_scaling_ratios = {\n",
        "    \"mio-tcd\"     : (0.7, 1.1),\n",
        "    \"aau\"         : (0.8, 1.1),\n",
        "    \"ndis\"        : (0.9, 3),\n",
        "    \"mtid\"        : (0.9, 2),\n",
        "    \"visdrone_det\": (1.5, 3),\n",
        "    \"detrac\"      : (0.8, 1.2)\n",
        "}\n",
        "\n",
        "train_datasets_repeats = {\n",
        "    \"mio-tcd\"     : 1,\n",
        "    \"aau\"         : 3, # There are some misannotations so don't make it too frequent\n",
        "    \"ndis\"        : 25,\n",
        "    \"mtid\"        : 6, # It's a video, so already a lot repeats, but it's a great dataset\n",
        "    \"visdrone_det\": 4, # Good dataset, but not very important in this project\n",
        "    \"detrac\"      : 2\n",
        "}\n",
        "\n",
        "train_datasets = []\n",
        "for dataset_name in list(common.datasets.keys()):\n",
        "    ds = dict(\n",
        "        type = \"RepeatDataset\",\n",
        "        times = train_datasets_repeats[dataset_name],\n",
        "        dataset = dict(\n",
        "            type = \"YOLOv5CocoDataset\",\n",
        "            ann_file = os.path.join(common.datasets_dirpath, common.datasets[dataset_name][\"path\"], common.gt_filename),\n",
        "            data_prefix = dict(img=data_root),\n",
        "            data_root = data_root,\n",
        "            pipeline = deepcopy(train_pipeline),\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Set RandomAffine scaling range individually for each dataset\n",
        "    assert ds[\"dataset\"][\"pipeline\"][4][\"type\"] == \"YOLOv5RandomAffine\"\n",
        "    ds[\"dataset\"][\"pipeline\"][4][\"scaling_ratio_range\"] = train_datasets_scaling_ratios[dataset_name]\n",
        "\n",
        "    train_datasets.append(ds)\n",
        "\n",
        "cfg.train_dataloader = dict(\n",
        "    batch_size = batch_size,\n",
        "\n",
        "    num_workers = num_workers,\n",
        "\n",
        "    persistent_workers = True,\n",
        "\n",
        "    sampler=dict(type=\"DefaultSampler\", shuffle=True),\n",
        "\n",
        "    collate_fn=dict(type='yolov5_collate'),\n",
        "\n",
        "    # TODO restore this and use class balanced dataset (was getting an exception when used)\n",
        "    # \"The dataset needs to instantiate self.get_cat_ids() to support ClassBalancedDataset.\"\n",
        "    # So if I have ConcatDataset in ClassBalancedDataset, the ConcatDataset must have get_cat_ids()\n",
        "    # dataset = dict(\n",
        "    #     type = 'ClassBalancedDataset',\n",
        "    #     # oversample_thr = 1e-3, # Default\n",
        "    #     oversample_thr = 0.1, # Seems good\n",
        "    #     dataset = dict(\n",
        "    #         type = \"ConcatDataset\",\n",
        "    #         datasets = train_datasets\n",
        "    #     )\n",
        "    # ),\n",
        "\n",
        "    # This works (omitting class balanced dataset)\n",
        "    dataset = dict(\n",
        "        type = \"ConcatDataset\",\n",
        "        datasets = train_datasets\n",
        "    ),\n",
        ")\n",
        "\n",
        "cfg.val_dataloader = dict(\n",
        "    batch_size = batch_size,\n",
        "    num_workers = num_workers,\n",
        "    persistent_workers = True,\n",
        "    drop_last = False,\n",
        "    sampler = dict(type=\"DefaultSampler\", shuffle=False),\n",
        "    dataset = dict(\n",
        "        type = \"YOLOv5CocoDataset\",\n",
        "        data_root = data_root,\n",
        "        ann_file = os.path.basename(common.dataset_val_filepath),\n",
        "        data_prefix = dict(img=\"\"),\n",
        "        test_mode = True,\n",
        "        pipeline = cfg.val_dataloader.dataset.pipeline, # Default\n",
        "    )\n",
        ")\n",
        "\n",
        "cfg.test_dataloader = dict(\n",
        "    batch_size = batch_size,\n",
        "    num_workers = num_workers,\n",
        "    persistent_workers = True,\n",
        "    drop_last = False,\n",
        "    sampler = dict(type=\"DefaultSampler\", shuffle=False),\n",
        "    dataset = dict(\n",
        "        type = \"YOLOv5CocoDataset\",\n",
        "        data_root = data_root,\n",
        "        ann_file = os.path.basename(common.dataset_test_filepath),\n",
        "        data_prefix = dict(img=\"\"),\n",
        "        test_mode = True,\n",
        "        pipeline = cfg.test_dataloader.dataset.pipeline, # Default\n",
        "    )\n",
        ")\n",
        "\n",
        "cfg.val_evaluator.ann_file = common.dataset_val_filepath\n",
        "\n",
        "cfg.test_evaluator.ann_file = common.dataset_test_filepath\n",
        "\n",
        "cfg.seed = int(time())\n",
        "\n",
        "# The original learning rate (LR) is set for 8-GPU training.\n",
        "# We divide it by 8 since we only use one GPU.\n",
        "# cfg.optim_wrapper.optimizer.lr = 0.02 # This instead of 0.02 / 8 - nope, that's too much\n",
        "# cfg.optim_wrapper.optimizer.lr = 0.001 # Not better than 0.00125\n",
        "cfg.optim_wrapper.optimizer.lr = 0.00125 # As seen on the internet, seems to be good (0.01/8)\n",
        "\n",
        "cfg.optim_wrapper.optimizer.batch_size_per_gpu = batch_size\n",
        "\n",
        "# Set to log every Nth batch\n",
        "for hook_name in list(cfg.default_hooks.keys()):\n",
        "    if cfg.default_hooks[hook_name].type == \"LoggerHook\":\n",
        "        cfg.default_hooks[hook_name].interval = 50\n",
        "\n",
        "# We can set the checkpoint saving interval to reduce the storage cost\n",
        "cfg.default_hooks.checkpoint.interval = 1\n",
        "cfg.default_hooks.checkpoint.max_keep_ckpts = 10\n",
        "\n",
        "# TODO this has no effect:\n",
        "cfg.max_epochs = 20\n",
        "\n",
        "cfg.default_hooks.param_scheduler.max_epochs = cfg.max_epochs\n",
        "cfg.default_hooks.param_scheduler.warmup_epochs = 1\n",
        "\n",
        "cfg.train_cfg.max_epochs = cfg.max_epochs\n",
        "cfg.train_cfg.dynamic_intervals = [(cfg.max_epochs - 10, 1)]\n",
        "\n",
        "# We can also use tensorboard to log the training process\n",
        "cfg.visualizer.vis_backends = [\n",
        "    dict(type='LocalVisBackend'),\n",
        "    dict(type='TensorboardVisBackend')\n",
        "]\n",
        "\n",
        "# TODO validation sometimes?\n",
        "# cfg.workflow = [('train', 1), ('val', 1)]\n",
        "\n",
        "# Removing useless keys so they don't confuse\n",
        "cfg.pop(\"data_root\")\n",
        "cfg.pop(\"dataset_type\")\n",
        "cfg.pop(\"train_pipeline\")\n",
        "cfg.pop(\"test_pipeline\")\n",
        "cfg.pop(\"max_epochs\")\n",
        "cfg.pop(\"base_lr\")\n",
        "cfg.pop(\"vis_backends\")\n",
        "cfg.pop(\"custom_hooks\")\n",
        "\n",
        "# Set number of classes\n",
        "# cfg.model.bbox_head.num_classes = len(common.classes_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(cfg.pretty_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ZSc7wQWEdLp"
      },
      "outputs": [],
      "source": [
        "from mmengine.runner import Runner\n",
        "\n",
        "try:\n",
        "    runner = Runner.from_cfg(cfg)\n",
        "    runner.train()\n",
        "except:\n",
        "    traceback.print_exc()\n",
        "    raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from mmdet.apis import single_gpu_test\n",
        "# from mmdet.datasets import build_dataloader, build_dataset\n",
        "# from mmdet.utils import build_dp\n",
        "\n",
        "# data_loader = build_dataloader(build_dataset(cfg.data.test), samples_per_gpu=64, workers_per_gpu=1)\n",
        "# dp = build_dp(model, cfg.device, device_ids=cfg.gpu_ids)\n",
        "# outputs = single_gpu_test(dp, data_loader, out_dir=paths.working_dirpath)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "vscode": {
      "interpreter": {
        "hash": "229fedebb1d7394cf57a31acf727fae7ea6323c87170158d2b4890534347897d"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
