% For compilation piecewise (see projekt.tex), it is necessary to uncomment it and change
% \documentclass[../projekt.tex]{subfiles}
% \begin{document}

% TODO plagát dať do textu?

% TODO prvé výskyty skratiek a iných neznámych slov vysvetliť


\newpage
\chapter{Introduction}

Vehicle detection systems are an~important tool for enhancing road safety, and
with the rise of convolutional neural networks and the recent advancements in
object detection, these systems have become more affordable and accurate.
However, high-performance object detectors still require significant
computational resources and sacrifices must be made to perform this task in
real-time on embedded devices or those with limited computational resources.
This paper aims to evaluate the performance trade-offs of state-of-the-art
object detectors on low-performance devices and to explore various strategies
for improving their efficiency.

A~popular family of real-time object detectors is the YOLO (You Only Look Once)
series, with the latest, state-of-the-art YOLOv8 model architecture being the
primary focus of this paper. To thoroughly evaluate its performance, several
YOLOv8 models of different sizes are benchmarked, including one trained with the
lightweight MobileNetV2 backbone. These models are tested across six different
devices, including three NVIDIA Jetson embedded platforms. To accelerate the
process of training, deploying and testing different models, this paper utilized
PyTorch-based libraries from the OpenMMLab project, including MMYOLO and
MMDeploy.

The models were trained on several different datasets that consist of
surveillance-type images, most commonly captured by traffic monitoring cameras
mounted on existing infrastructure. The datasets combined contain a~total of
\num{1204795} object instances in \num{218821} images.

In addition to benchmarking the trained models on all devices in terms of their
inference speeds (FPS) and mean Average Precision (mAP) metrics, this paper also
aims to explore additional factors that can influence their performance,
including weight quantization and the usage of different inference backends,
inference batch sizes and input resolutions of the models.

In \autoref{BackgroundAndRelatedWork}, we begin by explaining essential
concepts, tools and libraries relevant to this study. We then proceed with
a~detailed review of the datasets employed in our research in
\autoref{DatasetsChapter}, followed by a~discussion on the evaluated object
detector models and their training process in \autoref{ModelsChapter}.

Subsequently, in \autoref{Experiments}, we delve into the specifics of our
experiments, outlining the methodologies employed, providing an~overview of the
devices used for testing and discussing the important aspects to consider when
interpreting the results. These results are then presented in detail in
\autoref{Results}, where we share our findings and insights gained from the
experiments. Finally, in \autoref{FutureWork}, we suggest potential areas for
future work that were beyond the scope of this paper.


\chapter{Background and Related Work}
\label{BackgroundAndRelatedWork}


In this chapter, an~overview of the key concepts and techniques in the field of
vehicle detection is provided, with a~focus on those relevant to the study at
hand. The discussion begins with traditional approaches to vehicle detection
before the introduction of Convolutional Neural Networks (CNNs). This is
followed by an~exploration of the problem of object detection using CNNs,
network optimization, and network compression techniques. The chapter also
covers evaluation metrics for assessing a~model's performance, a~review of
existing literature on this subject, an~overview of embedded platforms
designed for machine learning applications and finally, a~review of various
tools and libraries utilized in this paper.


\section{Traditional Approaches to Vehicle Detection}

Camera-based object detectors based on convolutional neural networks are
evaluated in this paper. However, to provide further context to the problem,
other commonly used solutions for the problem of vehicle detection are briefly
explained in this section: non-camera-based object detectors and
camera-based image processing techniques.


\subsection{Vehicle Detection Without a~Camera}

Although camera-based vehicle detection systems have gained significant
popularity due to advancements in computer vision and machine learning, there
are alternative techniques that do not rely on cameras for vehicle detection.
These methods offer different advantages, mainly reduced computational
requirements or improved performance in certain environmental conditions. Since
each detector has its own limitations, a~vehicle detection system typically
consists of different types of detectors to overcome these limitations.
Following is a~summary of non-camera-based techniques widely used for vehicle
detection tasks.


\subsubsection*{Magnetic Sensors}

One of the simplest solutions to detect vehicles is to use an~induction
loop~\cite{MagneticSensors}. The sensor, composed of a~single wire, can be buried
in concrete to detect the presence of metal objects passing by. With
a~controller, induction loops typically provide data about vehicle presence, but
using more advanced algorithms, vehicle speed, length and much more
can be determined using this simple sensor. Although the design is very simple,
installation is not and induction loops can even get damaged over time, while
repairs call for temporary shutdowns of roads. It is worth noting that there
are many other types of magnetic sensors, which can provide more detailed and
accurate data with simpler sensor installation, but the idea stays the same.


\subsubsection*{Ultrasonic Sensors}

Another type of inexpensive and simple detector is the ultrasonic
sensor~\cite{UltrasonicSensors}. These sensors can operate in a~variety of
conditions and are typically mounted on existing infrastructure. These distance
measurement sensors are usually used to detect the presence and distance of nearby
vehicles and their speed, but they can be utilized in many different ways to
even provide a~simple shape of a~vehicle to classify it.


\subsubsection*{Radars and Lidars}

Radars (Radio Detection and Ranging), which can also be mounted on existing
infrastructure above ground, work similarly to ultrasonic sensors, but a~single
radar can oversee a~much wider area of a~road~\cite{RadarSensors}. They are
usually used to detect the presence, speed, heading and shape of a~vehicle. The
shape can then be used to predict the class of the vehicle. Lidars (Light
Detection and Ranging) can be used in a~similar way, but are far more accurate,
which makes them better at detecting shapes and locations of objects.



\subsection{Vehicle Detection Based on Image Processing}

Image processing techniques take a~camera feed as input and in addition to
detecting the presence, speed, heading and shape of a~vehicle, they can also
provide its color, license plate and countless other characteristics, limited
mostly by the software, not by the detector. These camera-based systems, which
are relatively inexpensive, can be mounted on existing infrastructure, do not
emit any energy and are highly versatile, while providing a~large amount of
data.

An~algorithm applies a~series of pre-defined filters and transformations to
an~image to extract patterns and features that resemble vehicles. Although
modern convolutional neural networks (which will be explained in the following
\autoref{CNNs}) are generally more effective at recognizing more complex
patterns, image processing can still be extremely helpful for simpler tasks or
as a~component of a~larger vehicle detection system.

A~huge amount of research has been conducted on using image processing to solve
the problem of vehicle detection~\cite{ImageProcessingOverview}. Many of the
techniques proposed can operate in real-time and are very reliable in typical
environmental conditions. However, they can be sensitive to changes in
illumination or have their performance affected by rain, snow, shadows,
occlusions, or noise. While there are methods for addressing some of these
challenges, such as shadow removal techniques~\cite{ShadowRemoval}, they add to
the computational requirements of the system. Overall, our research suggests
that the image processing approach is well-suited for simpler tasks or systems
with lower accuracy requirements, but it is often difficult to implement and may
lack versatility in more complex or demanding scenarios.


\section{Convolutional Neural Networks}
\label{CNNs}

Convolutional neural networks (CNNs) are a~class of deep learning models that
have gained widespread popularity in recent years, mainly thanks to their
ability to learn hierarchical features from raw image data~\cite{Li2022}. CNNs
are particularly useful in the field of computer vision for various tasks,
including image classification, object detection and segmentation. In the
context of vehicle detection, these models have demonstrated a~superior
combination of accuracy, efficiency and flexibility compared to traditional
detection methods explained earlier. For example, compared to object detection
based on image processing, the main advantage of CNNs is that they are able to
learn automatically from raw image data instead of relying on pre-defined
filters and transformations.


\subsection{Architecture of Convolutional Neural Networks}

A~typical CNN consists of several key components, including convolutional
layers, pooling layers and fully connected layers. In this subsection, details
of these key components and their roles in the context of object detection are
explained. Please note that a~modern convolutional neural network consists of
more building blocks which will not be discussed here. A~CNN with a~simple
architecture is illustrated in \autoref{CNNFig}.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.75\textwidth]{cnn.pdf}
    \captionsetup{width=0.75\textwidth}
    \caption{An~example architecture of a~CNN for hand-written digit
    recognition~\cite{OShea2015}. ReLu (Rectified Linear Unit) is a~commonly
    used activation function in neural networks, but won't be discussed here in
    detail.}
    \label{CNNFig}
\end{figure}


\subsubsection{Convolutional Layers}

Convolutional layers form the backbone of a~CNN and perform 2D convolution
operations on the input data using trainable kernels to detect specific patterns
or features~\cite{OShea2015}. A~kernel, also called a~filter, is responsible
for detecting a~particular feature, such as an~edge or a~texture. It is
practically a~matrix, usually small in spatial dimensionality and its
parameters (also called weights) are adjusted during the training phase.

A~two-dimensional convolution is a~mathematical operation that involves
computing element-wise multiplications of the convolution kernel and the
corresponding sub-region of the input image. This process is typically repeated
throughout the entire image in a~sliding manner, resulting in a~new image
(matrix) as an~output called a~feature map. It can be explained visually by
\autoref{Convolution}.

Many filters (often of different types) are used in a~typical CNN and together
produce a~set of feature maps that capture countless different aspects of the
input image.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.6\textwidth]{convolution.pdf}
    \captionsetup{width=0.6\textwidth}
    \caption{Computation of a~2D convolution on a~small input image with
    an~example $2 \times 2$ kernel~\cite{zhang2021dive}. The output is also
    called a~feature map.}
    \label{Convolution}
\end{figure}


\subsubsection{Pooling Layers}

The pooling layers serve to gradually reduce the spatial dimensions of feature
maps output from the convolutional layers while retaining as much information as
possible ~\cite{OShea2015}. These layers improve the computational efficiency of
the CNN by reducing the number of its parameters. Generally, the pooling
operation takes a~window or a~filter and moves it over the input feature map in
strides, most commonly taking the maximum value of the inputs within the window.
This specific operation is called max-pooling.


\subsubsection{Fully-Connected Layers}

A~fully-connected layer, also known as a~dense layer is a~type in which each
neuron in the next layer is connected to each neuron in the previous
one~\cite{OShea2015}. In a~CNN, it is typically used at the end of the network
to classify the input data (features extracted by the previous convolutional
layers).


\subsection{Popular Architectures of Convolutional Neural Networks}

Over the years, various CNN architectures have been developed to address
different challenges and requirements. This subsection contains a review of several
influential architectures which achieved state-of-the-art\footnote{The term
state-of-the-art refers to methods that have achieved superior results
compared to previous best methods in a~specific task or application.}
performance in a~wide range of tasks, including image classification, object
detection or semantic segmentation.


\subsubsection{LeNet-5}

LeNet-5~\cite{Lecun1998}, introduced by Yann LeCun and his team in 1998 is
considered one of the first successful applications of convolutional neural
networks. Designed for hand-written digit recognition, LeNet-5 was the source of
inspiration for modern CNNs with its combination of convolutional, pooling and
fully connected layers. Its simple architecture is illustrated in
\autoref{LenetFig}.

\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{lenet.pdf}
    \captionsetup{width=\textwidth}
    \caption{Architecture of the LeNet-5 convolutional neural network designed
    for the recognition of hand-written digits. In this figure, the planes
    represent feature maps.~\cite{Lecun1998}}
    \label{LenetFig}
\end{figure}


\subsubsection{AlexNet}

Developed by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton in 2012,
AlexNet~\cite{NIPS2012} marked a~significant breakthrough in the field of deep
learning and won the ImageNet Large Scale Visual Recognition Challenge (ILSVRC)
by a~considerable margin. It popularized the use of deep CNNs for image
classification and featured the use of Rectified Linear Units (ReLU) as
activation functions, dropout technique for regularization and data
augmentations for training.


\subsubsection{VGGNet}

VGGNet~\cite{Simonyan2014}, proposed by Karen Simonyan and Andrew Zisserman in
2014, is best known for its uniform architecture, consisting of a~series of
stacked convolutional layers with small ($3 \times 3$) filters followed by
max-pooling.  VGGNet demonstrated that deeper networks generally achieve better
performance, achieving top results in the ILSVRC with its 16-layer and 19-layer
variants (VGG-16 and VGG-19). However, its success mostly lies in being very
computationally expensive.


\subsubsection{ResNet}

In 2015, ResNet~\cite{He2015} (Residual Network) was introduced by Kaiming He
and his team, addressing the degradation problem that occurs in very deep CNNs.
ResNet incorporates residual connections, which allowed ResNet to scale up to
hundreds of layers while improving performance, which was thought impossible.
ResNet achieved state-of-the-art results in various computer vision tasks and
has inspired many subsequent architectures.


\subsubsection{MobileNet}

MobileNet~\cite{Howard2017}, developed by Andrew G. Howard and his team at
Google in 2017, is a~popular lightweight convolutional neural network
architecture that has been widely used on mobile and embedded devices with
limited computational resources. One of its key features is its use of depthwise
separable convolutions. Traditional convolutional layers perform a~full
convolution on the input, but depthwise separable ones perform a~depthwise
convolution followed by a~pointwise convolution, which reduces the number of
computations required while maintaining accuracy. Another advantage is the
model's small size and low computational requirements compared to previously
discussed networks allowing for real-time use, of course by sacrificing some
accuracy.


\subsubsection{MobileNetV2}

MobileNetV2~\cite{Sandler2018}, an~evolution of the original MobileNet, was
proposed by Mark Sandler and his team at Google in 2018. The architecture
introduces the concept of \textit{Inverted Residuals} and \textit{Linear
Bottlenecks}, aiming to enhance the network's representational capacity while
maintaining low computational complexity. \textit{Inverted Residual} blocks
leverage the idea of residual connections to mitigate the vanishing gradient
problem and improve the training phase. \textit{Linear Bottlenecks}, on the
other hand, reduce the computational cost of convolutional layers by decreasing
the number of channels before applying a~convolutional operation and restoring
the original number of channels afterward. These advancements enable MobileNetV2
to achieve superior accuracy and efficiency compared to its predecessor.


\section{Object Detection Based on Convolutional Neural Networks}

While the CNNs previously discussed in \autoref{CNNs} can be used for image
classification\footnote{Image classification is the task of classifying an~image
into a~class category\,---\,for example recognizing whether an~image contains
a~cat or a~dog.}, the object detection task also involves
localization\,---\,marking all objects in the input image by a~bounding box.
However, these CNNs can, and often are used as backbones\footnote{The backbone
of an~object detector is responsible for extracting features from the input
image.} in object detectors to extract features from the input images. Modern
object detectors can be divided into two categories: two-stage detectors and
one-stage detectors.


\subsection{Two-Stage Object Detectors}

In 2013, Girshick \textit{et al.} proposed the R-CNN
framework~\cite{Girshick2013} (Regions with CNN features) which revolutionized
object detection by using a~two-stage approach. In the first stage, the
detection system takes the input image and selects region proposals (regions
that are likely to contain an~object). These proposals are then passed to the
second stage where features are extracted from each of the proposed regions,
which can then be classified. This algorithm marked a~breakthrough in object
detection, outperforming the current state-of-the-art detectors.

Although accurate, object detectors based on the R-CNN architecture (including
fast R-CNN and faster R-CNN) are generally computationally expensive compared to
one-stage object detectors and won't be discussed here in further detail.


\subsection{One-Stage Object Detectors}

When building a~real-time object detector, most commonly a~one-stage detector is
selected. Although their accuracy is generally lower than that of two-stage
detectors, their speed allows for real-time object detection even on
resource-constrained devices.


\subsubsection{Single Shot MultiBox Detector (SSD)}

The Single Shot MultiBox Detector (SSD) is an~object detection algorithm that
aims to provide a~balance between accuracy and computational efficiency.
Proposed by Wei Liu \textit{et~al.} in their 2015 paper~\cite{Liu2015}, SSD
addresses the challenge of detecting objects in images with varying scales and
aspect ratios in real time.

Unlike the two-stage object detectors which consist of a~region proposal stage
followed by a~classification stage, SSD directly predicts both the object
locations and their corresponding class labels in a~single forward pass of the
network.


\subsubsection{You Only Look Once (YOLO)}

YOLO (You Only Look Once) is a~family of one-stage object detectors that
prioritize real-time performance. Introduced by Joseph Redmon \textit{et al.} in
2015~\cite{Redmon2015}, YOLO divides the input image into a~grid, where each
grid cell is responsible for predicting bounding boxes and class probabilities
for objects located within that cell. The original YOLO algorithm has undergone
many iterations each improving upon the previous version's performance and
efficiency: from YOLOv2 to YOLOv8 and including YOLOX, YOLOR and several more.

This work is focused on the latest, state-of-the-art object detector
YOLOv8~\cite{YOLOv8} developed by Ultralytics, which, at this time, does not have
a~paper released. However, information about the detector can be drawn from
a~post by OpenMMLab~\cite{YOLOv8OpenMMLab}.

Built upon YOLOv5, the YOLOv8 detector updated the head\footnote{The head of
an~object detector predicts the object classes and bounding box coordinates
using features input by the neck.} module to be a~decoupled one, separating the
classification and detection heads, while the new backbone and neck\footnote{The
neck module of an~object detector refines features input from the backbone.}
modules are based on the YOLOv7 ELAN concept. In terms of training strategy,
YOLOv8 extends the training epochs from 300 to 500 and the data augmentation
process is modified during the final 10 epochs, with a~reduction in the
intensity of augmentations. Additionally, the loss function undergoes
a~revision, incorporating the TaskAlignedAssigner from TOOD and introducing
Distribution Focal Loss for regression loss, which further enhances the
detector's accuracy and efficiency.


\section{Network Optimization and Compression}

As CNNs have grown deeper and more complex to improve accuracy, the
computational and memory requirements have increased significantly. This makes
it difficult to deploy CNNs on embedded devices with limited resources. To
address this challenge, researchers have developed various model optimization
and compression techniques that aim to reduce the computational cost and memory
footprint while trying to maintain the model's accuracy. A~brief overview of the
most significant model optimization and compression techniques is provided in
this section. However, in this paper, only one of these techniques\,---\,weight
quantization\,---\,will be used.


\subsection{Network Pruning}

As explained by Tejalal Choudhary \textit{et al.} in their
paper~\cite{Choudhary2020}, there are typically many parameters in a~CNN which
are not utilized during the training phase and do not contribute to the
network's performance. The pruning optimization technique aims to
remove these redundant parameters from the model while maintaining accuracy. Of
course, more aggressive pruning can be performed, removing even more parameters,
including not just redundant, but also less important ones, although
sacrificing some accuracy.

Various pruning techniques have been developed, including weight pruning
(removal of individual weights), neuron pruning (removal of an~entire neuron and
its connections), filter pruning and layer pruning. Although this technique was
developed to reduce the model's storage requirements, it can also be used to
reduce its computational requirements.


\subsection{Knowledge Distillation}

Knowledge distillation is a~model compression technique in which a~smaller, more
compact student network is trained to mimic the behavior of a~larger,
high-performing teacher network (or an~ensemble of them) to learn the teacher
model's generalization capability~\cite{Gou2021}. The student network learns
from outputs from the teacher network instead of the ground truth labels. Of
course, practically, more advanced and effective knowledge distillation methods
are used, but will not be discussed in this paper. Generally, the student
network cannot achieve an accuracy as high as the teacher network, but when
performed correctly, it typically achieves higher accuracy than if trained the
conventional way.


\subsection{Weight Quantization}

In convolutional neural networks, weights and biases are stored as 32-bit
floating-point numbers (also called \texttt{FP32}), which provide precision often
unnecessary for the CNNs to be accurate. Quantization is the process of reducing
the number of bits used to represent these parameters, and generally decreases
the storage and computational requirements of the network~\cite{Choudhary2020}.

Although reducing the precision of the model's parameters decreases its
accuracy, the drop in accuracy is typically insignificant when compared to the
substantial benefits in storage and computational efficiency gained, which are
essential factors when developing a~real-time object detector.

Most commonly, the parameters of models are quantized to either a~16-bit
floating-point (\texttt{FP16}) representation or an~8-bit integer
(\texttt{INT8}) representation. Quantizing to \texttt{FP16} usually does not
require any post-quantization steps. However, quantizing to the integer
representation (also called precision in this context), such as \texttt{INT8},
is a~different process. Because of the limitations of the integer representation
and the distinction from floating-point representations, weight
calibration\footnote{Weight calibration during weight quantization refers to the
process of adjusting the quantized weights to minimize the loss of accuracy that
occurs due to the reduction in numerical precision.} during the quantization
process or even model fine-tuning\footnote{Model fine-tuning refers to further
training of a~model with a~lower learning rate to refine the model's parameters.}
after it is recommended to maintain the highest possible prediction accuracy.
This, of course, only applies to post-training quantization (PTQ) while several
other quantization techniques are available\,---\,mainly training the model with
weights in the desired representation from the beginning (quantization-aware
training, QAT). Furthermore, a~more advanced technique called
mixed-precision quantization (MPQ) can be used to quantize parameters in a~more
nuanced manner by assigning different precisions to individual parameters or
parameter groups, depending on their sensitivity to numerical
errors~\cite{Tang2022}.

Most deep learning libraries which can be used for model deployment or inference
on a~target device\,---\,including TensorRT and ONNX Runtime\,---\,support
weight quantization and offer tutorials on how it's done.


\section{Evaluation Metrics}
\label{EvaluationMetrics}

Evaluation metrics are crucial for assessing the performance of different object
detection models, enabling comparison between different architectures and
tracking improvements during training. This section provides a~brief
explanation and background of the mean Average Precision (mAP) metric, which
will be used to evaluate the detectors trained in this project. The information
in this section is derived from a~blog post on the V7Labs website~\cite{mAP}.


\subsection*{Precision and Recall}

Precision is a~measure of the proportion of true positive detections out of all
detections (true positives and false positives), while recall measures the
proportion of true positive detections out of all ground truth\footnote{Ground
truth refers to the actual, true data used as a~reference for comparison with
the model's predictions.} objects (true positives and false negatives) in the
dataset. In other words, precision represents how many predictions of the model are
correct and recall measures how many of the ground truth objects were predicted
by the model.

If the precision and recall values are computed at different confidence
score\footnote{Confidence score is a~number output from a~detector for each
detection stating the model's confidence that the detection is correct.}
thresholds, they can be plotted against each other to obtain
a~precision-recall curve, which can be used to visually evaluate the overall
accuracy of the model.


\subsection*{Average Precision}

The Average Precision (AP) is a~metric widely used to represent the performance
of an~object detector. It is simply calculated as the area under the
precision-recall curve and ranges from 0 to 1, where an AP of 1 indicates
perfect precision and recall at all thresholds.


\subsection*{Mean Average Precision}

To evaluate multi-class object detectors, mean Average Precision (mAP) is used
instead of the previously explained Average Precision. Computing the mAP
involves finding the AP for each class and calculating the average over the
number of classes.

However, for object detection tasks, precision is typically calculated with
different thresholds of the Intersection over Union (IoU). The IoU
measures the overlap between two bounding boxes\,---\,the model's
prediction and the ground truth bounding box\,---\,and represents the quality of the
alignment between the two boxes. Calculating the IoU simply means dividing the
intersection area of the two bounding boxes over their union. When calculating
precision, the IoU metric is used to determine whether a~predicted bounding box
should be considered a~true positive (IoU is higher than the defined IoU
threshold) or a~false positive (IoU is lower than the threshold). For
visualization, see \autoref{IoU}.

\begin{figure}[t]
    \centering
    \begin{subfigure}[b]{0.3125\textwidth}
        \includegraphics[width=\textwidth]{iou_1.pdf}
        \caption{$IoU = 0$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.2272727\textwidth}
        \includegraphics[width=\textwidth]{iou_2.pdf}
        \caption{$IoU \approx 0.333$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.15151515\textwidth}
        \includegraphics[width=\textwidth]{iou_3.pdf}
        \caption{$IoU = 1$}
    \end{subfigure}
    \caption{Visualization of the Intersection over Union calculation.}
    \label{IoU}
\end{figure}

In this paper, we consider the COCO mAP specification and calculate the mAP as
the average of AP calculated for all classes and over ten IoU thresholds ranging
from \num{0.50} to (and~including) \num{0.95} with a step of \num{0.05}.
Similarly, values $\text{mAP}^{\text{IoU:0.50}}$ and
$\text{mAP}^{\text{IoU:0.75}}$ denote mAP calculated with IoU thresholds equal
to \num{0.50} and \num{0.75} respectively. Additionally,
$\text{mAP}^{\text{small}}$ is only calculated for objects of area smaller than
$32^2$ pixels, $\text{mAP}^{\text{medium}}$ for objects larger than $32^2$ px
but smaller than $96^2$ px and finally, $\text{mAP}^{\text{large}}$ for objects
larger than $96^2$ px. The mAP values in this paper will be presented as
percentages.


\section{Vehicle Detection Based on Convolutional Neural Networks}

This section delves into the existing literature on vehicle detection using deep
learning and convolutional neural networks. We advise the reader to be cautious
when comparing their results because they vary based on the selected evaluation
metrics, datasets used, and devices used to evaluate the models.

Deep learning has revolutionized vehicle detection by significantly improving
accuracy and robustness. In a~study from 2019~\cite{Wang2019}, the authors
compare five mainstream deep learning-based object detection algorithms for
vehicle detection in autonomous driving: Faster R-CNN, R-FCN, SSD, RetinaNet
and YOLOv3. The results indicate that two-stage detectors generally have better
detection accuracy than one-stage models, while SSD and YOLOv3 algorithms were
found to have excellent real-time performance and generalization abilities. 

In a~more recent paper by Maity \textit{et al.}~\cite{Maity2021},
the authors present a~comprehensive review of existing Faster R-CNN and
YOLO-based vehicle detection and tracking methods, comparing them and
highlighting their advantages over traditional vehicle detection methods. The
authors also discuss several vehicle detection methods featuring modified YOLO
models to improve detection performance.

In their paper, Kai Han \textit{et al.} propose a Ghost module~\cite{Han2019},
which focuses on generating more feature maps utilizing computationally cheap
operations. Using this, in their 2021 paper~\cite{Wu2021}, the authors
propose YOLOv5-Ghost\,---\,an~improved neural network structure based on
YOLOv5-small\,---\,to detect vehicles in the CARLA\footnote{CARLA (Car Learning
to Act) is an~open-source, realistic urban environment simulator designed for
the development, testing and validation of autonomous driving systems in various
traffic scenarios and conditions.} virtual environment. By replacing
BottleneckCSP in the YOLOv5-small with Ghost Bottleneck, the detection speed is
increased from 29\,FPS (YOLOv5-small) to 47\,FPS (YOLOv5-Ghost) while only
reducing the mAP from \num{83.3}\,\% to \num{80.7}\,\% on their test dataset.
Similarly, in their 2022 paper~\cite{Xiong2022}, the authors propose a
lightweight architecture of the YOLOX object detector based on the Ghost module
for vehicle detection. They then compare its results to YOLOv5 and YOLOX to show
the superiority of the proposed model in both speed and accuracy.

Based on the literature reviewed, the main focus of this paper is placed on
object detectors of the YOLO family, with the understanding that these models
can be further optimized to achieve faster inference speeds.


\section{Embedded Platforms for Machine Learning}
\label{EmbeddedPlatforms}

Embedded platforms are a~combination of hardware and software components that
are designed to perform specific tasks. For object detection, or machine
learning in general, these systems are optimized to be power efficient while
providing significant processing capabilities for the development, deployment
and execution of machine learning algorithms. The focus of this section is on
discussing some of the most popular embedded devices and platforms for machine
learning, including object detection.

These embedded devices typically incorporate custom processors, microcontrollers
or specialized accelerators specifically engineered for an~efficient execution of
machine learning tasks, such as graphics processing units (GPUs, which can
perform many computations in parallel) or tensor processing units (TPUs).


\subsection{Google Coral}

Designed for TensorFlow Lite\footnote{TensorFlow Lite is a~lightweight machine
learning framework designed for running TensorFlow models efficiently on devices
with limited computational resources.} models, the Google Coral platform offers
an~Edge TPU\,---\,a low-power, high-performance ASIC (application-specific
integrated circuit) enabling on-device machine learning
inference\footnote{Inference in machine learning is the process of using
a~trained model to make predictions on new data, previously unseen by the model.}.
Coral provides development boards and USB accelerators along with a~variety of
modules and peripherals for edge AI applications.


\subsection{Movidius Neural Compute Stick}

Intel's Movidius Neural Compute Stick, commonly paired with the popular
single-board computer Raspberry Pi, is a~small, low-power USB-based hardware
accelerator featuring a~vision processing unit (VPU) designed to accelerate
neural network computations.


\subsection{NVIDIA Jetson}
\label{Jetsons}

NVIDIA Jetson is a~series of widely-used embedded computing platforms that
feature powerful GPU accelerators and ARM-based CPUs while being
energy-efficient. In this paper, three NVIDIA Jetson devices will be evaluated.
Following is a~brief overview of the hardware and software for these platforms.


\subsubsection{NVIDIA Jetson AGX Xavier}

Jetson AGX Xavier is the flagship model in the NVIDIA Jetson family. It is
a~high-performance, energy-efficient platform designed for more demanding AI
workloads. With an~integrated NVIDIA Volta GPU with 512 CUDA cores and 64 Tensor
cores, an~8-core NVIDIA Carmel ARM CPU and 16\,GB of memory, it offers substantial
computational capabilities for deploying state-of-the-art real-time object
detectors. Its typical power consumption ranges from 20\,W to 30\,W.


\subsubsection{NVIDIA Jetson Xavier NX}

The NVIDIA Jetson Xavier NX features an~NVIDIA Volta GPU with 384 CUDA cores and
48 Tensor cores, a~6-core NVIDIA Carmel ARM CPU and 8\,GB of memory. Compared
to Jetson AGX Xavier, it is more compact and power-efficient, with the typical
consumption of 15\,W, while of course offering lower performance and memory
capacity.


\subsubsection{NVIDIA Jetson Nano}

The smallest and most popular module from the NVIDIA Jetson family is the Jetson
Nano. Equipped with a~128-core NVIDIA Maxwell GPU, a~quad-core ARM Cortex-A57
CPU and just 4\,GB of memory, it serves as an~entry-level, low-cost AI embedded
platform, offering lower performance but lower power consumption (ranging from 5
to 10\,W) compared to the previously mentioned Jetson boards.


\subsubsection{Software for NVIDIA Jetson devices}

Devices of the NVIDIA Jetson family support the Linux for Tegra (L4T) operating
system, a~customized Linux distribution designed specifically for the platform's
unique capabilities. NVIDIA also offers the JetPack SDK, which includes the
CUDA toolkit and the cuDNN library accelerating deep learning tasks on NVIDIA
GPUs, the TensorRT library used for optimizing deep learning models to achieve
higher inference speeds on NVIDIA GPUs, along with various multimedia and
computer vision libraries.

On all the NVIDIA Jetson platforms utilized in this paper, JetPack SDK version
\texttt{4.6.3} was used. It is important to note that using this version of the
JetPack SDK means using older software versions, including Python version
\texttt{3.6}, TensorRT version \texttt{8.2.1} and CUDA version \texttt{10.2}.
The reason for not using a~more recent JetPack SDK, despite its support for the
Jetson AGX Xavier and Jetson Xavier NX, as well as the complications it caused,
will be discussed in \autoref{Experiments}.


\section{Tools and Libraries}

In this section, we provide a~brief overview of some crucial tools and
libraries utilized throughout the project.


\subsection{LabelBox}

LabelBox~\cite{LabelBox} is a~web-based application and data labeling platform
widely used when training machine models. It offers an~intuitive interface and
a~suite of tools for efficient management and annotations of datasets. With the
option of importing data, the application was used to reannotate one of the
datasets.


\subsection{PyTorch}

PyTorch~\cite{PyTorch} is a~popular open-source, Python-based machine learning
library designed to provide flexibility, ease of use, and high performance for
deep learning applications. It offers a~rich ecosystem of tools and libraries for
various tasks, including computer vision. With support for GPU acceleration
using NVIDIA's CUDA platform, PyTorch enables fast and efficient computation of
large models, making it an~ideal choice for high-performance deep learning
tasks.


\subsection{MMDetection}

MMDetection~\cite{MMDetection} is a~popular open-source deep learning toolbox
for computer vision tasks, including object detection. Developed by the
Multimedia Laboratory at the Chinese University of Hong Kong (OpenMMLab),
MMDetection provides a~flexible, extensible and modular framework that aims to
simplify the process of training and deploying state-of-the-art models for
various computer vision tasks, and provides a~rich set of tools. Some of the key
features of the library include:

\begin{itemize}
    \item It is based on the PyTorch machine learning library.
    \item It builds upon and uses other open-source libraries from the OpenMMLab
    project, such as MMCV\footnote{MMCV is a~library for computer vision
    research including building blocks for convolutional neural networks, tools
    for image processing, transformations and much more.} and
    MMEngine\footnote{MMEngine library serves as the training engine for all
    OpenMMLab codebases, supporting hundreds of algorithms frequently used in
    deep learning.}.
    \item Its modular design allows for easy customization and extension
    of the codebase.
    \item It includes pre-trained models and their configurations, making it
    possible to use the transfer learning technique\footnote{Transfer learning
    is a~machine learning technique that leverages knowledge of a~pre-trained
    model to more efficiently train a~new model, reducing training time and data
    requirements.} for simpler and faster training and comparison of different
    models.
\end{itemize}


\subsection{MMYOLO}

Although the MMDetection library does not include the latest detectors of the
YOLO family, such as the YOLOv8 detector which will be used in this project, the
OpenMMLab project features a~different library called
MMYOLO~\cite{MMYOLO}\,---\,an~extension of the MMDetection library\,---\,which
addresses this issue and focuses solely on detectors of the YOLO family. It
contains implementations of YOLO-specific components, such as the CSPDarknet and
PANet backbone networks, and YOLO-specific training techniques, including data
augmentations or loss functions.


\subsection{MMDeploy}

The MMDeploy library~\cite{MMDeploy}, which is also a~part of the OpenMMLab project,
offers useful tools for deploying OpenMMLab models to a~wide range of platforms
and devices. It enables the conversion of PyTorch models trained with
MMDetection or MMYOLO into backend models for execution on target devices.
MMDeploy supports various backends, including ONNX, TensorRT, OpenVino,
TorchScript and numerous others. In addition to streamlining the deployment
process, the library also optimizes the converted models for their target
platforms.


\subsection{ONNX and ONNXRuntime}

ONNX~\cite{ONNX} (Open Neural Network Exchange) is an~open-source project aimed
at creating a~consistent format for deep learning models. It was initially
developed by Facebook and Microsoft, but several other companies and
organizations joined later on. The primary goal of ONNX is to enable developers
and researchers to easily switch between different machine learning frameworks
without having to worry about model compatibility. ONNX defines a~standard
representation for neural network models, making it possible to train a~model in
one framework and use it for inference in another.

Additionally, ONNX provides a~set of tools and libraries, such as ONNX
Runtime~\cite{ONNXRuntime}, which is a~high-performance inference engine for
ONNX models.


\subsection{TensorRT}

TensorRT\footnote{The TensorRT library is available at
\url{https://developer.nvidia.com/tensorrt}} is a~high-performance deep learning
inference optimization and runtime library developed by NVIDIA. It is designed
to accelerate the deployment and inference of models on NVIDIA GPUs for various
applications including computer vision.

In addition to optimizing the target models for improved inference
performance and reduced memory footprint, TensorRT also supports multiple
precision modes, including \texttt{FP32}, \texttt{FP16} and \texttt{INT8},
allowing developers to choose the best balance between accuracy and performance.




\chapter{Datasets}
\label{DatasetsChapter}

In this chapter, each of the used datasets is first thoroughly analyzed and at
the end, a~summary of all used datasets is provided along with details on how
the processed dataset was split into training, validation and testing
subsets\footnote{Developing an~object detector includes training it on the
training dataset while regularly evaluating its progress by testing it on the
validation dataset. Finally, the trained model is evaluated on the testing
dataset to provide an~independent metric.}. The main focus when selecting
appropriate datasets was on the camera's point of view (this paper aims to
develop a~vehicle detector for inputs of a~surveillance type) and the categories
to which the vehicles are classified. In this paper, eight vehicle classes are
considered:
\begin{itemize}
    \item Bicycle
    \item Motorcycle (any two-wheeled motorized vehicle)
    \item Passenger Car
    \item Transporter (including a~pick-up truck)
    \item Bus (including a~minibus)
    \item Truck
    \item Trailer
    \item Unknown
\end{itemize}


\section{Analysis of Individual Datasets}

This section individually analyzes all datasets used in this work, discussing
their sizes, origins, advantages and drawbacks and also provide an~example image
from each of the datasets.


\subsection{UA-DETRAC}
\label{DetracDataset}

The UA-DETRAC dataset~\cite{detrac} (hereafter referred to as DETRAC) provided
by the University at Albany is the biggest and the most important dataset for
this work, originally containing \num{1274055} annotations of \num{8250}
vehicles in \num{138252} images. The dataset is provided as frames of 100 video
sequences at 25\,FPS with resolutions of $960 \times 540$ pixels. The length of
these sequences varies but is usually between 30 seconds and 90 seconds. The
video is of a~surveillance type and almost all sequences have a~unique point of
view, usually from a~bridge. Many sequences were recorded in rain or at night
and there is no lens flare from cars' headlights. One of the images from this
dataset is shown in \autoref{DetracDatasetExample}.

\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{dataset_example_detrac.jpg}
    \caption{Example image from the DETRAC dataset.}
    \label{DetracDatasetExample}
\end{figure}

However, there are several issues with this dataset:
\begin{itemize}
    \item Bicycles, motorcycles and a~few vehicles that cannot be classified are
    not annotated at all.
    \item Bounding boxes are often loose and do not fit tightly to the objects.
    \item Vehicles near the edge of the frame, although fully visible, sometimes
    lack labels.
    \item In several sequences, vehicles are tracked and annotated even on
    frames on which they are fully occluded (by other vehicles or
    infrastructure).
    \item Vehicle annotations are inconsistent in relation to masks, as some are
    labeled even when masked, while others are left unlabeled even when already
    fully visible outside of a~mask. This is likely due to the camera being
    hand-held and therefore moving while the masks are static throughout the
    individual sequences.
\end{itemize}

These problems were fixed by importing the dataset to the LabelBox labeling
application, adjusting the masks (while also making them dynamic to compensate
for camera movements), annotating or masking bicycles, motorcycles and
``unknown'' vehicles and finally, carefully repairing individual annotations if
needed. Many sequences were hectic or were annotated so poorly that reannotating
would be too time-consuming, so only 71 of 100 sequences were reannotated.

The reannotated dataset contains \num{733833} annotations in \num{99771}
images and the following classes (the dataset contains no trailers):
\begin{itemize}
    \item Bicycle
    \item Motorcycle
    \item Car
    \item Bus
    \item Van (mapped to \textit{transporter})
    \item Others (mapped to \textit{truck})
    \item Unknown
\end{itemize}


\subsection{Miovision Traffic Camera Dataset}

The Miovision Traffic Camera Dataset~\cite{MIO2018} (hereafter referred to
as Miovision dataset) is another large and very important dataset. The images
were taken at different times of day by thousands of traffic cameras in Canada
and the United States. Roughly $79\,\%$ of all images are of resolution $720
\times 480$ pixels, while the rest is of
resolution $342 \times 228$ pixels. Example images of both resolutions are shown
in \autoref{MIOTCDDatasetExample}.

\begin{figure}[t]
    \centering
    \begin{subfigure}[b]{0.475\textwidth}
        \includegraphics[width=\textwidth]{dataset_example_mio-tcd.jpg}
        \caption{Resolution $720 \times 480$}
    \end{subfigure}
    \begin{subfigure}[b]{0.475\textwidth}
        \includegraphics[width=\textwidth]{dataset_example_mio-tcd_small.jpg}
        \caption{Resolution $342 \times 228$}
    \end{subfigure}
    \caption{Example images from the Miovision dataset.}
    \label{MIOTCDDatasetExample}
\end{figure}

The dataset consists of two parts: \textit{Classification dataset} and
\textit{Localization dataset}. Only the \textit{Train} subset of the
\textit{Classification} part is used here because the \textit{Test} subset is
not annotated. The part used contains objects of the following classes:
\begin{itemize}
    \item Pedestrian (ignored)
    \item Bicycle
    \item Motorcycle
    \item Car
    \item Pick-up truck (mapped to \textit{transporter})
    \item Work van (mapped to \textit{transporter})
    \item Bus
    \item Articulated truck (mapped to \textit{truck})
    \item Single unit truck (mapped to \textit{truck})
    \item Non-motorized vehicle (mapped to \textit{trailer})
    \item Motorized vehicle (mapped to \textit{unknown})
\end{itemize}
The processed dataset (without pedestrian annotations) contains \num{344416}
objects in \num{110000} images.


\subsection{AAU RainSnow Traffic Surveillance Dataset}
\label{AAUDataset}

Another important dataset is the AAU RainSnow Traffic Surveillance
dataset~\cite{Bahnsen2019} (hereafter referred to simply as AAU). The authors
mounted two synchronized (one RGB and one thermal) cameras on street lamps at
seven different Danish intersections to take 5-minute long videos at different
lighting and weather conditions\,---\,night and day, rain and snow. They then
extracted \num{2200} frames from the videos and annotated them on a~pixel-level.
Several different types of masks were also created and included in the dataset.

In this paper, only the annotated frames from the RGB camera are used,
containing \num{13297} annotations in \num{2200} frames (before processing) of
resolution $640 \times 480$ pixels. See an~example in
\autoref{AAUDatasetExample}.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.7\textwidth]{dataset_example_aau.png}
    \captionsetup{width=0.7\textwidth}
    \caption{Example image from the AAU dataset.}
    \label{AAUDatasetExample}
\end{figure}

The dataset uses these six classes:
\begin{itemize}
    \item Pedestrian
    \item Bicycle
    \item Motorbike
    \item Car
    \item Bus
    \item Truck
\end{itemize}
This introduces a~problem\,---\,vehicles of the internal class \textit{transporter}
(van) don't have their own class in the dataset, but are classified as trucks.
However, the dataset is small and it is impossible to perfectly divide
transporters and trucks into two classes as there are many different models
between which a~line cannot be drawn.  Ignoring this issue should therefore not
cause any problems.

Several other minor problems were found when processing this dataset:
\begin{itemize}
    \item Frames in groups \verb|Egensevej-1|, \verb|Egensevej-3| and
    \verb|Egensevej-5| are hardly usable because of the low-quality camera and
    challenging weather and lighting conditions, so they were dismissed.
    \item Some frames had bounding boxes over the whole frame\,---\,this was most
    certainly an~annotation error. These labels were ignored as well.
    \item The mask for \verb|Hadsundvej| intersection didn't fully cover the
    area that should be ignored. This was fixed by simply editing the mask.
\end{itemize}

After processing, the dataset contains \num{10545} objects in \num{1899} images.


\subsection{Multi-View Traffic Intersection Dataset (MTID)}

For the Multi-View Traffic Intersection Dataset~\cite{Jensen2020} (hereafter referred to as MTID), the
authors recorded a~single intersection from two points of view at 30\,FPS\,---\,one
camera was mounted on existing infrastructure and one was attached to a~hovering
drone. The dataset contains \num{65299} annotated objects in \num{5776} frames
(equal share of frames for both cameras). An~example from this dataset can be
seen in \autoref{MTIDDatasetExample}.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.8\textwidth]{dataset_example_mtid.jpg}
    \captionsetup{width=0.8\textwidth}
    \caption{Example image from the drone subset of the MTID dataset.}
    \label{MTIDDatasetExample}
\end{figure}

All annotated objects fall into one of four classes:
\begin{itemize}
    \item Bicycle
    \item Car
    \item Bus
    \item Lorry (mapped to \textit{truck})
\end{itemize}
This, at first, might not seem like enough, but a~closer inspection of the
annotated frames reveals that there are no pedestrians or motorcycles. However,
similarly to the AAU dataset, there are transporters in the frames
that are classified as trucks. Again, this issue is simply ignored.

When processing this dataset, two other problems were encountered:
\begin{itemize}
    \item Vehicles that are not on the road are not annotated, so they have to
    be masked out. This is not as easy for the drone video because the camera is
    moving, but it is still simple enough.
    \item Many frames of the drone footage lack some or all labels and have to
    be ignored. Ranges of image numbers that are ignored when processing the
    dataset: $[1,31]$, $[659,659]$, $[1001,1318]$ and $[3301,3327]$.
\end{itemize}
The processed dataset contains \num{64979} objects in \num{5399} frames.


\subsection{Night and Day Instance Segmented Park Dataset (NDISPark)}

Another useful dataset is the Night and Day Instance Segmented Park
dataset~\cite{Luca2022} (hereafter referred to as NDISPark), which contains
images of parked vehicles taken by a~camera mounted on infrastructure. Although
the annotated part of the dataset only contains \num{142} frames after
processing, there are \num{3302} objects annotated in total. This still makes it
a~small dataset, but it provides images of vehicles from many different points of
view and also contains many occluded vehicles. See \autoref{NDISDatasetExample}
for an~example image from this dataset. Additionally, all frames are \num{2400}
px in width and within \num{908} px and \num{1808} px in height.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.7\textwidth]{dataset_example_ndis.jpg}
    \captionsetup{width=0.7\textwidth}
    \caption{Example image from the NDISPark dataset.}
    \label{NDISDatasetExample}
\end{figure}

This dataset does not contain any classifications, but luckily, it only contains
cars, transporters and a~few unattached car trailers. All transporters and
trailers were manually classified.


\subsection{VisDrone Dataset}

The VisDrone dataset~\cite{Zhu2022} is very different from all the previous
datasets since it does not just contain traffic surveillance images. Images are
taken by a~camera mounted on a~drone, from many different points of view. In the
test subset of the object detection part of this dataset after processing, there
are \num{47720} annotated objects in \num{1610} frames, which will be used in
this paper. An~example is displayed in \autoref{VisDroneDatasetExample}.

\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{dataset_example_visdrone.jpg}
    \caption{Example image from the VisDrone dataset.}
    \label{VisDroneDatasetExample}
\end{figure}

We consider it to be a~helpful addition to our datasets as it contains useful
negative images, many pedestrians and new points of view, and it often captures
vehicles from a~bird's-eye view. Additionally, objects in this dataset are
classified into 12 classes, which can be easily mapped to the 8
internal classes.


\section{Dataset Processing}

Before training, all datasets used in this project must be converted to
a~unified format and vehicle classes need to be mapped to be the same in each dataset.
Additionally, some datasets include images with regions that should be masked
out, and certain subsets or images of some datasets need to be ignored.

Therefore, a~Python script was developed for each dataset. It first loads the
annotations from the original format\,---\,COCO\footnote{The widely used COCO
annotation format defines how a~dataset (its categories, list of images,
annotations and other metadata) should be stored in the JSON (JavaScript Object
Notation) format.} for AAU, MTID
and NDISPark, XML for DETRAC and different CSV formats for Miovision and VisDrone datasets.
The script then maps the classes to the ones shown earlier in this chapter and if
needed, removes the ignored subsets or images before saving the labels in the
COCO format. The processing script for NDISPark dataset also corrects the object
classes (since the original dataset annotates all objects with a~single class)
before saving, and therefore does not modify the original dataset file (class
corrections are defined in the script itself).

MMDetection's middle format was considered as it is a~more efficient alternative
to the COCO format, but the COCO format is more popular and is supported by most
relevant applications, while also being human-readable (the MMDetection's middle
format is saved as a~pickle\footnote{The Python's pickle format refers to
a~binary serialization and deserialization method allowing for the conversion of
Python objects into a~byte stream.} file), and most importantly, MMYOLO and the
most recent version of MMDetection at the time of preparing the datasets
(\texttt{v0.4.0} and \texttt{v3.0.0rc2}) do not seem to fully support datasets
in the middle format to be used in all of their dataset wrappers.

Several other Python processing scripts were developed, to apply masks, combine
all ground truth files into one and to split the combined ground truth file into
training, validation and testing subsets. Additionally, scripts to review
individual datasets manually were created\,---\,one to visualize a~dataset by simply
adding bounding boxes (with class labels) to the images and one to convert the
visualized images to video (or videos).

A~few more scripts were created, of which two are worth mentioning: one for
uploading the DETRAC dataset to the LabelBox application for reannotation and
one for downloading the reannotated labels.


\section{Summary of Datasets}

\autoref{DatasetsSummary} contains a comparison of used datasets on a~higher
level and shows the number of images and instances contained with additional
comments. It is clear that the reannotated DETRAC dataset amounts to most of
the data and might have been enough by itself. However, to train a~robust
vehicle detector, it is essential to utilize every available and relevant
dataset. The images from the selected datasets feature a~great variety of
lighting and weather conditions, points of view, object scales, occlusions and
other relevant factors. Additionally, \autoref{DatasetsCounts} shows the number
of annotated object instances per class in all used datasets combined.

\begin{table}[t]
    \centering
    \begin{threeparttable}
        \begin{tabular}{|l|rr|p{5cm}|}
            \hline
            Dataset     & Images        & Instances    & Comments \\
            \hline
            DETRAC      &  \num{99771}  & \num{733833} & Large \newline Continuous video \newline High-quality camera \newline Different lighting conditions \\
            \hline
            Miovision   &  \num{110000} & \num{344416} & Large \newline Low-quality images \\
            \hline
            AAU         &    \num{1899} &  \num{10545} & Small \newline Different weather conditions \\
            \hline
            MTID        &    \num{5399} &  \num{64979} & Small \newline Continuous video \\
            \hline
            NDISPark    &     \num{142} &   \num{3302} & Small \newline Occlusions \\
            \hline
            VisDrone    &    \num{1610} &  \num{47720} & Small \newline Negative images \newline New points of view \\
            \hline
            \hline
            Total       &  \num{218821} & \num{1204795} & -- \\
            \hline
        \end{tabular}
        \caption{High-level comparison of used datasets after processing: the number of
        images and object instances, and additional comments for each of the datasets.}
        \label{DatasetsSummary}
    \end{threeparttable}
\end{table}

\begin{table}[t]
    \centering
    \begin{threeparttable}
        \begin{tabular}{|p{4.2cm}|r|}
            \hline
            Class         & Instances \\
            \hline
            Bicycle       &  \num{14036} \\
            Motorcycle    &  \num{17187} \\
            Passenger car & \num{916317} \\
            Transporter   & \num{123585} \\
            Bus           &  \num{64529} \\
            Truck         &  \num{38069} \\
            Trailer       &   \num{2360} \\
            Unknown       &  \num{28712} \\
            \hline
            \hline
            Total         & \num{1204795} \\
            \hline
        \end{tabular}
        \caption{Numbers of class instances in all datasets combined after processing.}
        \label{DatasetsCounts}
    \end{threeparttable}
\end{table}


\section{Training, Validation and Testing Dataset Split}

We chose to only include the Miovision and DETRAC datasets for validation and
testing, because they represent data from a~typical traffic surveillance camera
the best.

Because only the train subset of the Miovision dataset was used (only this
subset contained annotations), the images used for validation and testing are
chosen randomly. This, however, should not be a~problem since the dataset
contains many different camera angles and each image is very unique.

From the reannotated DETRAC dataset, two sequences were selected for the
validation subset (\texttt{MVI\_40201} and \texttt{MVI\_40244}) and two for the
test subset (\texttt{MVI\_40204} and \texttt{MVI\_40243}). The chosen sequences
are very different from the ones in the training subset, which is important for
the evaluations to be accurate. However, the \texttt{MVI\_40201} sequence is
recorded from the same angle as \texttt{MVI\_40204} and the same applies to
sequences \texttt{MVI\_40244} and \texttt{MVI\_40243}, so the validation and
test subsets are alike, but of course, contain different data. Example images
are shown in \autoref{TestValExamples}.

\begin{figure}[t]
    \centering
    \begin{subfigure}[b]{0.495\textwidth}
        \includegraphics[width=\textwidth]{dataset_example_MVI_40201.jpg}
        \caption{\texttt{MVI\_40201} (validation subset)}
    \end{subfigure}
    \begin{subfigure}[b]{0.495\textwidth}
        \includegraphics[width=\textwidth]{dataset_example_MVI_40204.jpg}
        \caption{\texttt{MVI\_40204} (test subset)}
    \end{subfigure}
    \begin{subfigure}[b]{0.495\textwidth}
        \includegraphics[width=\textwidth]{dataset_example_MVI_40241.jpg}
        \caption{\texttt{MVI\_40241} (validation subset)}
    \end{subfigure}
    \begin{subfigure}[b]{0.495\textwidth}
        \includegraphics[width=\textwidth]{dataset_example_MVI_40243.jpg}
        \caption{\texttt{MVI\_40243} (test subset)}
    \end{subfigure}
    \caption{Example images from sequences from the DETRAC dataset used for
    the validation and testing subsets.}
    \label{TestValExamples}
\end{figure}

The validation subset contains a~total of \num{36969} objects on \num{7770}
images, of which \num{5500} images are from the Miovision dataset and \num{2270}
from DETRAC. Similarly, the test subset contains a~total of \num{50357}
objects on \num{7990} images\,---\,\num{5500} images from the Miovision dataset and
\num{2490} from the DETRAC dataset.




\chapter{Object Detection Models}
\label{ModelsChapter}

In this chapter, the reader will find an~overview of used object detection
models, their training configurations and information about the training
process.


\section{Model Architectures}

This section introduces all object detection architectures evaluated in this
paper. The main focus is on the YOLOv8 object detector, which is the current
state-of-the-art real-time object detector and offers different model sizes for
different applications.

Along with standard model sizes\,---\,YOLOv8-medium, YOLOv8-small and
YOLOv8-nano\,---\,several others were trained and evaluated in this work:
YOLOv8-pico and YOLOv8-femto model versions, which are simply smaller versions
of the same YOLOv8 model. Finally, a~YOLOv8-large model with the CSP Darknet
backbone replaced by a~popular MobileNetV2 backbone is introduced (hereafter
referred to as YOLOv8 MobileNetV2).

When training the standard-sized models, pre-trained
models were downloaded from the repository of the MMYOLO library and used as the
starting point to utilize the transfer learning technique. Similarly,
a~pre-trained MobileNetV2 model from the MMClasification (recently renamed to
MMPretrain) repository was used when training the YOLOv8 MobileNetV2. The
transfer learning technique was not employed when training the rest of the
models\,---\,YOLOv8-pico and YOLOv8-femto.

In the YOLOv8 MobileNetV2 model, the output indices\footnote{Output indices
enable the selection of specific layers in the architecture, from which the
output feature maps should be used. In this case, only these feature maps are
then used as inputs to the object detector's neck.} selected were $[2, 4, 6]$.
Several models with different \texttt{out\_indices} settings were trained for
just 75 epochs to compare their performance, however, none of the alternative
settings led to improved performance (in terms of both inference speed and
accuracy).

Normally, a~square resolution is used for the model's input, but standard traffic cameras
output a~video of a~rectangular shape, usually with the aspect ratio being 16:9.
To optimize the inference speed, a~rectangular input resolution is used for all
trained detectors. The aim was to use aspect ratios as close to 16:9 as
possible, but the widths and heights of the input resolution have to be multiples of
32, so some input resolutions are further from it than others.

For smaller models, several input resolutions were tested to provide insights
into how a~vehicle detection model can be optimized by decreasing the resolution
of the input image. Although the inference speed should be (approximately)
directly proportional to the number of pixels in the input image, we decided to
test it and also find the limit\,---\,how small can the model's input resolution
be for it to start performing poorly on our test dataset.

For a~summary of all used model architectures, their sizes, input resolutions and the
amounts of their floating point operations (FLOPS) and parameters, see
\autoref{ModelArchitectures}. Quantities of floating point operations and
parameters were calculated using MMDetection's analysis script
\verb|get_flops.py|. Although the script calculates the output using input
resolution $720 \times 480$, simply multiplying the output by the difference
between the input resolutions provides an~accurate result. This can be explained
by the following equation:
\begin{equation}
    N_{new} = N_{720 \times 480} \times \frac{new\_width \times new\_height}{720 \times 480}
\end{equation}

\begin{table}[t]
    \centering
    \small
    \begin{threeparttable}
        \begin{tabular}{|c|c|c|c|r|r|}
            \hline
            \multirow{2}{*}{Architecture}& Deepen                 & Widen                   & Input      & Floating point & Parameters \\
                                        & factor                 & factor                  & resolution & operations (G) & (M) \\
            \hline
            \hline
            YOLOv8-medium                & 0.67                   & 0.75                    & $640 \times 384$ &   7.85 &  18.39 \\
            \hline
            YOLOv8-small                 & 0.33                   & 0.5                     & $640 \times 384$ &   2.63 &   3.73 \\
            \hline
            YOLOv8 MobileNetV2           & 1                      & 1                       & $512 \times 288$ &  0.560 &  2.133 \\
            \hline
            \multirow{3}{*}{YOLOv8-nano} & \multirow{3}{*}{0.33}  & \multirow{3}{*}{0.25}   & $640 \times 384$ &  0.758 &  1.688 \\
                                         &                        &                         & $512 \times 288$ &  0.455 &  1.013 \\
                                         &                        &                         & $448 \times 256$ &  0.354 &  0.788 \\
            \hline
            \multirow{3}{*}{YOLOv8-pico} & \multirow{3}{*}{0.166} & \multirow{3}{*}{0.125}  & $512 \times 288$ & 0.1510 & 0.3067 \\
                                         &                        &                         & $448 \times 256$ & 0.1175 & 0.2386 \\
                                         &                        &                         & $384 \times 224$ & 0.0881 & 0.1790 \\
            \hline
            \multirow{4}{*}{YOLOv8-femto}& \multirow{4}{*}{0.166} & \multirow{4}{*}{0.0625} & $512 \times 288$ & 0.0780 & 0.1288 \\
                                         &                        &                         & $448 \times 256$ & 0.0607 & 0.1002 \\
                                         &                        &                         & $384 \times 224$ & 0.0455 & 0.0751 \\
                                         &                        &                         & $352 \times 192$ & 0.0358 & 0.0591 \\
            \hline
        \end{tabular}
        \caption{Summary of different YOLOv8 model architectures used and comparison of
        amounts of their floating point operations and parameters with various model
        input resolutions.}
        \label{ModelArchitectures}
    \end{threeparttable}
\end{table}


\section{Model Configurations}
\label{ModelConfigurations}

This section provides an~overview of configurations used to train the vehicle
detectors. A~configuration of an~object detection model when using the
MMDetection or the MMYOLO library contains a~set of parameters that define the
model's behavior and the training, validation and testing pipelines. These
parameters can have a~significant impact on the model's speed and accuracy and
tuning them is essential to achieve good results.

Most of the YOLOv8 parameters are left unchanged from the default configuration
of YOLOv8-medium\footnote{The default YOLOv8-medium configuration used,
\texttt{yolov8\_m\_syncbn\_fast\_8xb16-500e\_coco.py}, can be found at
\url{https://github.com/open-mmlab/mmyolo/tree/v0.4.0/configs/yolov8}}, like: \\
\textbf{Optimizer:} Stochastic Gradient Descent with momentum \num{0.937} and weight decay 0.0005 \\
\textbf{Parameter scheduler:} Linear \verb|YOLOv5ParamScheduler| with learning rate factor of \num{0.01}

However, many parameters related to datasets and augmentations were adjusted and
will be explained in the next subsections. Apart from those, only two relevant
parameters were changed: the learning rate (adjusted individually for different
models) and the batch size, which was set to the highest possible for every
trained model. For the smallest one, YOLOv8-femto with $352 \times 192$ input
resolution, the largest batch size of 760 was used. Because training batch sizes
above 128 usually result in lower model precision~\cite{LargeBatch}, models with
large batch sizes were trained for 500 epochs instead of the default 300 epochs.
Along with other model-specific training parameters, like the number of warmup
epochs\footnote{During warmup epochs, the learning rate is gradually increased
from a~lower value to the target learning rate.}, these settings can be found in
\autoref{ModelSpecificConfigurations}.

\begin{table}[t]
    \centering
    \small
    \begin{threeparttable}
        \begin{tabular}{|c|c|c|c|c|c|}
            \hline
            \multirow{2}{*}{Architecture}& \multirow{2}{*}{Input resolution} & Learning & Batch & \multirow{2}{*}{Epochs} & Warmup \\
                                        &                                   & rate     & size  &                         & epochs \\
            \hline
            \hline
            YOLOv8-medium                & $640 \times 384$ & 0.00125 & 46  & 300 & 5 \\
            \hline
            YOLOv8-small                 & $640 \times 384$ & 0.00125 & 76  & 300 & 5 \\
            \hline
            YOLOv8 MobileNetV2           & $512 \times 288$ & 0.01    & 96  & 300 & 5 \\
            \hline
            \multirow{3}{*}{YOLOv8-nano} & $640 \times 384$ & 0.00125 & 112 & 300 & 5 \\
                                        & $512 \times 288$ & 0.00125 & 192 & 300 & 5 \\
                                        & $448 \times 256$ & 0.00125 & 256 & 300 & 5 \\
            \hline
            \multirow{3}{*}{YOLOv8-pico} & $512 \times 288$ & 0.01    & 224 & 500 & 10 \\
                                        & $448 \times 256$ & 0.01    & 384 & 500 & 10 \\
                                        & $384 \times 224$ & 0.01    & 512 & 500 & 10 \\
            \hline
            \multirow{4}{*}{YOLOv8-femto}& $512 \times 288$ & 0.01    & 380 & 500 & 10 \\
                                        & $448 \times 256$ & 0.01    & 420 & 500 & 10 \\
                                        & $384 \times 224$ & 0.01    & 640 & 500 & 10 \\
                                        & $352 \times 192$ & 0.01    & 760 & 500 & 10 \\
            \hline
        \end{tabular}
        \caption{Training configurations for individual models, including learning rate, batch size, number of training epochs and number of warmup epochs.}
        \label{ModelSpecificConfigurations}
    \end{threeparttable}
\end{table}


\subsection{Dataset Wrappers}

In the MMYOLO (and the MMDetection) model configurations, datasets to use for
training, validation and testing are specified using dataset wrappers, from
which \texttt{dataloaders} (objects internally representing a~dataset) are
created. Because the datasets used in this project were in the COCO format, the
\texttt{YOLOv5CocoDataset} wrapper was used for each of the six used datasets.

To compensate for some datasets being smaller than others while being important
and of high quality, a~dataset wrapper \texttt{RepeatDataset} is used, which
makes the underlying dataset $n$-times more frequent during training. All
datasets are finally concatenated into one by the \texttt{ConcatDataset}
wrapper. The repetition factors of individual datasets are shown in
\autoref{RepetitionFactors}.

\begin{table}[t]
    \centering
    \small
    \begin{threeparttable}
        \begin{tabular}{|c|c|c|c|}
            \hline
            Dataset name & Images & Repetition factor & Images after over-sampling \\
            \hline
            DETRAC       &  \num{99771} &  1 & \num{99771} \\
            Miovision    & \num{110000} &  1 & \num{110000} \\
            AAU          &   \num{1899} &  3 & \num{5697} \\
            MTID         &   \num{5399} &  5 & \num{26995} \\
            NDISPark     &    \num{142} & 25 & \num{3550} \\
            VisDrone     &   \num{1610} &  4 & \num{6440} \\
            \hline
        \end{tabular}
        \caption{Repetition factors for each dataset used for training, including the
        numbers of images before and after over-sampling.}
        \label{RepetitionFactors}
    \end{threeparttable}
\end{table}


\subsection{Training Augmentation Pipeline}

In this subsection, the training augmentation pipeline is explained. Data
augmentations are important when training an~object detector, especially
detectors of the YOLO family. An~augmentation in this context refers to the
process of applying a~transformation to an~image to artificially increase the
size and diversity of the training dataset, which helps prevent overfitting and
improves the generalization ability of the trained model. To a~convolutional
neural network, even a~tiny rotation, translation, image flip, noise or color
distortion makes an~input image appear to be something completely different, so
applying these transformations randomly to the input images is crucial when
training a~robust model. Examples of images augmented by the training
augmentation pipeline can be seen in \autoref{AugmentedExamples}. Following are
the transformations in the main training augmentation pipeline:

\begin{figure}[t]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{augmented_detrac.jpg}
        \caption{DETRAC}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{augmented_mio-tcd.jpg}
        \caption{Miovision}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{augmented_mtid.jpg}
        \caption{MTID}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{augmented_ndis.jpg}
        \caption{NDISPark}
    \end{subfigure}
    \caption{Examples of images augmented by the training augmentation pipeline,
    including the ground truth labels.}
    \label{AugmentedExamples}
\end{figure}


\subsubsection*{Resize}

First, the image is resized to fit the model's input resolution while, of
course, keeping the aspect ratio unchanged.


\subsubsection*{Pad}

If the aspect ratio of the input image is not the same as the model's input, extra pixels around the input image must be added
to adapt to the model input's aspect ratio. The \textbf{color value} of the padded pixels is set to RGB(114, 114, 114).


\subsubsection*{Random Affine}

The \texttt{YOLOv5RandomAffine} applies affine transformations to the image,
while randomly selecting the values from configured ranges. Parameters:\\
\textbf{Maximum translation ratio:} 0.05 \\
\textbf{Maximum rotation degree:} 5 \\
\textbf{Maximum shear degree:} 3 \\
\textbf{Scaling ratio} is set individually for each dataset as shown in \autoref{RandomAffineScalingRatios}.

\begin{table}[h]
    \centering
    \begin{threeparttable}
        \begin{tabular}{|c|c|c|}
            \hline
            Dataset name & Minimum scaling ratio & Maximum scaling ratio \\
            \hline
            DETRAC       & 0.8 & 1.0 \\
            Miovision    & 1.0 & 1.1 \\
            AAU          & 0.9 & 1.1 \\
            MTID         & 0.9 & 2.0 \\
            NDISPark     & 0.9 & 1.5 \\
            VisDrone     & 1.5 & 2.5 \\
            \hline
        \end{tabular}
        \caption{Image scaling ratios in the \texttt{RandomAffine} transformation for each dataset.}
        \label{RandomAffineScalingRatios}
    \end{threeparttable}
\end{table}

The \textbf{color value for padding} around the transformed image (if needed)
is set to RGB(114, 114, 114) to be the same as in the \texttt{Pad} transformation.


\subsubsection*{Cut-Out}

The \texttt{Cut-Out} transformation randomly selects regions of the image and
fills them with a~single color. Again, parameters of this transformation are set
individually for each dataset and are shown in \autoref{CutOutParams}.

\begin{table}[h]
    \centering
    \begin{threeparttable}
        \begin{tabular}{|p{2.5cm}|c|c|}
            \hline
            Dataset name & Number of regions & Size of a~single region \\
            \hline
            DETRAC       &  6 & $22 \times 22$ px \\
            Miovision    &  4 & $26 \times 26$ px \\
            AAU          &  8 & $10 \times 10$ px \\
            NDISPark     & 12 & $20 \times 20$ px \\
            MTID         & 12 & $10 \times 10$ px \\
            VisDrone     & 20 & $8 \times 8$ px \\
            \hline
        \end{tabular}
        \caption{\texttt{Cut-Out} transformation parameters for each dataset.}
        \label{CutOutParams}
    \end{threeparttable}
\end{table}

The \textbf{fill color value} is again set to RGB(114, 114, 114), same as for
padding in the previous transformations.


\subsubsection*{Custom Cut-Out}

A~custom cut-out transformation was developed, similar to \texttt{Cut-Out} used
in the previous step. Here, the regions are selected within each bounding box
with a~certain probability, rather than being chosen randomly within the entire image.
Additionally, the region size is specified as a~range of areas in relation to the
bounding box area\,---\,if the upper value is $10\,\%$ and a~bounding box area is 100
pixels, the maximum area of a~cut-out region is 10 pixels.

With the boolean option \texttt{random\_pixels} toggled, the color of each pixel
of a~cut-out region is generated randomly instead of filling it with
a~pre-defined color. However, it was found to have no effect.

The \textbf{probability} of a~region being dropped from each bounding box is set
to \num{0.05} ($5\,\%$) and the \textbf{region area} is set to be randomly selected from
interval $[5\,\%, 35\,\%]$.


\subsubsection*{Albumentations}

Albumentations~\cite{Albumentations} is a~popular open-source library for data
augmentation. The MMYOLO library provides the option to use these transformations
in the data augmentation pipeline. Settings are left unchanged from the original
YOLOv8-medium configuration: \\
\textbf{Blur probability:} 0.01 \\
\textbf{Median blur probability:} 0.01 \\
\textbf{Grayscale probability:} 0.01 \\
\textbf{CLAHE probability:} 0.01


\subsubsection*{HSV Random Augmentations}

The \texttt{YOLOv5HSVRandomAug} simply adjusts the hue, saturation and value of
the image randomly.


\subsubsection*{Random Flip}

With \textbf{probability} of 0.5, the image is horizontally flipped using the
\texttt{RandomFlip} augmentation.


\subsubsection*{Photometric Distortion}

The \texttt{PhotoMetricDistortion} augmentation distorts an~image sequentially,
while each transformation is applied with a~probability of 0.5. It modifies the
brightness and contrast, converts color from BGR\footnote{BGR (Blue, Green, Red)
only differs from the well-known RGB format by the order of its channels. It is
widely used in image processing for legacy and compatibility reasons.} to
HSV\footnote{HSV (Hue, Saturation, Value) is a~cylindrical-coordinate color
model, where the hue represents the color type, saturation refers to the color's
intensity and value corresponds to brightness.}, modifies the saturation and the
hue, converts from HSV to BGR, modifies the contrast and finally, randomly swaps
the color channels.


\subsubsection*{Filter Annotations}

As the last step in the pipeline, \texttt{FilterAnnotations} is called to remove
bounding boxes with \textbf{width or height} lower than \num{8} pixels.


\subsubsection{Fine-Tuning Augmentation Pipeline}

Originally, MMYOLO's YOLOv8 models are configured to switch to a~simplified
augmentation pipeline for the last 10 training epochs. This model fine-tuning
strategy is kept in the augmentation pipeline and for the last 10 training
epochs, cut-out augmentations are omitted and affine transformations are changed
so that no rotation, translation or shear is applied to the samples.

However, this setting seemed to cause a~consistent decrease in the validation
mean Average Precision (mAP) metric during the fine-tuning phase (last 10
epochs). Because the epoch with the highest validation mAP is selected for
deployment, as a~result, none of the evaluated models used the last 10 training
epochs and this configuration was therefore not utilized.




\chapter{Experiments}
\label{Experiments}

This chapter provides details about future experiments, including the
benchmarked devices, information about the deployment and model optimization
processes, and the methodology employed for testing. Additionally, it offers
important considerations to keep in mind before drawing conclusions from the
results.


\section{Devices Used in the Experiments}

While the main focus is on NVIDIA Jetson embedded devices, which were designed
specifically for tasks like object detection, tests were run on several other
devices for comparison of both ends of the performance gauge. In this section,
details about each device that the models were evaluated on are provided,
including details about the software and device configurations.


\subsection{NVIDIA Jetson Platforms}

Technical details of NVIDIA Jetson embedded platforms were discussed in
\autoref{EmbeddedPlatforms} and software versions will be shown later in this section.
However, one more important detail to note before reading about the experiments
is the used power plan. All used Jetson devices feature several power plans to
choose from to adjust the performance and power consumption for a~specific task.
For all experiments, these power plans were selected: \\
\textbf{Jetson AGX Xavier}: 30\,W power plan with 4 out of 8 cores active \\
\textbf{Jetson Xavier NX}: 20\,W power plan with 4 out of 8 cores active \\
\textbf{Jetson Nano}: 10\,W \texttt{MAXN} power plan with all 4 cores active


\subsection{NVIDIA GeForce MX150}

To be able to compare inference speeds on these embedded devices to ones on
a~regular GPU, tests were also done on an~NVIDIA GeForce MX150 GPU with 2\,GB of
memory on a~DELL Latitude 5401 laptop. Because of the GPU memory constraint, not
all models can be evaluated with all inference batch sizes on this device, as
will be pointed out in individual relevant experiments. Additionally, we were
unable to perform any tests using the TensorRT library on this device because
some of the packages required could not be installed on the system.


\subsection{Intel Core i7-9850H}

Models were also evaluated on a~higher-end laptop CPU, Intel Core i7-9850H with
the base frequency 2.6\,GHz to demonstrate how the inference speeds of YOLOv8
object detection models differ between a~GPU and a~CPU.

Typically, the frequency at which a~CPU operates is adjusted to accommodate the
load and it often spikes up when a~computation-hungry process starts. After
a~while, when the CPU temperature rises above a~certain threshold, the CPU
frequency has to drop to avoid overheating. This is called dynamic frequency
scaling, also known as CPU throttling. To make the performance measurements as
accurate as possible, the number of warmup samples when testing is increased
from 10 to 100. This means that the inference speed (FPS) of these first 100
samples will be ignored when calculating the average FPS.

However, please note that the test results still not might be accurate enough
and might depend on the underlying operating system and the environment.


\subsection{Raspberry Pi 4B}

Finally, the popular Raspberry Pi 4B single-board computer with ARM Cortex-A72
CPU with base frequency 1.8\,GHz was used to test the trained models. Although it
was not developed to run object detection models, it is perfect to test the
smallest models and see how far go the possibilities of the real-time YOLOv8
object detector.


\subsection{Software versions}

Information about software installed on all previously mentioned devices can be
found here. See \autoref{DevicesPackages} for a~compact table displaying
versions of relevant software. Following are the reasons behind some odd choices
or compatibility issues.

\begin{table}[t]
    \centering
    \small
    \begin{threeparttable}
        \begin{tabular}{|l|c|c|c|c|}
            \hline
            % \multirow{2}{*}{Name} & NVIDIA Jetson & NVIDIA Jetson & NVIDIA Jetson \\
            %                       & AGX Xavier    & Xavier NX     & Nano          \\
            % Name             & NVIDIA Jetson devices & NVIDIA MX150 & Intel Core i7-9850H & Raspberry Pi \\
            \multirow{2}{*}{Name} & All NVIDIA     & NVIDIA & Intel Core  & Raspberry \\
                                  & Jetson devices & MX150  & i7-9850H    & Pi 4B \\
            \hline
            \multirow{2}{*}{Operating system} & Linux for Tegra             & \multicolumn{2}{c|}{Linux}     & Linux   \\
                                            & (L4T OS \texttt{32.7.3}) & \multicolumn{2}{c|}{Debian 12} & Raspbian 11   \\
            \hline
            JetPack SDK      & \texttt{4.6.3}                     & \multicolumn{3}{c|}{--}  \\
            \hline
            Python           & \texttt{3.6.9}                     & \multicolumn{2}{c|}{\texttt{3.10.0}} & \texttt{3.7.0} \\
            \hline
            CUDA             & \texttt{10.2}                      & \texttt{11.8} & \multicolumn{2}{c|}{--} \\
            \hline
            TensorRT         & \texttt{8.2.1} & \multicolumn{3}{c|}{--}             \\
            \hline
            ONNX             & \multicolumn{3}{c|}{1.13.1} & \texttt{1.12.0}             \\
            \hline
            ONNX Runtime     & \texttt{1.11.0} (ver. GPU) & \texttt{1.12.0} (ver. GPU) & \texttt{1.12.0} & \texttt{1.11.0}            \\
            \hline
            PyTorch          & \texttt{1.10.0} & \multicolumn{2}{c|}{\texttt{2.0.0}} & \texttt{1.8.0} \\
            \hline
            MMCV             & \multicolumn{4}{c|}{\texttt{2.0.0}} \\
            \hline
            MMDeploy         & \multicolumn{3}{c|}{\texttt{1.0.0}} & \texttt{1.0.0rc3} \\
            \hline
            MMDetection      & \multicolumn{4}{c|}{\texttt{3.0.0}} \\
            \hline
            MMEngine         & \multicolumn{4}{c|}{\texttt{0.7.2}} \\
            \hline
            MMYOLO           & \multicolumn{4}{c|}{\texttt{0.4.0}} \\
            \hline
        \end{tabular}
        \caption{Versions of relevant software installed on devices used to
        deploy and test the trained models.}
        \label{DevicesPackages}
    \end{threeparttable}
\end{table}


\subsubsection*{JetPack SDK}

Despite both Jetson Xavier NX and Jetson AGX Xavier being supported by the
NVIDIA JetPack SDK version \texttt{5.1.1}, deploying YOLOv8 models using this
version often led to an~untraceable fatal error. The error originated from one
of NVIDIA's proprietary libraries, and provided limited information regarding
its cause. Fortunately, downgrading the JetPack version to \texttt{4.6.3}
resolved this issue.

Although the root cause of the error and the specific package (or library) that
required downgrading was not fully determined, it is suspected that the problem
was related to TensorRT version \texttt{8.5.2}, as the error originated from the
TensorRT development library \texttt{libnvinfer}.

For the sake of providing further context to the reader, the error message
received was \verb|operation.cpp:203: DCHECK(!i->is_use_only()) failed|. No
other relevant warnings or error messages preceded this one, making it
challenging to pinpoint the exact cause.


\subsubsection*{Old Python Version on Jetson Devices}

All NVIDIA Jetson devices used to evaluate the trained models utilize the same
version of the JetPack SDK, \texttt{4.6.3}, which includes Python version
\texttt{3.6.9}. However, several essential Python packages\,---\,specifically,
\texttt{protobuf} version \texttt{3.20.2}, \texttt{MMCV} version \texttt{2.0.0}
and \texttt{MMEngine} version \texttt{0.7.2}\,---\,require a~Python version of
\texttt{3.7.0} or higher. As these packages (and the specified versions) were
crucial for the model deployment and testing to be possible, we had to manually
modify their requirements to allow for installation with the Python version
\texttt{3.6.9}.

Of course, this approach is not an~ideal solution to the problem and its success
was not guaranteed. Fortunately, no indications of compatibility issues were
discovered during model deployment or testing.

One might suggest that upgrading to a~newer Python version would be the most
appropriate solution. However, due to compatibility and dependency constraints
on Jetson devices, this is not feasible.

JetPack version \texttt{5.1.1} includes a~more recent Python version but does
not support Jetson Nano, and when installed on Jetson Xavier NX or Jetson AGX
Xavier, the deployment of the trained models fails, as explained earlier in this
section. Although installing a~different Python version than the one provided
with the JetPack SDK is possible, installing other necessary packages for the
newer Python is not (or would be too complex). This is because many such
packages were developed specifically for Jetson devices and only support
a~certain Python version\,---\,the one pre-installed with the JetPack SDK.


\section{Model Deployment and Optimizations}

For model deployment, an~automated Python script was created, utilizing the
MMDeploy library's deployment script. Our script uses all available deploy
configurations\footnote{When deploying models using MMDeploy, deploy
configuration files are used to specify the parameters of the deployment
process, including target backend or whether to apply post-training
quantization.} to deploy all trained PyTorch models to a~specified backend (or
backends).

There are only two deploy configurations for the ONNX Runtime backend: one
with a~static model shape\footnote{A model shape can be static or dynamic.
A~static model can only receive inputs of a~certain shape specified when deploying
(batch size and input resolution\,---\,width and height), whereas a~dynamic model
accepts various input shapes.} and one with a~dynamic shape in the batch size
dimension\footnote{A dynamic model shape in the batch size dimension means the
model accepts different input shapes but still requires a~fixed input image
resolution.}. The dynamic models were deployed to accept a~maximum batch size of
32, but were optimized for a~batch size of 1. For the TensorRT backend, two
additional deploy configurations were created, both for a~model with a~dynamic
shape in the batch dimension: one for weight quantization to the
\texttt{FP16} representation, and one for quantization to \texttt{INT8}
including weight calibration using the validation dataset.

To optimize inference on the Raspberry Pi 4B, we aimed to use the NCNN inference
framework designed for mobile and embedded devices with limited computing
resources. However, the MMYOLO library does not yet support converting the
YOLOv8 operations unsupported by NCNN to supported ones, so the NCNN framework
was not used.


\subsection*{Devices Used for Deployment}

The models in the ONNX format were all deployed on a~single device (with ONNX
package version \texttt{1.13.1}) and distributed to all devices. The TensorRT
backend is only used on NVIDIA Jetson devices and all models were deployed to
the TensorRT engine\footnote{A TensorRT engine is a~deployed model in the
TensorRT format.} individually on each NVIDIA Jetson device, because they are
platform-specific and transferring them across different devices is not
recommended.


\subsection*{Training Checkpoints Used for Deployment}

Checkpoint files, which can be selected for deployment, are saved periodically
during training. When deploying, usually the checkpoint with the highest
validation mAP is selected, which is particularly useful if the mAP starts
decreasing during training.

As mentioned in \autoref{ModelConfigurations}, the mAP usually decreased during
the last 10 epochs, but at times\,---\,most noticeably for YOLOv8-medium and
YOLOv8-small\,---\,it started decreasing even sooner, probably due to
overfitting. Because it sometimes stopped increasing (or started slowly
decreasing) while other mAP metrics (mainly the $\text{mAP}^{\text{small}}$),
were still rising, some checkpoints to use for deployment were selected manually
for a trade-off between negligibly lower mAP (at worst) and higher, for example,
$\text{mAP}^{\text{small}}$. The checkpoints used for deployment, including
their mAP values are shown in \autoref{CheckpointsUsed}.

We imagined the overfitting of YOLOv8-medium and YOLOv8-small models might be
due to a low learning rate (of 0.00125), so we also tried training the
YOLOv8-medium with the default learning rate (from the original MMYOLO
configuration) of 0.01. However, this proved unsuccessful as the validation mAP
of the model with the higher learning rate didn't come close to the one of the
original model, although it was rising more consistently.

\begin{table}[t]
    \centering
    \small
    \begin{threeparttable}
        \begin{tabular}{|c|c|rr|rr|}
            \hline
            \multirow{3}{*}{\parbox{1.9cm}{\centering Model\\(YOLOv8)}} &
                \multirow{3}{*}{\parbox{1.6cm}{\centering Input\\resolution}} &
                    \multicolumn{2}{c|}{Best epoch} & \multicolumn{2}{c|}{Selected epoch} \\
            \cline{3-6}
            & & Epoch  & mAP  & Epoch  & mAP \\
            & & number & (\%) & number & (\%) \\
            \hline
            \hline
            \multirow{1}{*}{medium}     & $640\times384$  &  75 & 62.6 &     -- & -- \\
            \hline                                          
            \multirow{1}{*}{small}      & $640\times384$  &  55 & 59.4 &     -- & -- \\
            \hline
            \multirow{1}{*}{MobileNetV2}& $512\times288$  & 289 & 57.2 &     -- & -- \\
            \hline                      
            \multirow{3}{*}{nano}       & $640\times384$  & 235 & 55.1 &     -- & -- \\
            \cline{2-6}
                                        & $512\times288$  & 280 & 54.6 & 290 & 54.6 \\
            \cline{2-6}
                                        & $448\times256$  & 289 & 55.3 &     -- & -- \\
            \hline
            \multirow{3}{*}{pico}       & $512\times288$  & 470 & 48.5 &     -- & -- \\
            \cline{2-6}
                                        & $448\times256$  & 440 & 46.4 & 450 & 46.4 \\
            \cline{2-6}
                                        & $384\times224$  & 470 & 45.7 & 490 & 45.7 \\
            \hline
            \multirow{4}{*}{femto}      & $512\times288$  & 460 & 36.0 &     -- & -- \\
            \cline{2-6}
                                        & $448\times256$  & 492 & 35.8 &     -- & -- \\
            \cline{2-6}
                                        & $384\times224$  & 460 & 33.7 & 490 & 33.7 \\
            \cline{2-6}
                                        & $352\times192$  & 410 & 30.2 &     -- & -- \\
            \hline
        \end{tabular}
        \caption{Epoch numbers and validation mAP for best epochs and manually
        selected epochs (if applicable).}
        \label{CheckpointsUsed}
    \end{threeparttable}
    \normalsize
\end{table}


\subsection*{Weight Quantization}

The MMDeploy library supports post-training quantization to \texttt{FP16} and
\texttt{INT8} representations during the process of model deployment to
TensorRT. To preserve the model's accuracy after quantization to \texttt{INT8},
weight calibration was done using the validation dataset.

Although the ONNX Runtime framework also supports quantization (to both
\texttt{FP16} and \texttt{INT8}), the process is not as straightforward and
does not seem to be supported by the MMDeploy library. Although evaluating models
quantized to \texttt{INT8} would be beneficial, the quantization to
\texttt{FP16} would probably have a~little effect on performance on CPUs as they
do not usually support operations between numbers in this representation and
calculate them using the same operators as numbers in the \texttt{FP32}
representation.

Additionally, TensorRT on the NVIDIA Jetson Nano with Maxwell GPU does not
support fast inference of models quantized to \texttt{INT8}, so quantization to
\texttt{INT8} was not performed on this device.


\section{Experiments and Evaluation}

In this section, we discuss additional details about all performed experiments,
including how the underlying measurements were taken and important things to
note before drawing conclusions from the results, which will be presented in
the following \autoref{Results}.

All experiments will analyze the data collected by testing. Testing was done using
a~Python script included as a~tool in the MMDeploy library, \texttt{test.py},
which uses the library to create a~wrapper for the deployed model. It then uses
the test dataset to evaluate the model's performance, calculating the
inference speed in FPS and six mAP metrics as explained in
\autoref{EvaluationMetrics}. The inference speed is calculated including the
input pre-processing and non-maximum suppression\footnote{Non-maximum
suppression in object detection is a~method that selects a~single prediction
bounding box out of several overlapping bounding boxes, which are likely
generated for the same object.} (NMS).

To test all possible combinations of models, backends, deployment configurations
and batch sizes on individual devices automatically, a~new Python script
\texttt{test\_all.py} was developed, which uses the MMDeploy's \texttt{test.py}
script and executes it for each possible test combination (unless specified
differently by the user).

The MMDeploy's \texttt{test.py} saves the test logs, from which the mAP metrics
and inference speeds can be read. This is also done automatically by our
\texttt{collect\_test\_results.py} Python script, which reads all available test
logs to output all metrics structured in JSON format to a~file. Although the
\texttt{test.py} script also outputs the final test metrics in JSON format to
a~folder, the folder is named after the test start time (eg.
\texttt{20230429\_043829/}) and it cannot be overridden, so reading the data from
the log file is simpler and less error-prone.


\subsubsection*{Number of Threads When Performing Inference on CPUs}

When testing models in the ONNX format on CPUs, the MMDeploy library does not
control how many processes the ONNX Runtime backend uses to run inference. To
provide accurate and reproducible results, the source code of the library was
modified to create the ONNX Runtime inference session with the option of only
running in a~single thread. This means that the inference speeds on CPUs are not
the highest possible. Please note that simply multiplying the inference speed by
the number of CPU cores to get the maximum FPS possible on the device is
generally incorrect, because running a~multi-threaded inference is not as
efficient as running it on a~single thread. On the other hand, when $n$ separate
inference processes on a~CPU with $n$ cores are used, it is theoretically
possible to achieve inference speeds $n$ times higher than with
a~single-threaded inference, but the duration of the inference for a~single
input would remain the same (at best).




\chapter{Results}
\label{Results}

This chapter presents the results of the conducted experiments and provides
a~detailed analysis to explain the insights that can be gained from these
findings. Thanks to the extensive data collected from the tested devices, we can
identify trends, strengths, and limitations of each model and hardware
combination, as well as highlight potential areas for future research. Of
course, complete data can be rarely displayed in a~single figure, so in these
experiments, we typically only focus on the most important or representative sets
of data, but include additional data in the \hyperref[Appendix]{Appendix}, which
will also be referenced from individual experiments.

The chapter begins by evaluating the trained models in terms of precision and
recall through precision-recall curves. We then examine several factors that
could potentially impact the performance of the tested models, namely input
resolution, inference backends, model shape (static or dynamic), and batch sizes
during inference. Subsequently, we benchmark the models in terms of their
inference speeds and various mean Average Precision metrics. Lastly, we present
the inference speeds of some of the trained models for comparison across all six
tested devices.

It is important to note that the mAP values measured on NVIDIA Jetson platforms
using the TensorRT backend (where the models were deployed individually on each
device) exhibit minor differences. Out of 930 metrics collected from each
device, 276 varied across the platforms. To analyze whether these deviations
were significant, we calculated the median, mean and maximum values of the
deviations. The median was found to be 0.1\,\%, the mean was calculated to be
0.1413\,\% and the maximum, with a~total of 12 occurrences, was 0.4\,\%. We
therefore consider these deviations insignificant and don't consider them in the
following experiments.


\section{Precision-Recall Curves of Major Models}

% python3 pr_curve.py working_dir_yolov8_m_640x384_lr0.00125/ working_dir_yolov8_l_mobilenet_v2_512x288_indices_246/ working_dir_yolov8_n_512x288/ working_dir_yolov8_f_352x192/
% For the appendix:
% python3 pr_curve.py working_dir_yolov8_s_640x384/ working_dir_yolov8_n_640x384/ working_dir_yolov8_n_448x256/ working_dir_yolov8_p_512x288/ working_dir_yolov8_p_448x256/ working_dir_yolov8_p_384x224/ working_dir_yolov8_f_512x288/ working_dir_yolov8_f_448x256/ working_dir_yolov8_f_384x224/

In this experiment, we measured the precision and recall values of four of the
trained models on the test dataset to plot the precision-recall (PR) curves in
\autoref{PRCurveMajor}. We could not show the curves of all the models because
the figure would simply not fit on a~single page, so instead, only the four most
representative models were selected. A~more complete figure with PR curves for
the rest of the models can be seen in \autoref{PRCurvesAdditional}. The
precision and recall values were calculated by MMDeploy's test script, to which
we have inserted a~few lines of code to save the values to a~file.

In the plots, the performance for the bus class appears abnormally high. We
hypothesize that the reason behind it is the presence of a~few
easily-detectable, large buses in numerous images in the reannotated DETRAC
dataset within the testing set. Additionally, the low performance of models in
detecting vehicles belonging to the \textit{unknown} category should not be
a~major concern, as the diversity of the vehicles of this category makes it
difficult to accurately detect and classify them.

As could be expected, the YOLOv8-medium model demonstrated superior performance,
achieving the highest precision and recall values. Furthermore, the performance
of the YOLOv8 MobileNetV2 model is comparable to that of the YOLOv8-nano,
although slightly superior. In contrast, the YOLOv8-femto with an~input
resolution of $352 \times 192$ could be an~example of a~model with considerably
low performance.

\begin{figure}[H]
    % \begin{framed}
        \centering
        \includegraphics[width=\textwidth]{pr_curve.pdf}
        \caption{Precision-Recall curves are displayed for four major models,
        including YOLOv8-femto with the smallest input resolution of $352 \times
        192$. It is important to note that, in this figure, the YOLOv8-nano
        model chosen for comparison has an~input resolution of $512 \times 288$,
        although most experiments used the model with an~input resolution of
        $640 \times 384$. This decision was made to simplify the comparison
        between YOLOv8-nano and YOLOv8 MobileNetV2. The measurements were taken
        utilizing the ONNX Runtime backend.}
        \label{PRCurveMajor}
    % \end{framed}
\end{figure}

% Effect of Model's Input Resolution on Inference Speed
\begin{figure}[H]
    % \begin{framed}
        \centering
        \includegraphics[width=\textwidth]{input_resolution_vs_fps.pdf}
        \caption{Inference speed (FPS) comparison of YOLOv8-nano, YOLOv8-pico, and
        YOLOv8-femto models with different input resolutions. The results generally
        support our hypothesis that the reduction in model's input resolution is directly
        proportional to the increase in inference speed, although with some
        deviations. These results were obtained by running tests on NVIDIA Jetson
        Nano using the TensorRT backend. The tested models were deployed with
        dynamic shape in the batch size dimension to run the tests with a~batch size
        of 16 to most accurately measure the speeds of the smallest models.}
        \label{InputResolutionVSFPS}
    % \end{framed}
\end{figure}


\section{Effect of Model's Input Resolution on Inference Speed}
\label{InputResolutionVSFPSExperiment}

Smaller models\,---\,YOLOv8-nano, YOLOv8-pico and YOLOv8-femto\,---\,were
trained with different input resolutions to understand how it affects the
model's performance on our test dataset.

We expect that the reduction in the number of pixels in a~model's input
resolution should be directly proportional to the increase in its inference
speed (FPS). For example, we anticipate that a~detector with an~input resolution
of $640 \times 320$ would be twice as fast compared to a~detector with an~input
resolution $640 \times 640$. \autoref{InputResolutionVSFPS} compares the
inference speeds of models with different input resolutions. The biggest input
resolution is selected as the base to which other input resolutions will be
compared, and the expected FPS is calculated using this simple equation:
\begin{equation}
    \text{Expected FPS} = \text{Base FPS} \times \frac{\text{Base input width} \times \text{Base input height}}{\text{Compared input width} \times \text{Compared input height}}
\end{equation}

While the results show that the relationship between input resolution and
FPS cannot be exactly captured by the above equation, they generally
supported our hypothesis. Following this experiment, we will suppose that the
input resolution reduction is indeed directly proportional to the inference
speed increase, and in some of the following experiments, we will only compare
one input resolution for each model to make the results more informative and
concise.

% Inference Speeds: ONNX Runtime vs. TensorRT
\begin{figure}[H]
    % \begin{framed}
        \centering
        \includegraphics[width=\textwidth]{onnx_vs_tensorrt_comparison.pdf}
        \caption{Comparison of inference speeds of all trained YOLOv8 model
        architectures between the TensorRT and the ONNX Runtime inference
        backends (of course, both utilizing the devices' GPU). Tests were
        conducted on NVIDIA Jetson Nano with a~batch size of 1 and on NVIDIA
        Jetson AGX Xavier with a~batch size of 32. Please note that for each
        model architecture, only the model with the largest input resolution was
        tested and all tested models were deployed as dynamic in the batch size
        dimension.}
        \label{OrtVsTrtFPS}
    % \end{framed}
\end{figure}


\section{Inference Speeds: ONNX Runtime vs. TensorRT}

This experiment studies how the inference speed is increased by using the
TensorRT backend for inference on NVIDIA Jetson devices instead of the ONNX
Runtime backend. The inference speeds (FPS) measured by the test script are
compared on both the lowest and the highest-performing NVIDIA Jetson devices
with different batch sizes to be able to draw general conclusions from the
results. For each model architecture, only the highest input resolution is
selected for testing, following the insights gained by the experiment in
\autoref{InputResolutionVSFPSExperiment}. Additionally, all tested models were
dynamic in shape. The results of this experiment are shown in
\autoref{OrtVsTrtFPS}.

The results clearly show the superiority of the TensorRT inference backend on
NVIDIA Jetson devices for all tested models and different batch sizes.
Naturally, the choice of the inference backend has no effect on the mean Average
Precision of the tested models. Therefore, none of the following experiments
will feature the ONNX Runtime backend on NVIDIA Jetson devices.


\section{Effect of Model's Shape on Inference Speed}

While a~model with a~static shape has a~fixed input resolution and a~fixed batch
size, it can be set to accept input images of different resolutions and
different batch sizes. This paper utilizes the MMDeploy library to deploy the
trained models into both static and dynamic shapes. However, the deployment
process was configured so that dynamic models would only be flexible in the
batch size dimension while the model's input resolution stays static. Anyways,
we will call these models dynamic.

In this experiment, we compare inference speeds achieved by static models and
dynamic models to learn whether deploying a~model to a~dynamic shape (in the
batch size dimension) decreases the inference speed. Only the most
representative models were selected for comparison to keep the plots concise.
Naturally, all tests were conducted with a~batch size of 1. Tests
were conducted on the NVIDIA Jetson Nano with the TensorRT inference backend and
on the Raspberry Pi 4B with ONNX Runtime to make the measurements as accurate as
possible by using the least-performing devices for both inference backends. The
comparison of inference speeds on all model architectures is presented in
\autoref{StaticDynamicModelShape}.

% Effect of Model's Shape on Inference Speed
\begin{figure}[t]
    % \begin{framed}
        \centering
        \includegraphics[width=\textwidth]{static_dynamic_comparison.pdf}
        \caption{Comparison of inference speeds between models with static and
        dynamic shapes in the batch size dimension. Only the most representative
        models were selected for comparison, as the rest of the data do not
        provide additional insights.}
        \label{StaticDynamicModelShape}
    % \end{framed}
\end{figure}

Because the inference speed is not consistently higher for static models, we
conclude that it does not significantly decrease if a~dynamic shape is used for
the model's input in the batch size dimension. Therefore, we will only evaluate
dynamic models in future experiments.


\section{Effect of Batch Size on Inference Speed}
\label{BatchSizeComparisonSection}

An~object detector can achieve higher inference speeds (FPS) if a~larger batch
size is used\,---\,if the detector is given more than one frame in a~single
input. Although rarely suitable for real-time object detection, a~larger batch
size enables more efficient utilization of GPU resources at the cost of higher
memory usage. This experiment investigates how the inference speed increases
with larger batch sizes used when performing inference with different models and
on different devices.

In \autoref{BatchSizeComparison} (please note the logarithmic horizontal axis),
results of tests on Raspberry Pi 4B, NVIDIA Jetson Nano and NVIDIA Jetson AGX
Xavier are shown. Although tests were run with six different batch sizes, to
make the figure more compact, only batch sizes 1, 2, 8 and 32 were selected for
comparison.

The results show that using a~larger batch size for inference does indeed often
increase the inference speed, but the increase is generally only significant
when performing inference on smaller models, where the inference speed is
higher. This is well illustrated by the NVIDIA Jetson AGX Xavier, where the
increase in inference speed with larger batch sizes is relatively subtle for the
largest detector, YOLOv8-medium $640 \times 384$. In contrast, for the smallest
model, YOLOv8-femto $352 \times 192$, raising the batch size from 1 to 32
results in an~almost ten-fold increase in inference speed. Therefore, using
larger batch sizes is generally only suitable in cases where high inference
speeds (higher than real-time) are desired, for example in a~multi-camera
real-time vehicle detection system.


% Effect of Batch Size on Inference Speed
\begin{figure}[H]
    % \begin{framed}
        \centering
        \includegraphics[width=\textwidth]{batch_size_comparison.pdf}
        \caption{Inference speeds achieved by using different batch sizes. Tests
        were conducted on Raspberry Pi 4B, NVIDIA Jetson Nano and NVIDIA Jetson
        AGX Xavier and only the most representative models were selected for
        comparison to keep the figure compact. Please note that the horizontal
        axes are logarithmic to better present the differences in inference
        speeds. Additionally, inference speed of YOLOv8-medium $640 \times 384$
        with the batch size of 32 on NVIDIA Jetson Nano is missing because the
        model did not fit into the memory.}
        \label{BatchSizeComparison}
    % \end{framed}
\end{figure}


\section{Mean Average Precision and Inference Speed Benchmark}
\label{FPSvsmAPComparison}

In this experiment, we compare the trained and quantized models in terms of
their mAP metrics (mean Average Precision) and their inference speeds in FPS.

For the tests, we wanted to choose two least-performing devices, with the
Raspberry Pi 4B being an~obvious choice. Additionally, the NVIDIA Jetson Nano
would be a~great choice, but since the device is not optimized for operations
with numbers in the \texttt{INT8} representation, NVIDIA Jetson Xavier NX was
selected instead. For the tests to be as representative as possible even for the
smallest models, tests on Jetson Xavier NX were run with a~large batch size of
32, while on the Raspberry Pi, where the batch size does not matter,
a~batch size of 1 was used instead.

Therefore two plots are presented, both comparing the mAP and FPS of all models.
On the Raspberry Pi 4B, ONNX Runtime backend was used to test the models on the
CPU. For the NVIDIA Jetson Xavier NX with TensorRT backend, the plot also
features results of models quantized to both \texttt{FP16} and \texttt{INT8}
representations and these models are connected in the plot. The mAP and FPS
comparison for the Raspberry Pi can be found in \autoref{FPSvsmAPComparisonRPi},
while the corresponding results for the Jetson Xavier NX including the quantized
models are shown in \autoref{FPSvsmAPComparisonNX}. Please note that in both
plots, a~logarithmic scale was chosen for the $x$-axis (FPS) to include all
models (large and tiny) in the figure. For a~complete benchmark of all trained
and quantized models on all devices, see \autoref{mAPvsFPS}.

% Mean Average Precision and Inference Speed Benchmark: RPI
\begin{figure}[t]
    % \begin{framed}
        \centering
        \includegraphics[width=\textwidth]{fps_vs_map_comparison_log_rpi.pdf}
        \caption{Performance comparison of all YOLOv8 models in terms of their
        mAP (mean Average Precision) and inference speed (FPS) on a~Raspberry Pi
        4B with ONNX Runtime inference backend and the batch size of 1. Please
        note that the $x$-axis is logarithmic to be able to display all models
        in a~single plot for easier analysis. We also remind the reader that
        only a~single CPU thread was used when testing models on this device.}
        \label{FPSvsmAPComparisonRPi}
    % \end{framed}
\end{figure}

An~important thing to notice is the effect of quantization on the mAP and FPS.
Although post-training quantization to \texttt{INT8} rarely seems to result in
better performance than training a~smaller model, it is clear that quantizing to
\texttt{FP16} results in a~significantly higher inference speed with no decrease
in accuracy.

% Mean Average Precision and Inference Speed Benchmark: NX
\begin{figure}[H]
    % \begin{framed}
        \centering
        \includegraphics[width=\textwidth]{fps_vs_map_comparison_log_nx.pdf}
        \caption{Performance comparison of all YOLOv8 models, including
        quantized ones, in terms of their mAP (mean Average Precision) and
        inference speed (FPS) on NVIDIA Jetson Xavier NX with TensorRT inference
        backend and the batch size of 32. Please note that the $x$-axis is
        logarithmic to be able to display all models in a~single plot for easier
        analysis.}
        \label{FPSvsmAPComparisonNX}
    % \end{framed}
\end{figure}

The performance of the YOLOv8 MobileNetV2 model can also be discussed here, as
tests on the Raspberry Pi show the superiority of the YOLOv8 model with the
MobileNetV2 backbone compared to the YOLOv8-nano models. However, tests
conducted on the NVIDIA Jetson Xavier NX with the TensorRT backend show that the
opposite is true and the MobileNetV2 brings little to no advantage when compared
to YOLOv8-small or YOLOv8-nano models on this device. To draw further
conclusions about this matter, the reader is encouraged to look at the tables in
\autoref{mAPvsFPS}, which present the complete mAP and FPS values. Additionally,
all 6 COCO mAP metrics of different models are compared in the next experiment.

In both plots resulting from this experiment, a~strange phenomenon can be seen
in which the mean Average Precision of a~model with a~higher input
resolution (YOLOv8-nano $512 \times 288$) is lower than the one of a~model
with a~lower input resolution (YOLOv8-nano $448 \times 256$). The same seems to apply for
YOLOv8-pico models, where the mAP with input resolution $448 \times 256$ is
lower (although not as significantly) than with $384 \times 224$, instead of
being higher. However, this problem does not seem to affect the YOLOv8-femto
models, of which the mAP and FPS values are as expected. We tried examining the
problem, mainly by double-checking the results and training configurations of
these models, but could not find the root cause and did not have enough
resources to conduct further experiments to provide more insights into this
phenomenon.


\section{Evaluating Models Across Multiple Mean Average Precision Metrics}

In other experiments, we compare the performance of models just by
using a~single metric\,---\,the mean Average Precision (mAP)\,---\,for objects of all
sizes and over ten IoU thresholds ranging from \num{0.50} to \num{0.95}.
However, models can also be compared using five additional, more specific mAP
metrics as explained in \autoref{EvaluationMetrics}: mAP with the IoU
threshold equal to \num{0.50}, with IoU threshold \num{0.75}, and mAP for small,
medium-sized or large objects.

In \autoref{mAPTableSmall}, we display these metrics for all trained models.
However, we omit the quantized models from the table because the mAP values tend
to stay the same after reducing precision to \texttt{FP16}, and quantizing to
\texttt{INT8} is rarely beneficial, as illustrated by the experiment in
\autoref{FPSvsmAPComparison}. For the complete table featuring all precisions of
all models, see \autoref{mAPTableBig}.

It can be observed in the results that smaller models detect large and even
medium-sized objects reasonably well. However, when it comes to detecting small
objects (of area lower than $32 \times 32$ px), their performance drops
significantly, even at higher input resolutions. However, the smaller models
might still be highly suitable for cases in which detecting small objects is not
a~priority, as they generally perform well for larger objects while being
dramatically faster.

\begin{table}[t]
    \centering
    \small
    \begin{threeparttable}
        \begin{tabular}{|c|c|rrrrrr|}
            \hline
            Model & Input & \multicolumn{6}{c|}{mAP (\%)} \\
            \cline{3-8}
            (YOLOv8)                     & resolution                       & mAP & IoU:50& IoU:75& small & medium& large \\
            \hline
            \hline
            \multirow{1}{*}{medium}      & \multirow{1}{*}{$640\times384$}  & 60.3 & 80.5 & 69.5 & 31.6 & 60.6 & 77.0 \\
            \hline                                                          
            \multirow{1}{*}{small}       & \multirow{1}{*}{$640\times384$}  & 57.8 & 78.6 & 66.0 & 27.3 & 58.5 & 74.4 \\
            \hline
            \multirow{1}{*}{MobileNetV2} & \multirow{1}{*}{$512\times288$}  & 54.8 & 75.9 & 61.8 & 23.5 & 55.0 & 71.5 \\
            \hline                      
            \multirow{3}{*}{nano}        & \multirow{1}{*}{$640\times384$}  & 53.0 & 74.9 & 60.8 & 22.9 & 52.9 & 68.5 \\
            \cline{2-2}
                                         & \multirow{1}{*}{$512\times288$}  & 49.2 & 69.9 & 56.9 & 21.0 & 48.3 & 67.3 \\
            \cline{2-2}
                                         & \multirow{1}{*}{$448\times256$}  & 51.1 & 72.3 & 59.2 & 20.6 & 50.8 & 68.1 \\
            \hline
            \multirow{3}{*}{pico}        & \multirow{1}{*}{$512\times288$}  & 45.4 & 66.4 & 50.2 & 15.1 & 44.8 & 61.4 \\
            \cline{2-2}
                                         & \multirow{1}{*}{$448\times256$}  & 41.5 & 62.1 & 46.8 & 13.6 & 40.8 & 55.2 \\
            \cline{2-2}
                                         & \multirow{1}{*}{$384\times224$}  & 41.7 & 61.5 & 46.5 & 12.8 & 40.5 & 58.9 \\
            \hline
            \multirow{4}{*}{femto}       & \multirow{1}{*}{$512\times288$}  & 31.8 & 48.8 & 35.1 &  7.2 & 29.8 & 41.6 \\
            \cline{2-2}
                                         & \multirow{1}{*}{$448\times256$}  & 30.0 & 47.7 & 31.8 &  7.0 & 25.9 & 42.9 \\
            \cline{2-2}
                                         & \multirow{1}{*}{$384\times224$}  & 28.1 & 45.2 & 30.2 &  7.0 & 26.3 & 38.6 \\
            \cline{2-2}
                                         & \multirow{1}{*}{$352\times192$}  & 25.9 & 43.0 & 27.5 &  5.4 & 22.7 & 37.0 \\
            \hline
        \end{tabular}
        % \caption{Displaying multiple mean Average Precision metrics for each trained
        % model and input resolution. The base mAP is measured over ten IoU
        % (Intersection over Union) thresholds ranging from \num{0.50} to \num{0.95}
        % with step \num{0.05}.  The IoU:50 and IoU:75 metrics are calculated
        % similarly, but only over one IoU threshold (\num{0.50} and \num{0.75}
        % respectively). Additionally, mAP for small, medium-sized and large objects
        % is simply calculated for objects of the respective sizes, where a small
        % object is a one with area lower than $32 \times 32$, a medium-sized object
        % is with area higher than $32 \times 32$ but lower than $96 \times 96$, while
        % a large object is a one with an~area higher than $96 \times 96$.}
        \caption{Comparison of multiple mean Average Precision metrics for each
        trained model and input resolution. The mAP metrics displayed in this table
        are explained in \autoref{EvaluationMetrics}.}
        \label{mAPTableSmall}
    \end{threeparttable}
\end{table}


\section{Inference Speeds on Different Devices}

In this experiment, we benchmark all available devices by measuring the
inference speeds of all model architectures. For each architecture, only one
input resolution was selected, however, differently than in the other
experiments: YOLOv8-nano $512 \times 288$ was selected to better compare with
YOLOv8 MobileNetV2, and YOLOv8-femto $352 \times 192$ was chosen to include the
smallest model. A~second set of measurements was created to only benchmark
models quantized to the \texttt{FP16} representation on the NVIDIA Jetson
devices. Both plots can be seen in \autoref{DevicesBenchmark}. Please note that
the $x$-axis is in a~logarithmic scale in both plots because of the performance
differences between individual devices.

We decided to measure the inference speed with higher batch sizes to get the
highest possible inference speeds possible, but since larger models do not fit
into memory on
lower-performing devices when a~large batch size was
used\footnote{List of tests that failed because the model could not fit into
the memory of a~device when performing inference with a~specific batch size:
YOLOv8-medium on NVIDIA Jetson Nano with batch size 32, YOLOv8-medium on NVIDIA
MX150 with batch sizes 16 and 32, YOLOv8-small on NVIDIA MX150 with batch size
32.}, we could not benchmark these models with the largest batch size of 32.
However, thanks to insights provided by \autoref{BatchSizeComparisonSection}, we
consider it safe to use a~smaller batch size of 8 for YOLOv8-medium,
YOLOv8-small and YOLOv8 MobileNetV2 on all devices. For the rest of the models,
a~batch size of 32 was used. This applies to both plots resulting from this
experiment.

What's interesting to see from the first plot is that the NVIDIA Jetson Nano is
comparable to the NVIDIA MX150, while consuming just a~fraction of the power.
Additionally, there is a~significant difference between the performance of the
Jetson Nano compared to the Jetson Xavier NX, while the inference speeds measured
on the Jetson AGX Xavier are only slightly higher than on the Jetson Xavier NX.

Another notable difference lies in how weight quantization to the \texttt{FP16}
number representation increases the inference speeds on NVIDIA Jetson devices.
The highest difference in FPS between \texttt{FP32} and \texttt{FP16} on Jetson
Nano was for the YOLOv8-small model, where the inference speed after
quantization increased by \num{50.13} \% from \num{11.19} FPS to \num{16.79}
FPS. On the other hand, the inference speed of the same model on Jetson Xavier
NX increased by \num{165.84} \% from \num{44.12} FPS to \num{117.29} FPS.
Furthermore, the difference is even higher for YOLOv8-medium, where the
quantization to \texttt{FP16} resulted in a~\num{228.33} \% increase in
inference speed. However, it is important to note that for smaller models, these
gains are lower and more comparable between the two devices.

For a~complete table of inference speeds of all models (including quantized
ones) on all devices with a~batch size of 1, please see \autoref{mAPvsFPS1} in
the appendix.

\begin{figure}[H]
    % \begin{framed}
        \centering
        \includegraphics[width=\textwidth]{devices_benchmark.pdf}
        \includegraphics[width=\textwidth]{devices_benchmark_FP16.pdf}
        \caption{Benchmark of inference speeds (FPS) on different devices
        featuring all model architectures. The second plot compares models
        quantized to \texttt{FP16} precision on NVIDIA Jetson devices. Please
        note that in both subplots, the $x$-axis is logarithmic and that for
        comparison, YOLOv8-nano model with $512 \times 288$ input resolution and
        YOLOv8-femto model with $352 \times 192$ input resolution were selected.
        A~batch size of 8 was selected for tests featuring larger
        models\,--\,YOLOv8-medium, YOLOv8-small and YOLOv8 MobileNetV2\,---\,and
        for the rest of the models, a~batch size of 32 was used. The TensorRT
        inference backend was utilized when testing on NVIDIA Jetson devices,
        while the ONNX Runtime backend was used on the rest of the devices.
        Additionally, we remind the reader that for the tests on CPUs, only
        a~single thread was used, utilizing just one of the CPU's cores.}
        \label{DevicesBenchmark}
    % \end{framed}
\end{figure}




\chapter{Conclusion}

In this paper, we have demonstrated the potential of the state-of-the-art YOLOv8
object detector for vehicle detection on various embedded devices, with a~focus
on optimizing the models for real-time performance. Our experiments involved
training several models of different sizes on a~large, diverse dataset of
surveillance-type images.

We have shown that the entry-level embedded device NVIDIA Jetson Nano can run
reasonably accurate vehicle detectors like the YOLOv8-nano in real-time. On
higher-performing devices, namely the Jetson Xavier NX or Jetson AGX Xavier,
even the largest of the trained models\,---\,YOLOv8-medium\,---\,operated at high
frame rates when quantized to \texttt{FP16} precision. We found that on these
devices, quantizing models to \texttt{FP16} results in higher inference speeds
with no real impact on accuracy, while quantizing to \texttt{INT8} rarely brings
an~advantage over using a~smaller model.

In contrast, the Raspberry Pi 4B can only run real-time inference of the
smallest models, which exhibit inferior performance, particularly when detecting
small objects. Therefore, vehicle detection on this device is only viable in
special cases where the accuracy requirements are not as high and the vehicles
present in the input images have a~large area.

To compare these embedded devices with processing units commonly found in
laptops or desktop computers, we also benchmarked the models on the Intel Core
i7-9850H CPU and the NVIDIA MX150 GPU. Additionally, we studied the effects of
other factors that influence the accuracy and inference speed of the models,
including different input resolutions, batch sizes during inference, and the
differences between the ONNX Runtime and TensorRT inference backends on NVIDIA
Jetson devices.

In conclusion, our study successfully demonstrates the real-time capabilities of
state-of-the-art vehicle detectors on various embedded devices, including the
popular NVIDIA Jetson Nano and the low-performance Raspberry Pi 4B. Our research
provides valuable insights for practical applications in the field of vehicle
detection and establishes a~foundation for future work aimed at refining object
detection models to run on embedded systems.




\chapter{Future Work}
\label{FutureWork}

In future studies, we suggest experimenting with additional network optimization
and compression techniques, such as knowledge distillation and network pruning.
These methods have the potential to improve the efficiency and accuracy of
object detection models when deployed on low-performance devices.

Additionally, since this paper focuses on embedded devices specifically designed
for machine learning, it is essential to investigate further optimizations to
draw more comprehensive conclusions about vehicle detection on devices like the
Raspberry Pi 4B. This includes examining the effect of weight quantization on
inference speeds on CPUs and exploring the use of specialized inference frameworks
such as NCNN, ARM NN, or TensorFlow Lite.

Furthermore, to enable a~more extensive comparison with related research papers
in the field, future studies may also consider benchmarking other object
detection models, such as the Single Shot MultiBox Detector (SSD) and earlier
YOLO versions like the YOLOX. This would also demonstrate how the recent
advances in object detection contribute to the development of more efficient and
accurate vehicle detection systems.




\begin{appendices}
\label{Appendix} % Won't work!


\chapter{Additional Precision-Recall Curves}
\label{PRCurvesAdditional}

Additional precision-recall curves, which were not included in
\autoref{PRCurveMajor}, are shown here, in \autoref{AdditionalPR} for
completeness.

\begin{figure}[H]
    % \begin{framed}
        \centering
        \includegraphics[width=\textwidth]{pr_curve_additional.pdf}
        \caption{Precision-Recall curves of the models which were not included
        in \autoref{PRCurveMajor}.}
        \label{AdditionalPR}
    % \end{framed}
\end{figure}


\chapter{Complete Benchmark of All Models on All Devices}
\label{mAPvsFPS}

In this appendix, the complete inference speeds of all trained and quantized
models on all six devices utilized in this paper are provided. Two tables are
shown\,---\,one for tests that were run with a~batch size of 1
(\autoref{mAPvsFPS1}) and one with a~batch size of 32 (\autoref{mAPvsFPS32}).
However, not all models could be benchmarked on all devices with the large batch
size, so for tests of YOLOv8-medium and YOLOv8-small on NVIDIA MX150 and
YOLOv8-medium on NVIDIA Jetson Nano, a~batch size of 8 was used instead. This,
however, shouldn't significantly affect the values, as demonstrated by
\autoref{BatchSizeComparisonSection}.

The tests were run using the TensorRT inference backend on NVIDIA Jetson
devices, while for the rest of the devices, ONNX Runtime was used. Additionally,
weight quantization was only performed using the TensorRT backend, while the
\texttt{INT8} precision was not used on NVIDIA Jetson Nano. Furthermore, we
remind the reader that on CPUs, only a~single CPU thread was utilized when using
the ONNX Runtime inference backend.

% Batch 1
\begin{table}[h]
    \footnotesize
    \centering
    \begin{threeparttable}
        \begin{tabular}{|c|c|c|c|rrrrrr|}
            \hline
            \multirow{4}{*}{\parbox{1.6cm}{\centering Model\\(YOLOv8)}} &
            \multirow{4}{*}{\parbox{1.5cm}{\centering Input\\Resolution}} &
                \multirow{4}{*}{\parbox{0.9cm}{\centering Preci-\\sion}} &
                \multirow{4}{*}{\parbox{0.8cm}{\centering mAP\\(\%)}} &
                    \multicolumn{6}{c|}{Inference Speed with a~batch size of 1 (FPS)} \\
            \cline{5-10}
            & & & & Rasp- & Intel   &\multirow{3}{*}{\parbox{1.2cm}{\raggedleft NVIDIA\\MX150}} & \multicolumn{3}{|c|}{NVIDIA Jetson} \\
            \cline{8-10}
            & & & & berry & Core i7 &                                                           & \multicolumn{1}{|c}{\multirow{2}{*}{Nano}} & Xavier & AGX \\
            & & & & Pi    & 9850H   &                                                           & \multicolumn{1}{|c}{}                      & NX     & Xavier \\
            \hline
            \hline
            \multirow{3}{*}{medium}                                   & \multirow{3}{*}{$640 \times 384$} & FP32 & 60.3 &  0.12 &   1.62 &  4.04 &  4.15 & 16.36 &  17.48 \\
                                                                    &                                   & FP16 & 60.3 &    -- &     -- &    -- &  6.82 & 47.49 &  49.29 \\
                                                                    &                                   & INT8 & 47.1 &    -- &     -- &    -- &    -- & 48.62 &  53.37 \\
            \hline
            \multirow{3}{*}{small}                                    & \multirow{3}{*}{$640 \times 384$} & FP32 & 57.8 &  0.34 &   5.12 &  9.54 & 10.91 & 39.48 &  46.18 \\
                                                                    &                                   & FP16 & 57.8 &    -- &     -- &    -- & 16.27 & 52.01 &  57.44 \\
                                                                    &                                   & INT8 & 54.4 &    -- &     -- &    -- &    -- & 70.68 &  64.25 \\
            \hline
            \multirow{3}{*}{\parbox{1.6cm}{\centering MobileNet\\V2}} & \multirow{3}{*}{$512 \times 288$} & FP32 & 54.8 &  1.19 &  15.82 & 15.34 & 18.55 & 51.59 &  66.72 \\
                                                                    &                                   & FP16 & 54.8 &    -- &     -- &    -- & 19.93 & 53.30 &  58.66 \\
                                                                    &                                   & INT8 & 45.9 &    -- &     -- &    -- &    -- & 69.95 &  55.24 \\
            \hline
            \multirow{9}{*}{nano}                                     & \multirow{3}{*}{$640 \times 384$} & FP32 & 53.0 &  1.02 &  14.78 & 20.40 & 25.08 & 54.30 &  57.50 \\
                                                                    &                                   & FP16 & 53.0 &    -- &     -- &    -- & 31.90 & 63.13 &  55.33 \\
                                                                    &                                   & INT8 & 43.0 &    -- &     -- &    -- &    -- & 59.78 &  64.21 \\
            \cline{2-10}
                                                                    & \multirow{3}{*}{$512 \times 288$} & FP32 & 49.2 &  1.70 &  23.67 & 30.58 & 37.56 & 57.25 &  63.53 \\
                                                                    &                                   & FP16 & 49.2 &    -- &     -- &    -- & 48.29 & 70.01 &  71.38 \\
                                                                    &                                   & INT8 & 35.6 &    -- &     -- &    -- &    -- & 81.91 &  83.87 \\
            \cline{2-10}
                                                                    & \multirow{3}{*}{$448 \times 256$} & FP32 & 51.1 &  2.18 &  29.55 & 36.21 & 49.25 & 67.73 &  58.17 \\
                                                                    &                                   & FP16 & 51.1 &    -- &     -- &    -- & 54.04 & 77.80 &  83.35 \\
                                                                    &                                   & INT8 & 37.2 &    -- &     -- &    -- &    -- & 61.81 &  91.60 \\
            \hline
            \multirow{9}{*}{pico}                                     & \multirow{3}{*}{$512 \times 288$} & FP32 & 45.4 &  4.23 &  50.48 & 43.96 & 54.02 & 69.78 &  71.11 \\
                                                                    &                                   & FP16 & 45.4 &    -- &     -- &    -- & 57.14 & 59.80 &  87.29 \\
                                                                    &                                   & INT8 & 29.7 &    -- &     -- &    -- &    -- & 64.16 &  94.32 \\
            \cline{2-10}
                                                                    & \multirow{3}{*}{$448 \times 256$} & FP32 & 41.5 &  5.39 &  61.38 & 52.89 & 57.37 & 78.47 &  81.29 \\
                                                                    &                                   & FP16 & 41.5 &    -- &     -- &    -- & 59.12 & 66.16 &  98.80 \\
                                                                    &                                   & INT8 & 35.6 &    -- &     -- &    -- &    -- & 71.28 & 101.98 \\
            \cline{2-10}
                                                                    & \multirow{3}{*}{$384 \times 224$} & FP32 & 41.7 &  7.07 &  77.38 & 59.04 & 60.39 & 62.16 &  88.68 \\
                                                                    &                                   & FP16 & 41.8 &    -- &     -- &    -- & 61.85 & 77.94 & 108.55 \\
                                                                    &                                   & INT8 & 32.5 &    -- &     -- &    -- &    -- & 80.42 & 113.24 \\
            \hline
            \multirow{12}{*}{femto}                                   & \multirow{3}{*}{$512 \times 288$} & FP32 & 31.8 &  7.11 &  62.40 & 55.16 & 58.78 & 55.81 &  84.13 \\
                                                                    &                                   & FP16 & 31.8 &    -- &     -- &    -- & 60.28 & 91.62 &  93.83 \\
                                                                    &                                   & INT8 & 30.0 &    -- &     -- &    -- &    -- & 65.07 &  97.91 \\
            \cline{2-10}
                                                                    & \multirow{3}{*}{$448 \times 256$} & FP32 & 30.0 &  9.03 &  81.19 & 63.29 & 61.44 & 62.33 &  95.11 \\
                                                                    &                                   & FP16 & 30.0 &    -- &     -- &    -- & 58.12 & 70.06 & 104.58 \\
                                                                    &                                   & INT8 & 26.4 &    -- &     -- &    -- &    -- & 72.38 & 107.26 \\
            \cline{2-10}
                                                                    & \multirow{3}{*}{$384 \times 224$} & FP32 & 28.1 & 11.87 & 103.49 & 76.33 & 66.01 & 72.29 & 103.47 \\
                                                                    &                                   & FP16 & 28.1 &    -- &     -- &    -- & 57.66 & 82.98 & 113.06 \\
                                                                    &                                   & INT8 & 23.5 &    -- &     -- &    -- &    -- & 83.54 & 115.56 \\
            \cline{2-10}
                                                                    & \multirow{3}{*}{$352 \times 192$} & FP32 & 25.9 & 14.82 & 121.83 & 84.24 & 64.84 & 78.18 & 109.02 \\
                                                                    &                                   & FP16 & 25.9 &    -- &     -- &    -- & 65.91 & 89.41 & 120.63 \\
                                                                    &                                   & INT8 & 21.4 &    -- &     -- &    -- &    -- & 88.15 & 123.08 \\
            \hline
        \end{tabular}
        \caption{Benchmark of all trained and quantized YOLOv8 models on all
        available devices. For these tests, a~batch size of 1 was used and all
        models were of a~dynamic shape in the batch size dimension. Additionally,
        the TensorRT inference backend was used on NVIDIA Jetson devices, while the
        ONNX Runtime backend was used for tests on the rest of the devices. Models
        were only quantized to \texttt{FP16} precision on NVIDIA Jetson devices and
        \texttt{INT8} precision was only used on Jetson Xavier NX and Jetson AGX
        Xavier.}
        \label{mAPvsFPS1}
    \end{threeparttable}
    \normalsize
\end{table}

% Batch 32
\begin{table}
    \footnotesize
    \centering
    \begin{threeparttable}
        \begin{tabular}{|c|c|c|c|rrrrrr|}
            \hline
            \multirow{4}{*}{\parbox{1.6cm}{\centering Model\\(YOLOv8)}} &
            \multirow{4}{*}{\parbox{1.5cm}{\centering Input\\Resolution}} &
                \multirow{4}{*}{\parbox{0.9cm}{\centering Preci-\\sion}} &
                \multirow{4}{*}{\parbox{0.8cm}{\centering mAP\\(\%)}} &
                    \multicolumn{6}{c|}{Inference Speed with a~batch size of 32* (FPS)} \\
            \cline{5-10}
            & & & & Rasp- & Intel   &\multirow{3}{*}{\parbox{1.2cm}{\raggedleft NVIDIA\\MX150}} & \multicolumn{3}{|c|}{NVIDIA Jetson} \\
            \cline{8-10}
            & & & & berry & Core i7 &                                                           & \multicolumn{1}{|c}{\multirow{2}{*}{Nano}} & Xavier & AGX \\
            & & & & Pi    & 9850H   &                                                           & \multicolumn{1}{|c}{}                      & NX     & Xavier \\
            \hline
            \hline
            \multirow{3}{*}{medium}                                   & \multirow{3}{*}{$640 \times 384$} & FP32 & 60.3 &  0.12 &   1.57 &  4.68* &  4.15* &   16.97 &   19.11 \\
                                                                    &                                   & FP16 & 60.3 &    -- &     -- &     -- &   6.96 &   58.20 &   63.91 \\
                                                                    &                                   & INT8 & 47.1 &    -- &     -- &     -- &     -- &   86.30 &  102.17 \\
            \hline
            \multirow{3}{*}{small}                                    & \multirow{3}{*}{$640 \times 384$} & FP32 & 57.8 &  0.34 &   4.98 & 10.70* &  11.23 &   44.73 &   54.78 \\
                                                                    &                                   & FP16 & 57.8 &    -- &     -- &     -- &  16.79 &  121.36 &  140.62 \\
                                                                    &                                   & INT8 & 54.4 &    -- &     -- &     -- &     -- &  191.18 &  206.82 \\
            \hline
            \multirow{3}{*}{\parbox{1.6cm}{\centering MobileNet\\V2}} & \multirow{3}{*}{$512 \times 288$} & FP32 & 54.8 &  1.19 &  15.46 &  17.17 &  20.36 &   89.98 &  108.48 \\
                                                                    &                                   & FP16 & 54.8 &    -- &     -- &     -- &  22.00 &  159.10 &  185.10 \\
                                                                    &                                   & INT8 & 45.9 &    -- &     -- &     -- &     -- &  261.10 &  290.35 \\
            \hline
            \multirow{9}{*}{nano}                                     & \multirow{3}{*}{$640 \times 384$} & FP32 & 53.0 &  1.03 &  14.38 &  21.22 &  26.30 &  115.28 &  139.49 \\
                                                                    &                                   & FP16 & 53.0 &    -- &     -- &     -- &  33.69 &  205.36 &  251.42 \\
                                                                    &                                   & INT8 & 43.0 &    -- &     -- &     -- &     -- &  264.79 &  305.43 \\
            \cline{2-10}
                                                                    & \multirow{3}{*}{$512 \times 288$} & FP32 & 49.2 &  1.70 &  23.71 &  40.13 &  41.86 &  192.31 &  219.17 \\
                                                                    &                                   & FP16 & 49.2 &    -- &     -- &     -- &  54.16 &  328.51 &  390.02 \\
                                                                    &                                   & INT8 & 35.6 &    -- &     -- &     -- &     -- &  416.75 &  466.65 \\
            \cline{2-10}
                                                                    & \multirow{3}{*}{$448 \times 256$} & FP32 & 51.1 &  2.21 &  30.16 &  51.74 &  55.73 &  246.03 &  283.35 \\
                                                                    &                                   & FP16 & 51.1 &    -- &     -- &     -- &  71.49 &  409.57 &  483.80 \\
                                                                    &                                   & INT8 & 37.2 &    -- &     -- &     -- &     -- &  524.52 &  575.99 \\
            \hline
            \multirow{9}{*}{pico}                                     & \multirow{3}{*}{$512 \times 288$} & FP32 & 45.4 &  4.30 &  53.92 &  67.29 &  70.51 &  338.69 &  378.36 \\
                                                                    &                                   & FP16 & 45.4 &    -- &     -- &     -- &  83.02 &  457.63 &  523.29 \\
                                                                    &                                   & INT8 & 29.7 &    -- &     -- &     -- &     -- &  511.57 &  596.65 \\
            \cline{2-10}
                                                                    & \multirow{3}{*}{$448 \times 256$} & FP32 & 41.5 &  5.48 &  68.66 &  83.70 &  92.82 &  432.56 &  484.74 \\
                                                                    &                                   & FP16 & 41.5 &    -- &     -- &     -- & 107.79 &  563.96 &  641.45 \\
                                                                    &                                   & INT8 & 35.6 &    -- &     -- &     -- &     -- &  631.91 &  740.18 \\
            \cline{2-10}
                                                                    & \multirow{3}{*}{$384 \times 224$} & FP32 & 41.7 &  7.18 &  89.89 & 105.58 & 118.47 &  564.88 &  648.23 \\
                                                                    &                                   & FP16 & 41.8 &    -- &     -- &     -- & 138.83 &  763.25 &  888.08 \\
                                                                    &                                   & INT8 & 32.5 &    -- &     -- &     -- &     -- &  878.58 & 1016.77 \\
            \hline
            \multirow{12}{*}{femto}                                   & \multirow{3}{*}{$512 \times 288$} & FP32 & 31.8 &  7.10 &  70.84 &  85.43 &  87.42 &  440.21 &  474.64 \\
                                                                    &                                   & FP16 & 31.8 &    -- &     -- &     -- &  98.72 &  527.16 &  579.90 \\
                                                                    &                                   & INT8 & 30.0 &    -- &     -- &     -- &     -- &  582.56 &  589.16 \\
            \cline{2-10}
                                                                    & \multirow{3}{*}{$448 \times 256$} & FP32 & 30.0 &  9.04 &  93.97 & 106.62 & 113.09 &  556.71 &  600.81 \\
                                                                    &                                   & FP16 & 30.0 &    -- &     -- &     -- & 127.27 &  656.74 &  682.84 \\
                                                                    &                                   & INT8 & 26.4 &    -- &     -- &     -- &     -- &  679.27 &  726.82 \\
            \cline{2-10}
                                                                    & \multirow{3}{*}{$384 \times 224$} & FP32 & 28.1 & 11.92 & 125.96 & 133.62 & 145.79 &  732.24 &  813.68 \\
                                                                    &                                   & FP16 & 28.1 &    -- &     -- &     -- & 158.38 &  843.47 & 1004.03 \\
                                                                    &                                   & INT8 & 23.5 &    -- &     -- &     -- &     -- & 1009.65 & 1034.69 \\
            \cline{2-10}
                                                                    & \multirow{3}{*}{$352 \times 192$} & FP32 & 25.9 & 14.80 & 145.94 & 158.72 & 175.98 &  878.53 &  968.72 \\
                                                                    &                                   & FP16 & 25.9 &    -- &     -- &     -- & 197.29 & 1040.57 & 1152.97 \\
                                                                    &                                   & INT8 & 21.4 &    -- &     -- &     -- &     -- & 1117.36 & 1237.66 \\
            \hline
            \end{tabular}
        \caption{Benchmark of all trained and quantized YOLOv8 models on all
        available devices. For these tests, a~batch size of 32 was used. However,
        for special cases marked by \texttt{*}, where the model did not fit into the
        device's memory with the large batch size, a~batch size of 8 was used
        instead. Models tested were of a~dynamic shape in the batch size dimension.
        Additionally, the TensorRT inference backend was used on NVIDIA Jetson
        devices, while the ONNX Runtime backend was used for tests on the rest of
        the devices. Models were only quantized to \texttt{FP16} precision on NVIDIA
        Jetson devices and \texttt{INT8} precision was only used on Jetson Xavier NX
        and Jetson AGX Xavier.}
        \label{mAPvsFPS32}
    \end{threeparttable}
    \normalsize
\end{table}


\chapter{Complete Mean Average Precision Metrics for YOLOv8 Models}
\label{mAPTableBig}

This appendix provides a~complete table of the six mean Average Precision
(mAP) metrics for each model trained and quantized in this paper. The data can
be seen in \autoref{mAPTableBigFig}. Please note that the quantization process
was performed by the TensorRT library individually for each of the utilized
NVIDIA Jetson devices. The results shown in the table were measured on the
NVIDIA Jetson AGX Xavier.

\begin{table}[t]
    \centering
    \small
    \begin{threeparttable}
        \begin{tabular}{|c|c|c|rrrrrr|}
            \hline
            Model & Input & \multirow{2}{*}{Precision} &  \multicolumn{6}{c|}{mAP (\%)} \\
            \cline{4-9}
            (YOLOv8)                     & resolution                      &      & mAP   & IoU:50& IoU:75& small & medium& large \\
            \hline
            \hline
            \multirow{3}{*}{medium}     & \multirow{3}{*}{$640\times384$}  & FP32 & 60.3 & 80.5 & 69.5 & 31.6 & 60.6 & 77.0 \\
                                        &                                  & FP16 & 60.3 & 80.5 & 69.3 & 31.6 & 60.7 & 77.0 \\
                                        &                                  & INT8 & 47.1 & 67.2 & 55.6 & 16.4 & 47.6 & 66.3 \\
            \hline                                                           
            \multirow{3}{*}{small}      & \multirow{3}{*}{$640\times384$}  & FP32 & 57.8 & 78.6 & 66.0 & 27.3 & 58.5 & 74.4 \\
                                        &                                  & FP16 & 57.8 & 78.6 & 66.0 & 27.4 & 58.5 & 74.2 \\
                                        &                                  & INT8 & 54.4 & 76.3 & 62.5 & 24.5 & 54.9 & 70.3 \\
            \hline
            \multirow{3}{*}{MobileNetV2}& \multirow{3}{*}{$512\times288$}  & FP32 & 54.8 & 75.9 & 61.8 & 23.5 & 55.0 & 71.5 \\
                                        &                                  & FP16 & 54.8 & 75.9 & 61.9 & 23.6 & 54.9 & 71.8 \\
                                        &                                  & INT8 & 45.9 & 65.5 & 52.3 & 21.2 & 45.3 & 60.7 \\
            \hline                      
            \multirow{9}{*}{nano}       & \multirow{3}{*}{$640\times384$}  & FP32 & 53.0 & 74.9 & 60.8 & 22.9 & 52.9 & 68.5 \\
                                        &                                  & FP16 & 53.0 & 74.9 & 60.7 & 23.0 & 52.9 & 68.3 \\
                                        &                                  & INT8 & 43.0 & 65.0 & 48.4 & 18.2 & 45.3 & 53.0 \\
            \cline{2-9}
                                        & \multirow{3}{*}{$512\times288$}  & FP32 & 49.2 & 69.9 & 56.9 & 21.0 & 48.3 & 67.3 \\
                                        &                                  & FP16 & 49.2 & 69.8 & 56.9 & 20.9 & 48.2 & 66.9 \\
                                        &                                  & INT8 & 35.6 & 53.5 & 39.8 & 15.0 & 35.9 & 50.0 \\
            \cline{2-9}
                                        & \multirow{3}{*}{$448\times256$}  & FP32 & 51.1 & 72.3 & 59.2 & 20.6 & 50.8 & 68.1 \\
                                        &                                  & FP16 & 51.1 & 72.3 & 59.2 & 20.5 & 50.8 & 68.3 \\
                                        &                                  & INT8 & 37.2 & 55.5 & 41.7 & 14.0 & 37.1 & 51.6 \\
            \hline
            \multirow{9}{*}{pico}       & \multirow{3}{*}{$512\times288$}  & FP32 & 45.4 & 66.4 & 50.2 & 15.1 & 44.8 & 61.4 \\
                                        &                                  & FP16 & 45.4 & 66.4 & 50.0 & 15.1 & 44.8 & 61.2 \\
                                        &                                  & INT8 & 29.7 & 44.6 & 32.9 &  6.5 & 26.0 & 49.7 \\
            \cline{2-9}
                                        & \multirow{3}{*}{$448\times256$}  & FP32 & 41.5 & 62.1 & 46.8 & 13.6 & 40.8 & 55.2 \\
                                        &                                  & FP16 & 41.5 & 62.1 & 46.8 & 13.6 & 40.9 & 55.1 \\
                                        &                                  & INT8 & 35.6 & 55.2 & 39.6 & 12.2 & 35.5 & 47.4 \\
            \cline{2-9}
                                        & \multirow{3}{*}{$384\times224$}  & FP32 & 41.7 & 61.5 & 46.5 & 12.8 & 40.5 & 58.9 \\
                                        &                                  & FP16 & 41.8 & 61.5 & 46.5 & 12.8 & 40.5 & 58.8 \\
                                        &                                  & INT8 & 32.5 & 49.0 & 36.1 & 10.1 & 30.8 & 48.6 \\
            \hline
            \multirow{12}{*}{femto}     & \multirow{3}{*}{$512\times288$}  & FP32 & 31.8 & 48.8 & 35.1 &  7.2 & 29.8 & 41.6 \\
                                        &                                  & FP16 & 31.8 & 48.8 & 35.0 &  7.2 & 29.8 & 41.7 \\
                                        &                                  & INT8 & 30.0 & 46.6 & 32.6 &  7.1 & 27.0 & 42.0 \\
            \cline{2-9}
                                        & \multirow{3}{*}{$448\times256$}  & FP32 & 30.0 & 47.7 & 31.8 &  7.0 & 25.9 & 42.9 \\
                                        &                                  & FP16 & 30.0 & 47.7 & 31.9 &  7.0 & 25.9 & 43.0 \\
                                        &                                  & INT8 & 26.4 & 41.3 & 28.7 &  6.1 & 21.9 & 40.4 \\
            \cline{2-9}
                                        & \multirow{3}{*}{$384\times224$}  & FP32 & 28.1 & 45.2 & 30.2 &  7.0 & 26.3 & 38.6 \\
                                        &                                  & FP16 & 28.1 & 45.3 & 30.2 &  7.0 & 26.4 & 38.7 \\
                                        &                                  & INT8 & 23.5 & 40.9 & 24.2 &  5.9 & 21.1 & 33.1 \\
            \cline{2-9}
                                        & \multirow{3}{*}{$352\times192$}  & FP32 & 25.9 & 43.0 & 27.5 &  5.4 & 22.7 & 37.0 \\
                                        &                                  & FP16 & 25.9 & 43.0 & 27.5 &  5.4 & 22.6 & 37.0 \\
                                        &                                  & INT8 & 21.4 & 34.8 & 23.5 &  4.4 & 18.2 & 32.3 \\
            \hline
        \end{tabular}
        \caption{Comparison of multiple mean Average Precision metrics for each
        trained model and input resolution, including models quantized to
        \texttt{FP16} and \texttt{INT8}. Results were measured on NVIDIA Jetson AGX
        Xavier.}
        \label{mAPTableBigFig}
    \end{threeparttable}
    \normalsize
\end{table}


\end{appendices}

% For compilation piecewise (see projekt.tex), it is necessary to uncomment it
% \end{document}
