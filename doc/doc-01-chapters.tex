% For compilation piecewise (see projekt.tex), it is necessary to uncomment it and change
% \documentclass[../projekt.tex]{subfiles}
% \begin{document}


% - Research pre viac aplikácií - napr. RPI s mini sieťou na riadenie križovatky
%   a congestion reduction, lebo tam netreba veľkú presnosť, a napr. jetson na
%   detekciu wrong-way a takých, a možno napr. colar na niečo medzi 

% TODO TODO vo fitthesis.cls nájsť riadok s tocdepth a zmeniť na 1
% TODO prvé výskyty skratiek a iných neznámych slov vysvetliť

% TODO mAP dávať v % a písať s '%'?

% TODO písať že NMS IoU threshold je 0.7

% TODO Správne písať et al.

% TODO napísať aký confidence threshold som použil - že som použil prakticky
% nula lebo to dávalo lepšie mAP aj keď viac false detections ale že na tom
% nezáleží lebo všetky modely boli testované resp. deploynuté s rovnakým
% thresholdom
% TODO TODO Ale POZOR: veď mAP sa kalkuluje s rôznymi thresholdmi, čiže
% threshold 0.001 pri deployi by mal byť v práve správne. Inak by asi boli
% predikcie s nižším skóre odfiltrované ešte pred post-processingom?
% - pýtal som sa 02.05. Špaňhela, tak počkať na odpoveď


\newpage
\chapter{Introduction}

TODO TODO TODO

% TODO malo by to asi byť viac o tom, čo budem v papieri písať

% TODO povedať že sa zameriavam na detekciu zo surveillance-type snímkov či videí

% Vehicle detection in real-time is crucial for enhancing traffic safety and flow.
% It can be used to manage traffic at an intersection using traffic lights, spot
% speeding or wrong-way drivers, enhance route planning to ease congestion, alert
% drivers to potentially dangerous situations, and offer insightful information
% about driving behavior.

% There are various methods for detecting vehicles, each of which has
% its advantages and limitations. In this paper, we focus on detection using a
% camera-based traffic surveillance system. This approach was chosen because of
% its low price, versatility, and ease of installation.

% There are two main techniques for vehicle detection using a camera: image
% processing and convolutional neural networks (CNNs). Both approaches are based
% on analyzing images to identify patterns and features to detect vehicles. Image
% processing techniques involve applying a series of pre-defined filters and
% transformations to an image to extract relevant information. CNNs, on the other
% hand, are a type of machine learning algorithm that learns to recognize patterns
% and features through training. CNNs are generally more effective than image
% processing techniques for vehicle detection, as they can learn to recognize
% complex patterns and features that may be difficult to extract using pre-defined
% filters and transformations, but they have traditionally required a significant
% amount of computing power, commonly provided by a graphical processing
% unit (GPU). However, recent developments suggest that it may now be possible to
% perform CNN-based tasks in real time with reduced processing requirements. 

% In this paper, we attempt to train a CNN with a smaller architecture that is
% able to run on a single central processing unit (CPU) instead of a GPU. This
% would allow the system to process the camera feed on-site, rather than relying
% on its transmission over the internet and remote processing. This would reduce
% the cost of the system and the amount of network bandwidth required, as well as
% expand the range of purposes for which the system can be utilized.



\chapter{Background and Related Work}


\section{Common Approaches to Vehicle Detection}

Camera-based object detectors based on convolutional neural networks are
evaluated in this paper. However, to provide further context to the problem,
other commonly used solutions for the problem of vehicle detection are briefly
explained in this section\,--\,non-camera-based object detectors and
camera-based image processing techniques.


\subsection{Vehicle Detection Without a Camera}

Although camera-based vehicle detection systems have gained significant
popularity due to advancements in computer vision and machine learning, there
are alternative techniques that do not rely on cameras for vehicle detection.
These methods offer different advantages, mainly reduced computational
requirements or improved performance in certain environmental conditions. Since
each detector has its own limitations, a vehicle detection system typically
consists of different types of detectors to overcome these limitations.
Following is a summary of non-camera-based techniques widely used for vehicle
detection tasks.


\subsubsection*{Magnetic Sensors}

One of the simplest solutions to detect vehicles is to use an induction
loop.\cite{MagneticSensors} The sensor, composed of a single wire, can be buried
under concrete and detect the presence of metal objects passing by. With a
controller, induction loops typically provide data about vehicle presence, but
using more advanced algorithms, speed, approximate classification and much more
can be determined from this simple sensor. Although the design is very simple,
installation is not and induction loops can even get damaged over time, while
repairs call for temporary shutdowns of roads. It is worth noting that there
are many other types of magnetic sensors, which can provide more detailed and
accurate data with simpler sensor installation, but the idea stays the same.


\subsubsection*{Ultrasonic Sensors}

Another type of inexpensive and simple detector is an ultrasonic
sensor.\cite{UltrasonicSensors} These sensors can operate in a variety of
conditions and are typically mounted on existing infrastructure. These distance
measurement sensors are usually used to detect the presence and distance of nearby
vehicles and their speed, but they can be utilized in many different ways to
even provide a simple shape of a vehicle to classify it.


\subsubsection*{Radars and Lidars}

Radars (Radio Detection and Ranging), which can also be mounted on existing
infrastructure above ground, work similarly to ultrasonic sensors, but a single
radar can oversee a much wider area of a road.\cite{RadarSensors} They are
usually used to detect the presence, speed, heading and shape of a vehicle. The
shape can then be used to predict a class of the vehicle. Lidars (Light
Detection and Ranging) can be used in a similar way, but are far more accurate,
which makes them better at detecting shapes and locations of objects.



\subsection{Vehicle Detection Based on Image Processing}

% Kamery sú fajn, univerzálne, neinvazívne

% Ako to funguje?

% Treba stabilizáciu, homography (projective transform), detekciu, ktorá tiež veľa stojí
% K tomu, je ťažké to prispôsobiť na sneh, dážď, tornádo, divné vozidlá a tak
% A konfigurácia závisí od kamery a na inú kameru by to trebalo prispôsobiť

% However, they often have a problem with illumination changes, rain or snow,
% shadows, occlusions or noise. 

% Of course, there are methods for eliminating most
% of these problems, most importantly shadow removal\cite{ShadowRemoval}, but
% these methods add to the computational requirements of the system. 

% As our research suggests, the image processing approach is highly suitable for
% simpler tasks or lower accuracy requirements, but lacks versatility and is very
% hard to implement.

Image processing techniques take a camera feed as input and in addition to
detecting the presence, speed, heading and shape of a vehicle, they can also
provide its color, license plate and countless other characteristics, limited
mostly by the software, not by the detector. These camera-based systems, which
are relatively inexpensive, can be mounted on existing infrastructure, do not
emit any energy and are highly versatile, while providing a large amount of
data.

An algorithm applies a series of pre-defined filters and transformations to an
image to extract patterns and features that resemble vehicles. Although modern
convolutional neural networks (explained in the following \autoref{CNNs}) are
generally more effective at recognizing more complex patterns, image processing
can still be extremely helpful for simpler tasks or as a component of a larger
vehicle detection system.

A huge amount of research has been conducted on using image processing to solve
the problem of vehicle detection \cite{ImageProcessingOverview}. Many of the
techniques proposed can operate in real-time and are very reliable in typical
environmental conditions. However, they can be sensitive to changes in
illumination or have their performance affected by rain, snow, shadows,
occlusions, or noise. While there are methods for addressing some of these
challenges, such as shadow removal techniques \cite{ShadowRemoval}, they add to
the computational requirements of the system. Overall, our research suggests
that the image processing approach is well-suited for simpler tasks or systems
with lower accuracy requirements, but it is often difficult to implement and may
lack versatility in more complex or demanding scenarios.



% \section{Vehicle Detection With a Camera}

% Camera-based systems, which are relatively inexpensive, can be mounted on
% existing infrastructure, do not emit energy and are highly versatile, while
% providing a large amount of data. In addition to detecting the presence, speed,
% heading and shape of a vehicle, these systems can also provide its color,
% license plate and countless other characteristics.

% There are two main techniques for vehicle detection using a camera: image
% processing and convolutional neural networks (CNNs). Both approaches are based
% on analyzing images to identify patterns and features to detect vehicles. Image
% processing techniques involve applying a series of pre-defined filters and
% transformations to an image to extract relevant information. CNNs, on the other
% hand, are a type of machine learning algorithm that learns to recognize patterns
% and features through training. CNNs are generally more effective than image
% processing techniques for vehicle detection, as they can learn to recognize
% complex patterns and features that may be difficult to extract using pre-defined
% filters and transformations. However, image processing can still be extremely
% helpful for simpler tasks or as a component of a larger vehicle detection
% system. Additionally, CNNs typically require a lot more processing power.

% \subsection{Image Processing Techniques}

% % Treba stabilizáciu, homography (projective transform), detekciu, ktorá tiež veľa stojí
% % K tomu, je ťažké to prispôsobiť na sneh, dážď, tornádo, divné vozidlá a tak
% % A konfigurácia závisí od kamery a na inú kameru by to trebalo prispôsobiť

% % However, they often have a problem with illumination changes, rain or snow,
% % shadows, occlusions or noise. 

% % Of course, there are methods for eliminating most
% % of these problems, most importantly shadow removal\cite{ShadowRemoval}, but
% % these methods add to the computational requirements of the system. 

% % As our research suggests, the image processing approach is highly suitable for
% % simpler tasks or lower accuracy requirements, but lacks versatility and is very
% % hard to implement.

% A huge amount of research has been conducted on using image processing to solve
% the problem of vehicle detection.\cite{ImageProcessingOverview} Many of the
% techniques proposed can operate in real-time and are very reliable in typical
% environmental conditions.  However, they can be sensitive to changes in
% illumination or have their performance affected by rain, snow, shadows,
% occlusions, or noise. While there are methods for addressing some of these
% challenges, such as shadow removal techniques\cite{ShadowRemoval}, they add to
% the computational requirements of the system. Overall, our research suggests
% that the image processing approach is well-suited for simpler tasks or systems
% with lower accuracy requirements, but it is often difficult to implement and may
% lack versatility in more complex or demanding scenarios.



% \subsection*{Convolutional Neural Networks}
% % rozdiel od image processing prístupu a proste čo robia. Nie ako fungujú

\section{Convolutional Neural Networks}
\label{CNNs}

\todo{Lepšie ich popísať? Sú základom projektu, no}

Convolutional neural networks (CNNs) are a class of deep learning models that
have gained widespread popularity in recent years, mainly thanks to their
ability to learn hierarchical features from raw image data. CNNs are
particularly useful in the field of computer vision for various tasks, including
image classification, object detection and segmentation. In the context of
vehicle detection, these models have demonstrated a superior combination of
accuracy, efficiency and flexibility compared to traditional detection methods
explained earlier. For example, compared to object detection based on image
processing, the main advantage of CNNs is that they are able to learn
automatically from raw image data instead of relying on pre-defined filters and
transformations. \cite{Li2022}


\subsection{Architecture of Convolutional Neural Networks}

A typical CNN consists of several key components, including convolutional
layers, pooling layers and fully connected layers. In this subsection, details
of these key components and their roles in the context of object detection are
explained. Please note that a modern convolutional neural network consists of
more building blocks which will not be discussed here to keep this introduction
short.

\subsubsection{Convolutional Layers}

Convolutional layers form the backbone of a CNN and perform 2D convolution
operations on the input data using trainable kernels to detect specific patterns
or features. A kernel, also known as a filter, is responsible for detecting a
particular feature, such as an edge or a texture. It is practically a matrix,
usually small in spatial dimensionality and its weights are trainable parameters
adjusted during the training phase. \cite{OShea2015}

A two-dimensional convolution is a mathematical operation that involves
computing element-wise multiplications of the convolution kernel and the
corresponding sub-region of the input image. This process is typically repeated
throughout the entire image in a sliding manner, resulting in a new image
(matrix) as an output called a feature map. It can be better explained by
\autoref{Convolution}.

Many filters (often of different types) are used in a typical CNN and together
produce a set of feature maps that capture countless different aspects of the
input image.

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{convolution.pdf}
    \caption{Example of computing a 2D convolution on an input vector with an example kernel, from \cite{OShea2015}.}
    \label{Convolution}
\end{figure}


\subsubsection{Pooling Layers}

The pooling layers serve to gradually reduce the spatial dimensions of feature
maps output from the convolutional layers while retaining as much information as
possible. These layers improve the computational efficiency of the CNN by
reducing the number of its parameters. Generally, the pooling operation takes a
window or a filter and moves it over the input feature map in strides, most
commonly taking the maximum value of the inputs within the window. This
operation is called max-pooling. \cite{OShea2015}


\subsubsection{Fully-connected Layers}

A fully-connected layer, also known as a dense layer is a type in which each
neuron in the next layer is connected to each neuron in the previous one. In a
CNN, it is typically used at the end of the network to classify the input data
(features extracted by the previous convolutional layer). \cite{OShea2015}


\subsection{Popular Architectures of Convolutional Neural Networks}

\todo{Treba vôbec toto? Nejak to neviem naviazať na nasledujúce subsections}

Over the years, various CNN architectures have been developed to address
different challenges and requirements. In this subsection, we review several
influential architectures which achieved state-of-the-art\footnote{The term
\uv{state-of-the-art} refers to methods that have achieved superior results
compared to previous best methods in a specific task or application.}
performance in a wide range of tasks, including image classification, object
detection or semantic segmentation.

\todo{skontrolovať či som niečo nesplietol a či je všetko správne}

\subsubsection{LeNet-5}

LeNet-5 \cite{Lecun1998}, introduced by Yann LeCun and his team in 1998 is
considered one of the first successful applications of convolutional neural
networks. Designed for handwritten digit recognition, LeNet-5 was the source of
inspiration for modern CNNs with its combination of convolutional, pooling and
fully connected layers.


\subsubsection{AlexNet}

Developed by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton in 2012,
AlexNet \cite{NIPS2012} marked a significant breakthrough in the field of deep
learning and won the ImageNet Large Scale Visual Recognition Challenge (ILSVRC)
by a considerable margin. It popularized the use of deep CNNs for image
classification and featured the use of Rectified Linear Units (ReLU) as
activation functions, dropout technique for regularization and data
augmentations for training.


\subsubsection{VGGNet}

VGGNet \cite{Simonyan2014}, proposed by Karen Simonyan and Andrew Zisserman in
2014, is best known for its uniform architecture, consisting of a series of
stacked convolutional layers with $3 \times 3$ filters followed by max-pooling.
VGGNet demonstrated that deeper networks generally achieve better performance,
achieving top results in the ILSVRC with its 16-layer and 19-layer variants
(VGG-16 and VGG-19). However, its success lies in being very computationally
expensive.


\subsubsection{ResNet}

In 2015, ResNet (Residual Network) \cite{He2015} was introduced by Kaiming He
and his team, addressing the degradation problem that occurs in very deep CNNs.
ResNet incorporates skip connections, enabling the network to effectively
\uv{skip} layers during the training process. This innovation allowed ResNet to
scale up to hundreds of layers while improving performance, which was thought
impossible.  ResNet achieved state-of-the-art results in various computer vision
tasks and has inspired many subsequent architectures.


\subsubsection{MobileNet}

MobileNet \cite{Howard2017}, developed by Andrew G. Howard and his team at
Google in 2017, is a popular light-weight convolutional neural network
architecture that has been widely used on mobile and embedded devices with
limited computational resources. One of its key features is its use of depthwise
separable convolutions. Traditional convolutional layers perform a full
convolution on the input, but depthwise separable ones perform a depthwise
convolution followed by a pointwise convolution, which reduces the number of
computations required while maintaining accuracy. Another advantage is the
model's small size and low computational requirements compared to previously
discussed networks allowing for real-time use, of course by sacrificing some
accuracy.


\subsubsection{MobileNetV2}

MobileNetV2 \cite{Sandler2018}, an evolution of the original MobileNet
proposed by Mark Sandler and his team at Google in 2018. The architecture
introduces the concept of \textit{Inverted Residuals} and \textit{Linear
Bottlenecks}, aiming to enhance the network's representational capacity while
maintaining low computational complexity. \textit{Inverted Residual} blocks
leverage the idea of residual connections to mitigate the vanishing gradient
problem and improve the training phase. \textit{Linear Bottlenecks}, on the
other hand, reduce the computational cost of convolutional layers by decreasing
the number of channels before applying a convolutional operation and restoring
the original number of channels afterward. These advancements enable MobileNetV2
to achieve superior accuracy and efficiency compared to its predecessor.


\section{Object Detection}

While the CNNs discussed in \autoref{CNNs} can be used for image
classification\footnote{Image classification is the task of classifying an image
into a class category\,--\,for example recognizing whether an image contains a
cat or a dog.}, the object detection task also involves
localization\,--\,marking all objects in the input image by a bounding box.
However, these CNNs can and often are used as backbones\footnote{The backbone of an object detector is responsible for extracting
features from the input image.} to extract features from
the input images. Modern object detectors can be divided into two
categories\,--\,one-stage detectors and two-stage detectors.


\subsection{Two-Stage Object Detectors}

In the two-stage object detection task, the first stage selects region proposals
(selecting regions that are likely to contain an object) and passes them to the
second stage for classification. In 2013, the R-CNN framework
\cite{Girshick2013} (Regions with CNN features) was designed by Girshick
\textit{et al.}, replacing the old and inefficient sliding window detection
technique, marking a breakthrough in object detection. \cite{Li2022}

Although accurate, object detectors based on the R-CNN architecture (including
fast R-CNN and faster R-CNN) are generally computationally expensive compared to
one-stage object detectors and won't be discussed here in detail.


\subsection{One-Stage Object Detectors}

When building a real-time object detector, most commonly a one-stage detector is
selected. Although their accuracy is generally lower than that of two-stage
detectors, their speed allows for real-time object detection even on
resource-constrained devices.

\subsubsection{Single Shot MultiBox Detector (SSD)}

The Single Shot MultiBox Detector (SSD) is an object detection algorithm that
aims to provide a balance between accuracy and computational efficiency.
Proposed by Wei Liu \textit{et al.} in their 2015 paper \cite{Liu2015}, SSD
addresses the challenge of detecting objects in images with varying scales and
aspect ratios in real-time.

Unlike the two-stage object detectors which consist of a region proposal stage
followed by a classification stage, SSD directly predicts both the object
locations and their corresponding class labels in a single forward pass of the
network.

Built upon a base network such as VGG-16, SSD adds a series of convolutional
layers to capture information at different scales, allowing detection of various
object sizes. It uses anchor boxes placed uniformly across feature maps to
handle varying aspect ratios.

\subsubsection{You Only Look Once (YOLO)}

YOLO (You Only Look Once) is a family of one-stage object detectors that
prioritize real-time performance. Introduced by Joseph Redmon \texttt{et al.} in
2015 \cite{Redmon2015}, YOLO divides the input image into a grid, where each
grid cell is responsible for predicting bounding boxes and class probabilities
for objects located within that cell. YOLOv1 has undergone many iterations each
improving upon the previous version's performance and efficiency\,--\,from
YOLOv2 to YOLOv8 and including YOLOX, YOLOR and several more.

In this work, we focus on the latest, YOLOv8 state-of-the-art object detector
\cite{YOLOv8} developed by Ultralytics, which, at this time, doesn't have a
paper released. However, information about the detector can be drawn from a post
by OpenMMLab \cite{YOLOv8OpenMMLab}.

Built upon the YOLOv5, the YOLOv8 detector updated the head\footnote{The head of
an object detector predicts the object classes and bounding box coordinates
using features input by the neck.} module to be a decoupled one, separating the
classification and detection heads, while the new backbone and neck\footnote{The
neck module of an object detector refines features input from the backbone.}
modules are based on the YOLOv7 ELAN concept. In terms of training strategy,
YOLOv8 extends the training epochs from 300 to 500 and the data augmentation
process is modified during the final 10 epochs, with a reduction in the
intensity of augmentations. Additionally, the loss function undergoes a
revision, incorporating the TaskAlignedAssigner from TOOD and introducing
Distribution Focal Loss for regression loss, which further enhances
the detector's accuracy and efficiency.


% dať tu samozrejme nejaké images, pokojne aj architektúru

% \begin{itemize}
%     \item \textbf{YOLOv1} \todo{cite} simultaneously detects all bounding boxes by dividing the input image into a grid and
%     predicting bounding boxes for each grid element. The architecture consists of 24 convolutional layers and two fully-connected
%     layers with leaky rectified linear unit activations (leaky ReLU).
%     \item \textbf{YOLOv2} \todo{cite} uses batch normalization on all convolutional layers,
%     a fully convolutional architecture, 
% \end{itemize}


% \subsubsection{Efficient Model Architectures}
% \todo{Hlavne MobileNetV2, ak som to nepopísal pri mobilenete; squeezenet a shufflenet?}




\section{Network Optimization and Compression}

As CNNs have grown deeper and more complex to improve accuracy, the
computational and memory requirements have increased significantly. This makes
it difficult to deploy CNNs on embedded devices with limited resources. To
address this challenge, researchers have developed various model optimization
and compression techniques that aim the reduce the computational cost and memory
footprint while trying to maintain the model's accuracy. A brief overview of the
most significant model optimization and compression techniques is given in this
section. However, in this paper, only one of these techniques\,--\,weight
quantization will be used.


\subsection{Network Pruning}

Typically, there are many parameters in a CNN which were not utilized during the
training phase and do not contribute to the network's performance. The pruning
model optimization technique aims to simply remove these redundant parameters
from the model while maintaining accuracy. Of course, more aggressive pruning
can be performed, removing even more parameters, including useful but still less
important ones, although sacrificing some accuracy.

Various pruning techniques exist, including weight pruning (removal of a single
weight), neuron pruning (removal of an entire neuron and its connections),
filter pruning and layer pruning.

Although the pruning technique was developed to reduce the model's storage
requirements, it is also used to reduce its computational requirements.

\todo{cite https://link.springer.com/article/10.1007/s10462-020-09816-7}


\subsection{Knowledge Distillation}

Knowledge distillation is a model compression technique in which a smaller, more
compact student network is trained to mimic the behavior of a larger,
high-performing teacher network (or an ensemble of them) to learn the teacher
model's generalization capability. The student network learns from outputs from
the teacher network instead of the ground truth labels. For more effective
knowledge distillation methods, these outputs often include intermediate feature
maps of the teacher network. Generally, the student network cannot achieve
accuracy as high as the teacher network, but when performed correctly, it
typically achieves higher accuracy than if trained the conventional way.

\todo{cite 2022 structural KD for object detection}

\todo{vymazať alebo rephrase:}
Because this method practically involves training two models, with one being
much larger than the other, it requires more training time than training just
the student model would. It is however very useful when training and comparing
several student networks.

Although this technique was initially considered highly advantageous for
addressing the problem proposed in this paper, upon careful analysis, we assumed
that the associated complexity would pose significant obstacles, primarily in
terms of increased development time. Therefore, this technique was not employed
in the current study; however, its exploration is highly recommended for future work.


\subsection{Weight Quantization}

\todo{todo \cite{Choudhary2020} nasledujúce tri odstavce}

In convolutional neural networks, weights and biases are stored as 32-bit
floating-point numbers (\texttt{FP32}), which provide precision often
unnecessary for the CNNs to be accurate. Quantization is the process of reducing
the number of bits used to represent these parameters and generally decreases
the storage and computational requirements of the network.

Although reducing the precision of the model's parameters decreases its
accuracy, the drop in accuracy is typically insignificant when compared to the
substantial benefits in storage and computational efficiency gained, which are
essential factors when developing a high-performance object detector.

Most commonly, models parameters are quantized to either a 16-bit floating-point
(\texttt{FP16}) representation or an 8-bit integer (\texttt{INT8})
representation. Quantizing to \texttt{FP16} usually doesn't require any
post-quantization steps. However, quantizing to an integer representation, such
as \texttt{INT8}, is a different process. Because of the limitations of the
integer representation and the distinction from floating-point representations,
weight calibration\footnote{Weight calibration during weight quantization refers
to the process of adjusting the quantized weights to minimize the loss of
accuracy that occurs due to the reduction in numerical precision.} during the
quantization process or even model fine-tuning\footnote{Model fine-tuning refers
to further training of a model with lower learning rate to refine the model's
parameters.} after it is recommended to maintain the highest possible prediction
accuracy. This, of course, only applies to post-training quantization (PTQ) while
several other quantization techniques are available\,--\,mainly training the
model with weights in the desired representation from the beginning (quantization-aware training - QAT).
Furthermore, a more advanced technique called the mixed-precision quantization
(MPQ) can be used to quantize parameters in a more nuanced manner by assigning
different precisions to individual parameters or parameter groups depending on
their sensitivity to numerical errors \cite{Tang2022}.

While the weight quantization technique can significantly decrease a model's
computational requirements, it is crucial to note that not all devices are
compatible with every parameter representation. Therefore, it's important to
verify whether a specific device supports computations with the desired number
representations.

% \todo{Nechať to tu keď nemám normálne zdroje? Ak áno, upraviť}
% \todo{o jetsone napísať že to napísal tensorrt do logov a aj na nvidia fóre to píšu + presunúť k experimentom. O CPU support asi môžem nechať}

% To give an example, most CPUs developed by Intel use the same
% multiplication operation for numbers in both the \texttt{FP16} and \texttt{FP32}
% representation, so numbers in the \texttt{FP16} representation need conversion
% to \texttt{FP32} before computation \todo{cite
% https://www.intel.com/content/www/us/en/docs/cpp-compiler/developer-guide-reference/2021-8/intrinsics-for-converting-half-floats.html}.
% This should generally apply to all CPU manufacturers, but no credible sources
% were found to support this claim. 

% As another example, NVIDIA Jetson
% Nano\,--\,small computer designed for AI applications with Maxwell GPU doesn't
% support inference (\todo{footnote že čo je inference?}) of models quantized to
% \texttt{INT8}. However, no official sources were found to support this claim either \todo{mám iba https://forums.developer.nvidia.com/t/nano-jetson/78525}.

Most deep learning libraries which can be used for model deployment or inference
on a target device support weight quantization and offer tutorials on how it's
done. These libraries include TensorRT, ONNX Runtime, OpenVINO and many others.
Ones relevant to this project will be discussed later in this chapter.



\section{Evaluation Metrics}
\label{EvaluationMetrics}

Evaluation metrics are crucial for assessing the performance of different object
detection models, enabling comparison between different architectures and
tracking improvements during training. In this section, we provide a brief
explanation and background of the mean Average Precision (mAP) metric, which
will be used to evaluate the detectors trained in this project. The information
in this section is derived from a blog post on the V7Labs website \cite{mAP}.

\subsection*{Precision and Recall}

Precision is a measure of the proportion of true positive detections out of all
detections (true positives and false positives), while recall measures the
proportion of true positive detections out of all ground truth\footnote{Ground
truth refers to the actual, true data used as a reference for comparison with
the model's predictions.} objects (true positives and false negatives) in the
dataset. Therefore, precision represents how many predictions of the model are
correct and recall measures how many of the ground truth objects were predicted
by the model.

If the precision and recall values are computed at different confidence
score\footnote{Confidence score is a number output from a detector for each
detection stating the model's confidence that the detection is correct.}
thresholds, they can be plotted one against the other to obtain a
precision-recall curve, which can be used to visually evaluate the overall
accuracy of the model.

\subsection*{Average Precision}

The Average Precision (AP) is a metric widely used to represent the performance
of an object detector. It is simply calculated as the area under the
precision-recall curve and ranges from 0 to 1, where AP of 1 indicates perfect
precision and recall at all thresholds.

\subsection*{Mean Average Precision}

To evaluate multi-class object detectors, mean Average Precision (mAP) is used
instead of the previously explained Average Precision. Computing the mAP
involves finding the AP for each class and calculating the average over the
number of classes.

However, for object detection tasks, precision is typically calculated with
different thresholds of the Intersection over Union (IoU) metric. The IoU metric
measures the overlap between two bounding boxes\,--\,between the model's
prediction and the ground truth bounding box, and represents the quality of the
alignment between the two boxes. Calculating the IoU simply means dividing the
intersection area of the two bounding boxes over their union. When calculating
precision, the IoU metric is used to determine whether a predicted bounding box
should be considered a true positive (IoU is higher than the defined IoU
threshold) or a false positive (IoU is lower than the threshold). For
visualization, see \autoref{IoU}.

\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{0.3125\textwidth}
        \includegraphics[width=\textwidth]{iou_1.pdf}
        \caption{$IoU = 0$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.2272727\textwidth}
        \includegraphics[width=\textwidth]{iou_2.pdf}
        \caption{$IoU \approx 0.333$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.15151515\textwidth}
        \includegraphics[width=\textwidth]{iou_3.pdf}
        \caption{$IoU = 1$}
    \end{subfigure}

    % \vspace{1cm}
    % \begin{subfigure}[b]{0.5\textwidth}
    %     \includegraphics[width=\textwidth]{iou_all.pdf}
    %     % \caption{Visualization of the Intersection over Union calculation}
    % \end{subfigure}
    \caption{Visualization of the Intersection over Union calculation.}
    \label{IoU}
\end{figure}

In this paper, we consider the COCO mAP specification and calculate the mAP as
an average of AP calculated for all classes and over 10 IoU thresholds ranging
from \num{0.50} to (and~including) \num{0.95} with step \num{0.05}. Similarly,
values $\text{mAP}^{\text{IoU:0.50}}$ and $\text{mAP}^{\text{IoU:0.75}}$ denote
mAP calculated with IoU thresholds equal to \num{0.50} and \num{0.75}
respectively. Additionally, $\text{mAP}^{\text{small}}$ is only calculated for
objects of area smaller than $32^2$ pixels, $\text{mAP}^{\text{medium}}$ for
objects larger than $32^2$ px but smaller than $96^2$ px and finally,
$\text{mAP}^{\text{large}}$ for objects larger than $96^2$ px.




\section{Vehicle Detection Based on Convolutional Neural Networks}
\todo{tu by toho trebalo viac, nie? Ale nemám moc z čoho čerpať.. hlavne
ohľadom embedded resp. optimalizovaných sú všetky články proste zlé.}
\todo{Ale môžem písať o embedded a optimalizáciach všeobecne pre detektory, nemusí byť pre vozidlá}


In this section, we discuss existing literature on vehicle detection using deep
learning and convolutional neural networks. In our case, papers comparing the
performance of different object detectors in the vehicle detection task were
important for us to choose the right models to evaluate. Additional research was
done on refining these detectors for detecting small vehicles, optimizing them
for inference speed or vehicle detection accuracy. Of course, we advise the
reader to be cautious when comparing their results because they vary based on
the selected evaluation metrics, used datasets and devices used to evaluate the
models.

Deep learning has revolutionized vehicle detection by significantly improving
accuracy and robustness. In \cite{Wang2019}, the authors compare five mainstream
deep learning-based object detection algorithms for vehicle detection in
autonomous driving\,--\,Faster R-CNN, R-FCN, SSD, RetinaNet and YOLOv3. The
results indicate that two-stage detectors generally have better detection
accuracy than one-stage models, while SSD and YOLOv3 algorithms were found to
have excellent real-time performance and generalization abilities. 

In a more recent paper by Maity \texttt{et al.} \cite{Maity2021},
the authors present a comprehensive review of existing Faster R-CNN and
YOLO-based vehicle detection and tracking methods, comparing them and
highlighting their advantages over traditional vehicle detection methods. The
authors also discuss several vehicle detection methods featuring modified YOLO
models to improve detection performance.

In \cite{Wu2021}, the authors propose YOLOv5-Ghost\,--\,an improved neural
network structure based on YOLOv5-small, to detect vehicles in the
CARLA\footnote{CARLA (Car Learning to Act) is an open-source, realistic urban
environment simulator designed for the development, testing and validation of
autonomous driving systems in various traffic scenarios and conditions.} virtual
environment. By replacing BottleneckCSP in the YOLOv5-small with Ghost
Bottleneck, the detection speed is increased from 29 FPS (YOLOv5-small) to 47
FPS (YOLOv5-Ghost) while only reducing the mAP from \num{83.36} to \num{80.75}
on their test dataset.


\todo{my sa zameriavame na porovnanie yolov8 na embedded zariadeniach a nič viac k tomu relevantné sme nenašli}












\section{Embedded Platforms for Machine Learning}

Embedded platforms are a combination of hardware and software components that
are designed to perform specific tasks. For object detection, or machine
learning in general, these systems are optimized to be power efficient while
providing significant processing capabilities for the development, deployment and
execution of machine learning algorithms. In this section, we will discuss some
of the most popular embedded devices and platforms used for machine learning,
including object detection.

These embedded devices typically incorporate custom processors, microcontrollers
or specialized accelerators specifically engineered for efficient execution of
machine learning tasks, such as graphics processing units (GPUs), which can
perform many computations in parallel, tensor processing units (TPUs) and many
more.

\subsection{Google Coral}

Designed for TensorFlow Lite\footnote{TensorFlow Lite is a lightweight machine
learning framework designed for running TensorFlow models efficiently on devices
with limited computational resources.} models, the Google Coral platform offers
an Edge TPU\,--\,a low-power, high-performance ASIC (application-specific
integrated circuit) enabling on-device machine learning
inference\footnote{Inference in machine learning is the process of using a
trained model to make predictions on new data, previously unseen by the model.}.
Coral provides development boards, USB accelerators along with a variety of
modules and peripherals for edge AI applications.


\subsection{Movidius Neural Compute Stick}

Intel's Movidius Neural Compute Stick, commonly paired with a popular
single-board computer Raspberry Pi, is a small, low-power USB-based hardware
accelerator featuring a vision processing unit (VPU) designed to accelerate
neural network computations.


\subsection{NVIDIA Jetson}
\label{Jetsons}

NVIDIA Jetson is a series of widely-used embedded computing platforms that
feature powerful GPU accelerators and ARM-based CPUs while being
energy-efficient.

\todo{Nejakú fotku jetsonov?}


\subsubsection{NVIDIA Jetson AGX Xavier}

Jetson AGX Xavier is the flagship model in the NVIDIA Jetson family. It is a
high-performance, energy-efficient platform designed for more demanding AI
workloads. With an integrated NVIDIA Volta GPU with 512 CUDA cores and 64 Tensor
cores, an 8-core NVIDIA Carmel ARM CPU and 16 GB of memory, it offers substantial
computational capabilities for deploying state-of-the-art real-time object
detectors. Its typical power consumption ranges from 20 W to 30 W.


\subsubsection{NVIDIA Jetson Xavier NX}

The NVIDIA Jetson Xavier NX features an NVIDIA Volta GPU with 384 CUDA cores and
48 Tensor cores, a 6-core NVIDIA Carmel ARM CPU and 8 GB of memory. Compared
to Jetson AGX Xavier, it is more compact and power-efficient, with the typical
consumption of 15 W, while of course offering lower performance and memory
capacity.


\subsubsection{NVIDIA Jetson Nano}

The smallest and most popular module from the NVIDIA Jetson family is the Jetson
Nano. Equipped with a 128-core NVIDIA Maxwell GPU, a quad-core ARM Cortex-A57
CPU and just 4 GB of memory, it serves as an entry-level, low-cost AI embedded
platform, offering lower performance but lower power consumption (ranging from 5
to 10 W) compared to the previously mentioned Jetson boards.


\subsubsection{Software for NVIDIA Jetson devices}

Devices of the NVIDIA Jetson family support the Linux for Tegra (L4T) operating
system, a customized Linux distribution designed specifically for the platform's
unique capabilities. NVIDIA also offers the JetPack SDK, which includes the
CUDA toolkit and the cuDNN library accelerating deep learning tasks on NVIDIA
GPUs, the TensorRT library used for optimizing deep learning models to achieve
higher inference speeds on NVIDIA GPUs, along with various multimedia and
computer vision libraries.

On all used NVIDIA Jetson platforms, JetPack SDK version \texttt{4.6.3} was
used. It's important to note that using this version of the JetPack SDK means
using older software versions, including Python version \texttt{3.6}, TensorRT
version \texttt{8.2.1} and CUDA version \texttt{10.2}. The reason for not using
a more recent JetPack SDK (even though it supports the Jetson AGX Xavier and
Jetson Xavier NX), along with the complications it caused, will be discussed in
\autoref{Experiments}.


\section{Tools and Libraries}

In this section, we will provide a brief overview of some crucial tools and
libraries utilized throughout the project.


\subsection{LabelBox annotation app}

LabelBox \cite{LabelBox} is a web-based application and data labeling platform
widely used when training machine models. It provides an easy-to-use interface
and a suite of tools to manage and annotate datasets efficiently. With the
option of importing data, we have used the app to reannotate one of the
datasets.


\subsection{PyTorch}

PyTorch \cite{PyTorch} is a popular open-source, Python-based machine learning
library designed to provide flexibility, ease of use, and high performance for
deep learning applications. It offers a rich ecosystem of tools and libraries for
various tasks, including computer vision. With support for GPU acceleration
using NVIDIA's CUDA platform, PyTorch enables fast and efficient computation of
large models, making it an ideal choice for high-performance deep learning
tasks.

\subsection{MMDetection Library}

MMDetection \cite{MMDetection} is a popular open-source deep learning toolbox
for computer vision tasks, including object detection. Developed by the
Multimedia Laboratory at the Chinese University of Hong Kong (OpenMMLab),
MMDetection provides a flexible, extensible and modular framework that aims to
simplify the process of training and deploying state-of-the-art models for
various computer vision tasks, and provides a rich set of tools. Some of the key
features of the library include:

\begin{itemize}
    \item It is based on the PyTorch machine learning library.
    \item It builds upon and uses other open-source libraries from the OpenMMLab
    project, such as MMCV\footnote{MMCV is a library for computer vision
    research including building blocks for convolutional neural networks, tools
    for image processing, transformations and much more.} and
    MMEngine\footnote{MMEngine library serves as the training engine for all
    OpenMMLab codebases, supporting hundreds of algorithms frequently used in
    deep learning.}.
    \item Library's modular design allows for easy customization and extension
    of the codebase.
    \item It includes pre-trained models and their configurations which makes
    training and comparing different models fast and simple, while making it
    possible to use the transfer learning technique\footnote{Transfer learning
    is a machine learning technique that leverages knowledge of a pre-trained
    model to more efficiently train a new model, reducing training time and data
    requirements.} which significantly speeds up the process.
\end{itemize}


\subsection{MMYOLO Library}

Although the MMDetection library doesn't include the latest detectors of the
YOLO family, such as the YOLOv8 detector which will be used in this project, the
OpenMMLab project features a different library called MMYOLO
\cite{MMYOLO}\,--\,an extension of the MMDetection library, which addresses this
issue and focuses solely on detectors of the YOLO family. It contains
implementations of YOLO-specific components, such as the CSPDarknet and PANet
backbone networks, and YOLO-specific training techniques, like data
augmentations or loss functions.


\subsection{MMDeploy Library}

The MMDeploy library \cite{MMDeploy}, which is also a part of the OpenMMLab project,
offers useful tools for deploying OpenMMLab models to a wide range of platforms
and devices. It enables the conversion of PyTorch models trained with
MMDetection or MMYOLO into backend models for execution on target devices.
MMDeploy supports various backends, including ONNX, TensorRT, OpenVino,
TorchScript and numerous others. In addition to streamlining the deployment
process, the library also optimizes the converted models for their target
platforms.


\subsection{ONNX and ONNXRuntime}

ONNX (Open Neural Network Exchange) \cite{ONNX} is an open-source project aimed
at creating a consistent format for deep learning models. It was initially
developed by Facebook and Microsoft, but several other companies and
organizations joined later on. The primary goal of ONNX is to enable developers
and researchers to easily switch between different machine learning frameworks
without having to worry about model compatibility. ONNX defines a standard
representation for neural network models, making it possible to train a model in
one framework and use it for inference in another.

Additionally, ONNX provides a set of tools and libraries, such as ONNX Runtime
\cite{ONNXRuntime}, which is a high-performance inference engine for ONNX
models.


\subsection{TensorRT}

TensorRT \todo{cite ale nenašiel som citáciu} is a high-performance deep learning inference optimizer and runtime
library developed by NVIDIA. It is designed to accelerate the deployment and
inference of models on NVIDIA GPUs for various applications including computer
vision.

In addition to optimizing the target models for improved inference
performance and reduced memory footprint, TensorRT also supports multiple
precision modes, including \texttt{FP32}, \texttt{FP16} and \texttt{INT8},
allowing developers to choose the best balance between accuracy and performance.


% \subsection{NCNN}




\chapter{Datasets}

Big and high-quality datasets are very important when training a CNN-based
detector. In this section, used datasets are listed and analyzed. First, the
criteria for recognizing an appropriate dataset for our task are explained. Each
chosen dataset is then analyzed individually. Finally, a summary of all used
datasets is provided along with details on how the dataset was split into
training, validation and testing subsets.

\todo{Niekde popísať rozdiel medzi trénovacím, validačným a testovacím datasetom a na čo treba všetky?}

\section{Dataset Criteria}

\todo{Nespísať to priamo do kapitoly? Zbytočný section je toto}

Here, we briefly explain the most important criteria for selecting datasets to
be used in a project like this.

\subsubsection*{Camera Angle}

Since we're building a detector for a camera mounted on infrastructure, it is
recommended to use datasets containing surveillance-type images. Datasets
containing only images taken from, for example, a car dashboard camera, were
therefore disregarded.

\subsubsection*{Classes}
\label{Classes}

If we don't want to re-label the dataset manually, its classes must be mappable
to \textit{our} classes. In this work, 8 object classes are considered:
\begin{itemize}
    \item Bicycle
    \item Motorcycle (any two-wheeled motorized vehicle)
    \item Passenger Car
    \item Transporter (or a van, pick-up truck, etc.)
    \item Bus (including a minibus)
    \item Truck
    \item Trailer
    \item Unknown
\end{itemize}

Many available datasets didn't annotate some of these classes or aggregated some of them into one
and were therefore ignored.

\subsubsection*{Diversity of Images}

For the trained detector to generalize well, a dataset needs to
contain images with diverse camera angles, lighting conditions, weather, and
other factors. For example, a 60 FPS continuous video does not bring much of an
advantage.

\subsubsection*{Dataset Quality}

We observed that many datasets contained incorrect annotations or
classifications. It is important to check the dataset and either fix faulty
annotations, ignore incorrectly annotated images or even disregard the whole
dataset.



\section{Individual Datasets}

% This section individually analyzes all datasets used in this work. Later in this chapter, \autoref{DatasetsSummary} 
% compares these datasets on a higher level for an overview and at the end of this chapter, we discuss how the datasets were split
% for training, validation and testing.

This section individually analyzes all datasets used in this work. We discuss
their sizes, origins, advantages and drawbacks and also provide an example image
from each of the datasets.


\subsection{UA-DETRAC}
\label{DetracDataset}

The DETRAC dataset \cite{detrac} provided by the University at Albany is the
biggest and the most important dataset for this work, originally containing
\num{1274055} annotations of \num{8250} vehicles in \num{138252} images. The
dataset is provided as frames from 100 video sequences of \SI{25}{fps} with the
resolution being $960 \times 540$ pixels. The length of these sequences varies,
but is usually between 30 seconds and 90 seconds. The video is of a surveillance
type and almost all sequences have a unique point of view, usually from a
bridge. Many sequences were recorded in rain or at night and there is no lens
flare from cars' headlights. One of the images from this dataset is shown in
\autoref{DetracDatasetExample}.

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{dataset_example_detrac.jpg}
    \caption{Example image from the UA-DETRAC dataset.}
    \label{DetracDatasetExample}
\end{figure}

However, there are several issues with this dataset:
\begin{itemize}
    \item Bicycles, motorcycles and a few vehicles that cannot be classified are
    not annotated at all.
    \item Bounding boxes are often loose and do not fit tightly to the objects.
    \item Vehicles near the edge of the frame, although fully visible, sometimes
    lack labels.
    \item In several sequences, vehicles are tracked and annotated even on
    frames on which they are fully occluded (by other vehicles or
    infrastructure).
    \item Vehicle annotations are inconsistent in relation to masks, as some are
    labeled even when masked, while others are left unlabeled even when already
    fully visible outside of a mask. This is likely due to the camera being
    hand-held and therefore moving while the mask is static throughout the
    sequence.
\end{itemize}

These problems were fixed by importing the dataset to the LabelBox labeling
application, adjusting the masks (while also making them dynamic to compensate
for camera movements), annotating or masking bicycles, motorcycles and
\uv{unknown} vehicles and finally, carefully repairing individual annotations if
needed. Many sequences were hectic or were annotated so poorly that reannotating
would be too time-consuming, so only 71 of 100 sequences were reannotated.

The reannotated dataset contains \num{733833} annotations in \num{99771}
images and these classes (the dataset contains no trailers):
\begin{itemize}
    \item Bicycle
    \item Motorcycle
    \item Car
    \item Bus
    \item Van - mapped to \textit{transporter}
    \item Others - mapped to \textit{truck}
    \item Unknown
\end{itemize}


\subsection{Miovision Traffic Camera Dataset}

The MIO-TCD (Miovision traffic camera dataset) \cite{MIO2018} is another huge
and very important dataset. Images are taken at different times of day by
thousands of traffic cameras in Canada and the United States. Roughly $79\%$ of
all images are of resolution $720 \times 480$ pixels, but the image quality
seems to be lower. The rest is of resolution $342 \times 228$ pixels. Example images of
both resolutions are shown in \autoref{MIOTCDDatasetExample}.

\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{0.475\textwidth}
        \includegraphics[width=\textwidth]{dataset_example_mio-tcd.jpg}
        \caption{Resolution $720 \times 480$}
    \end{subfigure}
    \begin{subfigure}[b]{0.475\textwidth}
        \includegraphics[width=\textwidth]{dataset_example_mio-tcd_small.jpg}
        \caption{Resolution $342 \times 228$}
    \end{subfigure}
    \caption{Example images from the Miovision dataset.}
    \label{MIOTCDDatasetExample}
\end{figure}

The dataset consists of two parts: \textit{Classification dataset} and
\textit{Localization dataset}. Only the \textit{Train} subset of the
\textit{Classification} part is used here because the \textit{Test} subset is
not annotated.

The part used contains \num{351549} objects of these classes:
\begin{itemize}
    \item Pedestrian - ignored
    \item Bicycle
    \item Motorcycle
    \item Car
    \item Pickup truck - mapped to \textit{transporter}
    \item Work van - mapped to \textit{transporter}
    \item Bus
    \item Articulated truck - mapped to \textit{truck}
    \item Single unit truck - mapped to \textit{truck}
    \item Non-motorized vehicle - mapped to \textit{trailer}
    \item Motorized vehicle - mapped to \textit{unknown}
\end{itemize}
The processed dataset (without pedestrian annotations) contains \num{344416}
objects in \num{110000} images.

% The images are of low quality, there aren't many bicycles and motorcycles, and
% more pictures with different weather and lighting conditions are needed.


\subsection{AAU RainSnow Traffic Surveillance Dataset}
\label{AAUDataset}

Another important dataset is the \textit{AAU RainSnow} dataset
\cite{Bahnsen2019}. The authors mounted two synchronized (one RGB and one
thermal) cameras on street lamps at seven different Danish intersections to take
5-minute long videos at different lighting and weather conditions - night and
day, rain and snow. They then extracted \num{2200} frames from the videos and
annotated them on a pixel-level. Several different types of masks were also
created and included in the dataset.

In our work, we only use the annotated frames from the RGB camera, containing
\num{13297} annotations in \num{2200} frames (before processing) of resolution
$640 \times 480$ pixels. See an example in \autoref{AAUDatasetExample}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{dataset_example_aau.png}
    \caption{Example image from the AAU RainSnow dataset.}
    \label{AAUDatasetExample}
\end{figure}

The dataset uses these 6 classes:
\begin{itemize}
    \item Pedestrian
    \item Bicycle
    \item Motorbike
    \item Car
    \item Bus
    \item Truck
\end{itemize}
This introduces a problem - vehicles of our internal class \textit{transporter}
(van) don't have their own class in the dataset, but are classified as trucks.
However, the dataset is small and it is impossible to perfectly divide
transporters and trucks into two classes as there are many different models
between which a line cannot be drawn.  Ignoring this issue should therefore not
cause any problems.

Several other minor problems were found when processing this dataset:
\begin{itemize}
    \item Frames in groups \verb|Egensevej-1|, \verb|Egensevej-3| and
    \verb|Egensevej-5| are hardly usable because of the low-quality camera and
    challenging weather and lighting conditions, so they were dismissed.
    \item Some frames had bounding boxes over the whole frame - this is most
    certainly an annotation error. These labels were ignored as well.
    \item The mask for \verb|Hadsundvej| intersection didn't fully cover the
    area that should be ignored. This was fixed by simply editing the mask.
\end{itemize}

After processing, the dataset contains \num{10545} objects in \num{1899} images.


\subsection{Multi-View Traffic Intersection Dataset}

For the MTID dataset, the authors \cite{Jensen2020} recorded one intersection
from two points of view at \SI{30}{fps} - one camera was mounted on existing
infrastructure and one was attached to a hovering drone. The dataset contains
\num{65299} annotated objects in \num{5776} frames (equal share of frames for
both cameras). An example from this dataset can be seen in
\autoref{MTIDDatasetExample}.

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{dataset_example_mtid.jpg}
    \caption{Example image from the drone subset of the Multi-View Traffic Intersection dataset.}
    \label{MTIDDatasetExample}
\end{figure}

All annotated objects fall into one of four classes:
\begin{itemize}
    \item Bicycle
    \item Car
    \item Bus
    \item Lorry - mapped to \textit{truck}
\end{itemize}
This, at first, might not seem like enough, but a closer inspection of the
annotated frames reveals that there are no pedestrians or motorcycles. However,
similarly to the AAU dataset in \autoref{AAUDataset}, there are transporters in the frames
that are classified as trucks. Again, this issue is simply ignored.

When processing this dataset, two other problems were encountered:
\begin{itemize}
    \item Vehicles that are not on the road are not annotated, so they have to
    be masked out. This is not as easy for the drone video because the camera is
    moving, but it is still simple enough.
    \item Many frames of the drone footage lack some or all labels and have to
    be ignored. Ranges of images numbers which are ignored: $[1,31]$,
    $[659,659]$, $[1001,1318]$ and $[3301,3327]$.
\end{itemize}
The processed dataset contains \num{64979} objects in \num{5399} frames.


\subsection{Night and Day Instance Segmented Park Dataset}

Another useful dataset is the \textit{NDISPark} \cite{Luca2022}, which contains
images of parked vehicles taken by a camera mounted on infrastructure. Although
the annotated part of the dataset only contains \num{142} frames after
processing, there are \num{3302} objects annotated in total. This still makes it
a tiny dataset, but it provides images of vehicles from many different points of
view and also contains many occluded vehicles. See \autoref{NDISDatasetExample}
for an example image from this dataset. Additionally, all frames are \num{2400}
px in width and within \num{908} px and \num{1808} px in height.

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{dataset_example_ndis.jpg}
    \caption{Example image from the NDISPark dataset.}
    \label{NDISDatasetExample}
\end{figure}

This dataset does not contain any classifications, but luckily, it only contains
cars, \textit{transporters} and a few unattached car trailers. All
\textit{transporters} and \textit{trailers} were manually classified.


\subsection{VisDrone Dataset}

The VisDrone dataset \cite{Zhu2022} is very different from all the previous
datasets since it doesn't just contain traffic surveillance images. Images are
taken by a camera mounted on a drone, from many different points of view. After
processing, there are \num{47720} annotated objects in \num{1610} frames. An
example is displayed in \autoref{VisDroneDatasetExample}.

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{dataset_example_visdrone.jpg}
    \caption{Example image from the VisDrone dataset.}
    \label{VisDroneDatasetExample}
\end{figure}

This dataset might be a helpful addition to our datasets as it contains useful
negative images (many annotated people and new points of view) and it often
captures vehicles from a bid's-eye view.

Additionally, objects in this dataset are classified into 12 classes, which can
be easily mapped to our 8 \textit{internal} classes.


\section{Dataset processing}

Before training, all datasets used in this project must be converted to a
unified format, object classes need to be mapped to be the same in each dataset.
Additionally, some datasets include images with regions that should be masked
out, and certain subsets or images of some datasets need to be ignored.

Therefore, a Python script was developed for each dataset. It first loads the
annotations from the original format\,--\,COCO\footnote{The widely used COCO
annotation format defines how a dataset (its categories, list of images,
annotations and other metadata) should be stored in the JSON (JavaScript Object
Notation) format as a file.} for AAU RainSnow, MTID
and NDISPark, XML for DETRAC and different CSV formats for MIO-TCD and VisDrone.
The script then maps the classes to ones shown in \autoref{Classes} and if
needed, removes the ignored subsets or images before saving the labels in the
COCO format. The processing script for NDISPark dataset also corrects the object
classes (since the original dataset annotates all objects with the same class)
before saving, and therefore does not modify the original dataset file (class
corrections are defined in the script itself).

MMDetection's middle format was considered as it's a more efficient alternative
to the COCO format, but the COCO format is more popular and is supported by most
relevant application, while also being human-readable (the MMDetection's middle
format is saved as a pickle\footnote{The Python's pickle format refers to a
binary serialization and deserialization method allowing for the conversion of
Python objects into a byte stream.} file), and most importantly, MMYOLO and the
most recent version of MMDetection at the time of preparing the datasets
(\texttt{v0.4.0} and \texttt{v3.0.0rc2}) do not seem to fully support datasets
in the middle format to be used in all of their dataset wrappers.

Several other Python processing scripts were developed, to apply masks, combine
all ground truth files into one and to split the combined ground truth file into
training, validation and testing subsets. Additionally, scripts to review
individual datasets manually were created - one to visualize a dataset by simply
adding bounding boxes (with class labels) to the images and one to convert the
visualized images to video (or videos).

A few more scripts were created, of which two are worth mentioning\,--\,one for
uploading the DETRAC dataset to the LabelBox application for reannotation and
one for downloading the reannotated labels.


\section{Summary of Datasets}

In \autoref{DatasetsSummary}, we compare used datasets on a higher level and
show the number of images and instances contained with additional comments. It
is clear that the DETRAC dataset amounts for most of our
data and might have been enough by itself, but to make this project as
successful as possible, every available useful dataset should be used. The
images from the selected datasets feature a great variety of lighting and
weather conditions, points of view, object scales, occlusions and other relevant factors.
Additionally, in \autoref{DatasetsCounts} we show the number of annotated object
instances per class in all used datasets combined.

\begin{table}[h]
\centering
\begin{tabular}{|l|rr|p{5cm}|}
    \hline
    Dataset tag & \# images & \# instances & Comments \\
    \hline
    DETRAC      &  \num{99771}  & \num{733833} & Large \newline Continuous video \newline High-quality camera \newline Different lighting conditions \\
    \hline
    MIO-TCD     &  \num{110000} & \num{344416} & Large \newline Low-quality images \\
    \hline
    AAU         &    \num{1899} &  \num{10545} & Small \newline Different weather conditions \\
    \hline
    MTID        &    \num{5399} &  \num{64979} & Small \newline Continuous video \\
    \hline
    NDISPark    &     \num{142} &   \num{3302} & Small \newline Occlusions \\
    \hline
    VisDrone    &    \num{1610} &  \num{47720} & Small \newline Negative images \newline New points of view \\
    \hline
    \hline
    Total       &  \num{218821} & \num{1204795} & - \\
    \hline
\end{tabular}
\caption{High-level comparison of used datasets\,--\,the number of images and object instances and additional comments for each of the datasets.}
\label{DatasetsSummary}
\end{table}

\begin{table}[h]
\centering
\begin{tabular}{|l|r|}
    \hline
    Class         & \# instances \\
    \hline
    Bicycle       &  \num{14036} \\
    Motorcycle    &  \num{17187} \\
    Passenger car & \num{916317} \\
    Transporter   & \num{123585} \\
    Bus           &  \num{64529} \\
    Truck         &  \num{38069} \\
    Trailer       &   \num{2360} \\
    Unknown       &  \num{28712} \\
    \hline
    \hline
    Total         & \num{1204795} \\
    \hline
\end{tabular}
\caption{Numbers of class instances in all datasets combined.}
\label{DatasetsCounts}
\end{table}


\section{Training, Validation and Testing Dataset Split}

We chose to only include the Miovision and DETRAC-UA datasets for validation and
testing, because they represent data from a typical traffic surveillance camera
the best. 

Because only the train subset of the Miovision dataset was used (only this
subset contained annotations), the images used for validation and testing are
chosen randomly. This, however, should not be a problem since the dataset
contains many different camera angles and each image is very unique.

From the DETRAC-UA dataset, two sequences were selected for the validation
subset (\texttt{MVI\_40201} and \texttt{MVI\_40244}) and two for the test subset
(\texttt{MVI\_40204} and \texttt{MVI\_40243}). The chosen sequences are very
different from the ones in the training subset, which is important for the
evaluations to be accurate. However, the \texttt{MVI\_40201} sequence is
recorded from the same angle as \texttt{MVI\_40204} and the same applies to
sequences \texttt{MVI\_40244} and \texttt{MVI\_40243}, so the validation and
test subsets are alike, but of course, contain different data. Example images
are shown in \autoref{TestValExamples}.

\begin{figure}
    \centering
    \begin{subfigure}[b]{0.475\textwidth}
        \includegraphics[width=\textwidth]{dataset_example_MVI_40201.jpg}
        \caption{\texttt{MVI\_40201} (validation subset)}
    \end{subfigure}
    \begin{subfigure}[b]{0.475\textwidth}
        \includegraphics[width=\textwidth]{dataset_example_MVI_40204.jpg}
        \caption{\texttt{MVI\_40204} (test subset)}
    \end{subfigure}
    \begin{subfigure}[b]{0.475\textwidth}
        \includegraphics[width=\textwidth]{dataset_example_MVI_40241.jpg}
        \caption{\texttt{MVI\_40241} (validation subset)}
    \end{subfigure}
    \begin{subfigure}[b]{0.475\textwidth}
        \includegraphics[width=\textwidth]{dataset_example_MVI_40243.jpg}
        \caption{\texttt{MVI\_40243} (test subset)}
    \end{subfigure}
    \caption{Example images from sequences from the DETRAC-UA dataset used for
    the validation and testing subsets.}
    \label{TestValExamples}
\end{figure}

The validation subset contains a total of \num{36969} objects on \num{7770}
images, of which \num{5500} images are from the Miovision dataset and \num{2270}
from DETRAC-UA. Similarly, the test subset contains a total of \num{50357}
objects on \num{7990} images\,--\,\num{5500} images from the Miovision dataset and
\num{2490} from DETRAC-UA.




\chapter{Object Detection Models}

\todo{Nedať to k experimentom? Asi to nepotrebuje vlastnú kapitolu, no}

% In this chapter, the reader will find all information about the used object
% detection models, from their architecture to the deployment process to
% understand what exactly will be evaluated.

% First, the architectures of evaluated models will be


In this chapter, the reader will find an overview of used object detection
models, their training configurations and information about the training
process.


\section{Model Architectures}


This section introduces all object detection architectures evaluated in this
paper.  The main focus is on the YOLOv8 object detector, which is currently the
state-of-the-art real-time object detector and offers different model sizes for
different applications.

Along with standard model sizes, YOLOv8-medium, YOLOv8-small and YOLOv8-nano,
several others were trained and evaluated in this work: YOLOv8-pico and
YOLOv8-femto model versions, which are simply smaller versions of the same
YOLOv8 model. Finally, a YOLOv8-large model with the CSP Darknet backbone
replaced by a popular MobileNetV2 backbone is introduced (hereafter referred to
as YOLOv8 MobileNetV2).

In the YOLOv8 MobileNetV2 model, the output indices\footnote{Output indices
enable the selection of specific layers in the architecture, from which the
output feature maps should be used. In this case, only these feature maps are
then used as inputs to the object detector's neck.} selected were $[2, 4, 6]$.
Several models with different \texttt{out\_indices} settings were trained for
just 75 epochs to compare their performance, however, none of the alternative
settings led to improved performance (in terms of both inference speed and
accuracy).

Normally, a square resolution is used for the model's input, but standard traffic cameras
output a video of a rectangular shape, usually with the aspect ratio being 16:9.
To optimize the inference speed, a rectangular input resolution is used for all
trained detectors. The aim was to use aspect ratios as close to 16:9 as
possible, but the widths and heights of the input resolution have to be multiples of
32, so some input resolutions are further from it than others.

For smaller models, several input resolutions were tested to provide insights
into how a vehicle detection model can be optimized by decreasing the resolution
of the input image. Although the inference speed should be (approximately)
directly proportional to the number of pixels in the input image, we decided to
test it and also find the limit\,--\,how small can the model's input resolution
be for it to start performing poorly on our datasets.

For a summary of all used model architectures, their sizes, input resolutions and the
amounts of their floating point operations (FLOPS) and parameters, see
\autoref{ModelArchitectures}. Quantities of floating point operations and
parameters were calculated using MMDetection's analysis script
\verb|get_flops.py|. Although the script calculates the output using input
resolution $720 \times 480$, simply multiplying the output by the difference
between the input resolutions provides an accurate result. This can be explained
by the following equation:
\begin{equation}
    N_{new} = N_{720 \times 480} \times \frac{new\_width \times new\_height}{720 \times 480}
\end{equation}

\todo{napísať tu toho viac?}

\begin{table}[h]
\centering
\small
\begin{tabular}{|c|c|c|c|c|c|}
    \hline
    \multirow{2}{*}{Architecture}& Deepen                 & Widen                   & Input      & Number of floating & Number of \\
                                 & factor                 & factor                  & resolution & point operations   & parameters \\
    \hline
    \hline
    YOLOv8-medium                & 0.67                   & 0.75                    & $640 \times 384$ & 7.85 G & 18.39 M \\
    \hline
    YOLOv8-small                 & 0.33                   & 0.5                     & $640 \times 384$ & 2.63 G & 3.73 M \\
    \hline
    YOLOv8 MobileNetV2           & 1                      & 1                       & $512 \times 288$ & 0.560 G & 2.133 M \\
    \hline
    \multirow{3}{*}{YOLOv8-nano} & \multirow{3}{*}{0.33}  & \multirow{3}{*}{0.25}   & $640 \times 384$ & 0.758 G & 1.688 M \\
                                 &                        &                         & $512 \times 288$ & 0.455 G & 1.013 M \\
                                 &                        &                         & $448 \times 256$ & 0.354 G & 0.788 M \\
    \hline
    \multirow{3}{*}{YOLOv8-pico} & \multirow{3}{*}{0.166} & \multirow{3}{*}{0.125}  & $512 \times 288$ & 0.1510 G & 0.3067 M \\
                                 &                        &                         & $448 \times 256$ & 0.1175 G & 0.2386 M \\
                                 &                        &                         & $384 \times 224$ & 0.0881 G & 0.1790 M \\
    \hline
    \multirow{4}{*}{YOLOv8-femto}& \multirow{4}{*}{0.166} & \multirow{4}{*}{0.0625} & $512 \times 288$ & 0.0780 G & 0.1288 M \\
                                 &                        &                         & $448 \times 256$ & 0.0607 G & 0.1002 M \\
                                 &                        &                         & $384 \times 224$ & 0.0455 G & 0.0751 M \\
                                 &                        &                         & $352 \times 192$ & 0.0358 G & 0.0591 M \\
    \hline
\end{tabular}
\caption{Summary of different YOLOv8 model architectures used and comparison of amounts of their floating point operations and parameter with various model input resolutions.}
\label{ModelArchitectures}
\end{table}


\section{Model Configurations}


This section provides an overview of configurations used to train the vehicle
detectors. A configuration of an object detection model when using the
MMDetection or the MMYOLO library contains a set of parameters that define the
model's behavior and the training, validation and testing pipelines. These
parameters can have a significant impact on the model's speed and accuracy and
tuning them is essential to achieve good results.

Most of the YOLOv8 parameters are left unchanged from the default configuration
of YOLOv8-m\footnote{The default YOLOv8-m configuration used,
\texttt{yolov8\_m\_syncbn\_fast\_8xb16-500e\_coco.py}, can be found at
\url{https://github.com/open-mmlab/mmyolo/tree/v0.4.0/configs/yolov8}}, like: \\
\textbf{Optimizer:} Stochastic Gradient Descent with momentum \num{0.937} and weight decay \num{0.0005} \\
\textbf{Parameter scheduler:} Linear \verb|YOLOv5ParamScheduler| with learning rate factor of \num{0.01}

However, many parameters related to datasets and augmentations were adjusted and
will be explained in the next subsections. Apart from those, the only relevant
parameter that was changed is the batch size, which was set to the highest
possible for every trained model. For the smallest one, YOLOv8-femto with $352
\times 192$ input resolution, the largest batch size of 760 was used. Because
training batch sizes above 128 usually result in lower model precision \cite{LargeBatch}, models
with large batch sizes were trained for 500 epochs instead of the default 300
epochs. Along with other model-specific training parameters, including the
number of warmup epochs\footnote{During warmup epochs, the learning rate is
gradually increased from a lower value to the target learning rate.}, these
settings can be found in \autoref{ModelSpecificConfigurations}.


\begin{table}[h]
\centering
\small
\begin{tabular}{|c|c|c|c|c|c|}
    \hline
    \multirow{2}{*}{Architecture}& \multirow{2}{*}{Input resolution} & Learning & Batch & \multirow{2}{*}{Epochs} & Warmup \\
                                 &                                   & rate     & size  &                         & epochs \\
    \hline
    \hline
    YOLOv8-medium                & $640 \times 384$ & 0.00125 & 46  & 300 & 5 \\
    \hline
    YOLOv8-small                 & $640 \times 384$ & 0.00125 & 76  & 300 & 5 \\
    \hline
    YOLOv8 MobileNetV2           & $512 \times 288$ & 0.01    & 96  & 300 & 5 \\
    \hline
    \multirow{3}{*}{YOLOv8-nano} & $640 \times 384$ & 0.00125 & 112 & 300 & 5 \\
                                 & $512 \times 288$ & 0.00125 & 192 & 300 & 5 \\
                                 & $448 \times 256$ & 0.00125 & 256 & 300 & 5 \\
    \hline
    \multirow{3}{*}{YOLOv8-pico} & $512 \times 288$ & 0.01    & 224 & 500 & 10 \\
                                 & $448 \times 256$ & 0.01    & 384 & 500 & 10 \\
                                 & $384 \times 224$ & 0.01    & 512 & 500 & 10 \\
    \hline
    \multirow{4}{*}{YOLOv8-femto}& $512 \times 288$ & 0.01    & 380 & 500 & 10 \\
                                 & $448 \times 256$ & 0.01    & 420 & 500 & 10 \\
                                 & $384 \times 224$ & 0.01    & 640 & 500 & 10 \\
                                 & $352 \times 192$ & 0.01    & 760 & 500 & 10 \\
    \hline
\end{tabular}
\caption{Training configurations for individual models, including learning rate, batch size, number of training epochs and number of warmup epochs.}
\label{ModelSpecificConfigurations}
\end{table}


\subsection{Dataset Wrappers}

In the MMYOLO (and the MMDetection) model configurations, datasets to use for
training, validation and testing are specified using a dataset wrapper, from
which a \texttt{dataloader} (an object internally representing a dataset) is
created. Because the datasets used in this project were in the COCO format, the
\texttt{YOLOv5CocoDataset} wrapper was used for each of the 6 used datasets.

To compensate for some datasets being smaller than others while being important
and of high quality, a dataset wrapper \texttt{RepeatDataset} is used, which
makes the underlying dataset n-times more frequent when training. All datasets are finally
concatenated into one by the \texttt{ConcatDataset} wrapper. The repetition
factors of individual datasets are shown in \autoref{RepetitionFactors}.

\begin{table}[h]
\centering
\small
\begin{tabular}{|c|c|c|c|}
    \hline
    Dataset name & \# images & Repetition factor & \# images after over-sampling \\
    \hline
    DETRAC       &  \num{99771} &  1 & \num{99771} \\
    MIO-TCD      & \num{110000} &  1 & \num{110000} \\
    AAU RainSnow &   \num{1899} &  3 & \num{5697} \\
    MTID         &   \num{5399} &  5 & \num{26995} \\
    NDISPark     &    \num{142} & 25 & \num{3550} \\
    VisDrone     &   \num{1610} &  4 & \num{6440} \\
    \hline
\end{tabular}
\caption{Repetition factors for each dataset used when training.}
\label{RepetitionFactors}
\end{table}



\subsection{Training Augmentation Pipeline}

In this subsection, the training augmentation pipeline is explained. Data
augmentations are important when training an object detector, especially
detectors of the YOLO family. An augmentation in this context refers to the
process of applying a transformation to an image to artificially increase the
size and diversity of the training dataset, which helps prevent overfitting and
improves the generalization ability of the trained model. To a convolutional
neural network, even a tiny rotation, translation, image flip, noise or color
distortion makes an input image appear to be something completely different, so
applying these transformations randomly to the input images is crucial when
training a robust model. Examples of images augmented by the training
augmentation pipeline can be seen in \autoref{AugmentedExamples}. Following are
the transformations in the main training augmentation pipeline:

\begin{figure}[t]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{augmented_detrac.jpg}
        \caption{DETRAC-UA}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{augmented_mio-tcd.jpg}
        \caption{MIO-TCD}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{augmented_mtid.jpg}
        \caption{MTID}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{augmented_ndis.jpg}
        \caption{NDISPark}
    \end{subfigure}
    \caption{Examples of images augmented by the training augmentation pipeline,
    including the ground truth labels.}
    \label{AugmentedExamples}
\end{figure}


\subsubsection*{Resize}

First, the image is resized to fit the model's input resolution while, of
course, keeping the aspect ratio unchanged.

\subsubsection*{Pad}

If the aspect ratio of the input image is not the same as the model's input, extra pixels around the input image must be added
to adapt to the model input's aspect ratio. The \textbf{color value} of the padded pixels is set to RGB(114, 114, 114).

\subsubsection*{Random Affine}

The \texttt{YOLOv5RandomAffine} applies affine transformations to the image,
while randomly selecting the values from configured ranges. Parameters:\\
\textbf{Maximum translation ratio:} 0.05 \\
\textbf{Maximum rotation degree:} 5 \\
\textbf{Maximum shear degree:} 3 \\
\textbf{Scaling ratio} is set individually for each dataset as shown in \autoref{RandomAffineScalingRatios}.

\begin{table}[h]
\centering
\begin{tabular}{|c|c|c|}
    \hline
    Dataset name & Minimum scaling ratio & Maximum scaling ratio \\
    \hline
    DETRAC       & 0.8 & 1.0 \\
    MIO-TCD      & 1.0 & 1.1 \\
    AAU RainSnow & 0.9 & 1.1 \\
    MTID         & 0.9 & 2.0 \\
    NDISPark     & 0.9 & 1.5 \\
    VisDrone     & 1.5 & 2.5 \\
    \hline
\end{tabular}
\caption{Image scaling ratios in the \texttt{RandomAffine} transformation for each dataset.}
\label{RandomAffineScalingRatios}
\end{table}

The \textbf{color value for padding} around the tranformed image (if needed)
is set to RGB(114, 114, 114) to be the same as in the \texttt{Pad} transformation.

\subsubsection*{Cut-Out}

The \texttt{CutOut} transformation randomly selects regions of the image and
fills them with a single color. Again, parameters of this transformation are set
individually for each dataset and are shown in \autoref{CutOutParams}.

\begin{table}[h]
\centering
\begin{tabular}{|c|c|c|}
    \hline
    Dataset name & Number of regions & Size of a single region \\
    \hline
    DETRAC       &  6 & $22 \times 22$ px \\
    MIO-TCD      &  4 & $26 \times 26$ px \\
    AAU RainSnow &  8 & $10 \times 10$ px \\
    NDISPark     & 12 & $20 \times 20$ px \\
    MTID         & 12 & $10 \times 10$ px \\
    VisDrone     & 20 & $8 \times 8$ px \\
    \hline
\end{tabular}
\caption{\texttt{CutOut} transformation parameters for each dataset.}
\label{CutOutParams}
\end{table}

The \textbf{fill color value} is again set to RGB(114, 114, 114), same as for
padding in the previous transformations.

\subsubsection*{Custom Cut-Out}

A custom cut-out transformation was developed, similar to \texttt{CutOut} used
in the previous step. Here, the regions are selected within each bounding box
with a certain probability, rather than being chosen randomly within the entire image.
Also, the region size is specified as a range of areas in relation to the
bounding box area - if the upper value is $10\%$ and a bounding box area is 100
pixels, the maximum cut-out region area can be 10 pixels.

With the boolean option \texttt{random\_pixels} toggled, the color of each pixel
of a cut-out region is generated randomly instead of filling it with a
pre-defined color. However, it was found to have no effect.

The \textbf{probability} of a region being dropped from each bounding box is set
to \num{0.05} ($5 \%$) and the \textbf{region area} is set to be randomly selected from
interval $[5\%, 35\%]$.

\subsubsection*{Albumentations}

Albumentations \cite{Albumentations} is a popular open-source library for data
augmentation. The MMYOLO library provides the option to use its transformations
in the data augmentation pipeline. Settings are left unchanged from the original
YOLOv8-m configuration: \\
\textbf{Blur probability:} 0.01 \\
\textbf{Median blur probability:} 0.01 \\
\textbf{Grayscale probability:} 0.01 \\
\textbf{CLAHE probability:} 0.01

\subsubsection*{HSV Random Augmentations}

The \texttt{YOLOv5HSVRandomAug} simply adjusts the hue, saturation and value of
the image randomly.

\subsubsection*{Random Flip}

With \textbf{probability} of 0.5, the image is horizontally flipped using the
\texttt{RandomFlip} augmentation.

\subsubsection*{Photometric Distortion}

The \texttt{PhotoMetricDistortion} augmentation distorts an image sequentially,
while each transformation is applied with a probability of 0.5. It modifies the
brightness, contrast, converts color from BGR\footnote{BGR (Blue, Green, Red)
only differs from the well known RGB format by the order of its channels. It is
widely used in image processing for legacy and compatibility reasons.} to
HSV\footnote{HSV (Hue, Saturation, Value) is a cylindrical-coordinate color
model, where hue represents the color type, saturation refers to the color's
intensity and value corresponds to brightness.}, modifies the saturation,
hue, converts from HSV to BGR, modifies the contrast and finally, randomly swaps
the color channels.

\subsubsection*{Filter Annotations}

As the last step in the pipeline, \texttt{FilterAnnotations} is called to remove
bounding boxes with \textbf{width or height} lower than \num{8} pixels.

\subsubsection{Fine-Tuning Augmentation Pipeline}

Originally, MMYOLO's YOLOv8 models are configured to switch to a simplified
augmentation pipeline for the last 10 training epochs. This model fine-tuning
strategy is kept and in the augmentation pipeline and for the last 10 training
epochs, cut-out augmentations are omitted and affine transformations are changed
so that no rotation, translation or shear is applied to a sample.

However, this setting seemed to cause a consistent decrease in the validation
mean Average Precision (mAP) metric during the fine-tuning phase (last 10
epochs).



\todo{že ktorú epochu som vybral pre každý model?? Aj grap validačného mAP? To asi nie}





\chapter{Experiments}
\label{Experiments}


\section{Devices Used in Experiments}

While the main focus is on NVIDIA Jetson embedded devices, which were designed
specifically for tasks like object detection, tests were run on several other
devices for comparison of both ends of the performance gauge. In this section,
details about each device that the models were evaluated on are provided,
including details about the software and device configurations.

% \subsection{NVIDIA RTX A5000} # Nedá sa spojazdniť, treba sudo

\subsection{NVIDIA Jetson Platforms}

Technical details of NVIDIA Jetson embedded platforms were discussed in
\autoref{Jetsons} and software versions will be shown later in this section.
However, one more important detail to note before reading about the experiments
is the used power plan. All used Jetson devices feature several power plans to
choose from to adjust the performance and power consumption for a specific task.
For all experiments, these power plans were chosen: \\
\textbf{NVIDIA Jetson AGX Xavier}: 30 W power plan with 4 out of 8 cores running \\
\textbf{NVIDIA Jetson Xavier NX}: 20 W power plan with 4 out of 8 cores running \\
\textbf{NVIDIA Jetson Nano}: 10 W \texttt{MAXN} power plan with all 4 cores running


\subsection{NVIDIA GeForce MX150}

To be able to compare inference speeds on these embedded devices to ones on a
regular GPU, tests were also done on an NVIDIA GeForce MX150 GPU with 2 GB of
memory on a DELL Latitude 5401 laptop. Because of the GPU memory constraint, not
all models can be evaluated with all inference batch sizes on this device, as
will be pointed out in individual relevant experiments. Additionally, we were
unable to perform any tests using the TensorRT library on this device because
some of the packages required could not be installed on the system.


\subsection{Intel Core i7-9850H}

Models were also evaluated on a higher-end laptop CPU, Intel Core i7-9850H with
the base frequency 2.6 GHz to demonstrate how the inference speeds of YOLOv8
object detection models differ between a GPU and a CPU.

Typically, the frequency at which a CPU operates is adjusted to accommodate the
load and it often spikes up when a computation-hungry process starts. After a
while, when the CPU temperature rises above a certain threshold, the CPU
frequency has to drop to avoid overheating. This is called dynamic frequency
scaling, also known as CPU throttling. To make the performance measurements as
accurate as possible, the number of warmup samples when testing is increased
from 10 to 100. This means that the inference speed (FPS) of these first 100
samples will be ignored when calculating the average FPS.

However, please note that the test results still not might be accurate enough
and might depend on the underlying operating system, running applications and
the environment.


\subsection{Raspberry PI 4B}

Finally, the popular Raspberry PI 4B single-board computer with ARM Cortex-A72
CPU with base frequency 1.8 GHz was used to test the trained models. Although it
was not developed to run object detection models, it is perfect to test the
smallest models and see how far go the possibilities of the real-time YOLOv8
object detector.


\subsection{Software versions}

Information about software installed on all previously mentioned devices can be
found here. See \autoref{DevicesPackages} for a compact table displaying
versions of relevant software. Following are the reasons behind some odd choices
or compatibility issues.

\begin{table}[t]
\centering
\small
\begin{tabular}{|l|c|c|c|c|}
    \hline
    % \multirow{2}{*}{Name} & NVIDIA Jetson & NVIDIA Jetson & NVIDIA Jetson \\
    %                       & AGX Xavier    & Xavier NX     & Nano          \\
    % Name             & NVIDIA Jetson devices & NVIDIA MX150 & Intel Core i7-9850H & Raspberry PI \\
    \multirow{2}{*}{Name} & All NVIDIA     & NVIDIA & Intel Core  & Raspberry \\
                          & Jetson devices & MX150  & i7-9850H    & PI 4B \\
    \hline
    \multirow{2}{*}{Operating system} & Linux for Tegra             & \multicolumn{2}{c|}{Linux}     & Linux   \\
                                      & (L4T OS \texttt{32.7.3}) & \multicolumn{2}{c|}{Debian 12} & Raspbian 11   \\
    \hline
    JetPack SDK      & \texttt{4.6.3}                     & \multicolumn{3}{c|}{-}  \\
    \hline
    Python           & \texttt{3.6.9}                     & \multicolumn{2}{c|}{\texttt{3.10.0}} & \texttt{3.7.0} \\
    \hline
    CUDA             & \texttt{10.2}                      & \texttt{11.8} & \multicolumn{2}{c|}{-} \\
    \hline
    TensorRT         & \texttt{8.2.1} & \multicolumn{3}{c|}{-}             \\
    \hline
    ONNX             & \multicolumn{3}{c|}{1.13.1} & \texttt{1.12.0}             \\
    \hline
    ONNX Runtime     & \texttt{1.11.0} (ver. GPU) & \texttt{1.12.0} (ver. GPU) & \texttt{1.12.0} & \texttt{1.11.0}            \\
    \hline
    PyTorch          & \texttt{1.10.0} & \multicolumn{2}{c|}{\texttt{2.0.0}} & \texttt{1.8.0} \\
    \hline
    MMCV             & \multicolumn{4}{c|}{\texttt{2.0.0}} \\
    \hline
    MMDeploy         & \multicolumn{3}{c|}{\texttt{1.0.0}} & \texttt{1.0.0rc3} \\
    \hline
    MMDetection      & \multicolumn{4}{c|}{\texttt{3.0.0}} \\
    \hline
    MMEngine         & \multicolumn{4}{c|}{\texttt{0.7.2}} \\
    \hline
    MMYOLO           & \multicolumn{4}{c|}{\texttt{0.4.0}} \\
    \hline
\end{tabular}
\caption{Versions of relevant software installed on devices used to deploy and test the trained models.}
\label{DevicesPackages}
\end{table}


\subsubsection*{JetPack SDK}

Despite both Jetson Xavier NX and Jetson AGX Xavier being supported by NVIDIA
JetPack SDK version \texttt{5.1.1}, deploying YOLOv8 models using this version
often led to an untraceable fatal error. The error originated from one of
NVIDIA's proprietary libraries and provided limited information regarding its
cause. Fortunately, downgrading the JetPack version to \texttt{4.6.3} resolved
this issue.

Although the root cause of the error and the specific package (or library) that
required downgrading were not fully determined, it is suspected that the problem
was related to TensorRT version \texttt{8.5.2}, as the error originated from the
TensorRT development library \texttt{libnvinfer}.

For the sake of providing further context to the reader, the error message
received was \verb|operation.cpp:203: DCHECK(!i->is_use_only()) failed|. No
other relevant warnings or error messages preceded this one, making it
challenging to pinpoint the exact cause.


\subsubsection*{Old Python Version on Jetson Devices}

All NVIDIA Jetson devices used to evaluate the trained models utilize the same
version of the JetPack SDK, \texttt{4.6.3}, which includes Python version
\texttt{3.6.9}. However, several essential Python packages\,--\,specifically,
\texttt{protobuf} version \texttt{3.20.2}, \texttt{MMCV} version \texttt{2.0.0}
and \texttt{MMEngine} version \texttt{0.7.2}\,--\,require a Python version of
\texttt{3.7.0} or higher. As these packages (and the specified versions) were
crucial for the model deployment and testing to be possible, we had to manually
modify their requirements to allow for installation with Python version
\texttt{3.6.9}.

Of course, this approach is not an ideal solution to the problem and its success
was not guaranteed.  Fortunately, no indications of compatibility issues were
discovered during model deployment or testing.

One might suggest that upgrading to a newer Python version would be the most
appropriate solution. However, due to compatibility and dependency constraints
on Jetson devices, this is not feasible.

JetPack version \texttt{5.1.1} includes a more recent Python version but does
not support Jetson Nano, and when installed on Jetson Xavier NX or Jetson AGX
Xavier, the deployment of the trained models fails, as explained earlier in this
section. Although installing a different Python version than the one provided
with the JetPack SDK is possible, installing other necessary packages for the
newer Python is not. This is because many such packages were developed
specifically for Jetson devices and only support a certain Python
version\,--\,the one pre-installed with the JetPack SDK.


\section{Model Deployment and Optimizations}

\todo{niekde napísať ktoré epochy som nakoniec vybral pre evaluation a že to je čisto na základe validačných mAP?}

For model deployment, we have created an automated script utilizing the MMDeploy
library's deployment script. Our script uses all available deploy
configurations\footnote{When deploying models using MMDeploy, deploy
configuration files are used to specify the parameters of the deployment process,
including target backend or whether to apply post-training quantization} to
deploy all trained PyTorch models to a specified backend (or backends).

There are only two deploy configurations for the ONNX Runtime backend\,--\,one
with a static model shape\footnote{A model shape can be static or dynamic. A
static model can only receive inputs of a certain shape specified when deploying
(batch size and input resolution\,--\,width and height), whereas a dynamic model
accepts various input shapes.} and one with a dynamic shape in the batch size
dimension\footnote{A dynamic model shape in the batch size dimension means the
model accepts different input shapes but still requires a fixed input image
resolution.}. The dynamic models were deployed to accept a maximum batch size of
32, but were optimized for a batch size of 1.  For the TensorRT backend, two
additional deploy configurations were created, both for a model with a dynamic
shape in the batch dimension\,--\,one for weight quantization to the
\texttt{FP16} representation, and one for weight quantization to the
\texttt{INT8} representation including weight calibration using the validation
dataset.

To optimize inference on the Raspberry PI 4B, we aimed to use the NCNN inference
framework designed for mobile and embedded devices with limited computing
resources. However, deploying a YOLOv8 model to the NCNN format is not yet
possible, as the YOLOv8 model contains operations that are not yet supported by
the framework and the MMYOLO library does not yet support converting the
unsupported operations to supported ones.


\subsection*{Devices Used for Deployment}

The models in the ONNX format were all deployed on a single device (with ONNX
package version \texttt{1.13.1}) and distributed to all devices. The TensorRT
backend is only used on NVIDIA Jetson devices and all models were deployed to
the TensorRT engine\footnote{A TensorRT engine is a deployed model in the
TensorRT format.} individually on each NVIDIA Jetson device, because they are
platform-specific and transferring them across different devices is not
recommended.

% Overall, the ONNX Runtime library was used to test the trained models on all
% devices, although, for the NVIDIA MX150 and NVIDIA Jetson devices, the GPU
% version of the Python package was used. The TensorRT library, on the other hand, was
% only used for experiments on NVIDIA Jetson devices.

% For the rest of the devices, the popular ONNX (Open Neural Network Exchange)
% model format was selected. It was designed to represent models in a portable and
% efficient way, enabling them to be run on a variety of hardware platforms. To
% run inference on a model in the ONNX format, the ONNX Runtime inference engine
% can be used. It was designed to optimize the inference of ONNX models across
% different hardware platforms and operating systems, including CPUs, GPUs, FPGAs
% and ASICs.


\subsection*{Weight Quantization}

The MMDeploy library supports post-training quantization to \texttt{FP16} and
\texttt{INT8} representations during the process of model deployment to TensorRT.
To preserve the model's accuracy after quantization to \texttt{INT8} precision, weight calibration
was done using the validation dataset.


Although the ONNX Runtime framwork also supports quantization (to both
\texttt{FP16} and \texttt{INT8}), the process is not as straightforward and
doesn't seem to be supported by the MMDeploy library. Although evaluating models
quantized to \texttt{INT8} representation would be beneficial, the quantization
to the \texttt{FP16} precision would probably have a little effect on performance on CPUs as
they do not usually support operations with this precision and calculate them using the same
operators as numbers in the \texttt{FP32} representation.

Additionally, TensorRT on the NVIDIA Jetson Nano with Maxwell GPU doesn't
support fast inference of models quantized to \texttt{INT8}, so quantization to
\texttt{INT8} won't be performed on this device.



\section{Experiments and Evaluation}
% Metrics
% What models are tested (onnx, tensorrt, quantized, static, dynamic, batches..)
% How tests are ran (MMDeploy)
% Tiež zmieniť ONNX inter a intra thread!

In this section, we will discuss additional details of all performed
experiments\,--\,mainly the combinations of models, devices, inference backends,
deployment configurations, optimization techniques and batch sizes of individual
tests. Insights gained from these experiments and additional experiment-specific
details will be discussed in \autoref{Results}.

All experiments will come from data collected by testing. Testing was done using
a Python script included as a tool in the MMDeploy library, \texttt{test.py},
which uses the library to create a wrapper for a deployed model. It then uses
the test dataset provided to evaluate the model's performance, calculating the
inference speed in FPS and mAP metrics. The inference speed is calculated
including the input pre-processing and non-maximum
suppression\footnote{Non-maximum suppression in object detection is a method
that selects a single prediction bounding box out of several overlapping
bounding boxes, which are likely generated for the same object.} (NMS).

To test all possible combinations of models, backends, deployment configurations
and batch sizes on individual devices automatically, a new Python script
\texttt{test\_all.py} was developed, which uses the MMDeploy's \texttt{test.py}
script and executes it for each possible test combination (unless specified
differently by the user).

The MMDeploy's \texttt{test.py} saves the test logs, from which the mAP metrics
and inference speed can be read. This is also done automatically by our
\texttt{collect\_test\_results.py} Python script, which reads all available test
logs to output all metrics structured in JSON format to a file. Although the
\texttt{test.py} script also outputs the final test metrics in JSON format to a
folder, the name of the folder cannot be specified and is named after the test
start time (eg. \texttt{20230429\_043829/}), so reading the data from the log file
is simpler and less error-prone.

When testing models on CPUs in the ONNX format using the MMDeploy library, the
library doesn't control how many processes the ONNX Runtime library uses to run
inference. To provide accurate results, the source code was modified to create
the ONNX Runtime inference session with the option of only running in a single
thread. This means that the inference speeds on CPUs are not the highest
possible. However, simply multiplying the FPS metric by the number of CPU cores
to get the maximum inference speed possible on the device is generally
incorrect, because running a multi-threaded inference is not as efficient as
running it on a single thread. On the other hand, if running $n$ separate
inference processes on a CPU with $n$ cores, it is theoretically possible to
achieve inference speeds $n$ times higher than with a single-threaded inference,
although the inference duration would remain the same.

\todo{Možno tabuľka všetkých testov alebo tak? Alebo, ako na začiatku sľubujem, napísať že testujem všetky modely atď.}










\chapter{Results}
\label{Results}
povedať krátko že v akom poradí to budem evaluovať a napr. že graf FPS/mAP bude až na konci alebo tak








\section{Precision-Recall Curves of Major Models}

% python3 pr_curve.py working_dir_yolov8_m_640x384_lr0.00125/ working_dir_yolov8_l_mobilenet_v2_512x288_indices_246/ working_dir_yolov8_n_512x288/ working_dir_yolov8_f_352x192/
% A pre appendix:
% python3 pr_curve.py working_dir_yolov8_s_640x384/ working_dir_yolov8_n_640x384/ working_dir_yolov8_n_448x256/ working_dir_yolov8_p_512x288/ working_dir_yolov8_p_448x256/ working_dir_yolov8_p_384x224/ working_dir_yolov8_f_512x288/ working_dir_yolov8_f_448x256/ working_dir_yolov8_f_384x224/

In this experiment, we measured the precision and recall values of four of the
trained models on the test dataset to plot the precision-recall (PR) curves in
\autoref{PRCurveMajor}. We could not show the curves of all the models because
the figure would simply not fit on a single page, so instead, only the four most
representative models were selected. A more complete figure with PR curves for
the rest of the models can be seen in \autoref{PRCurvesAdditional}. The
precision and recall values were calculated by MMDeploy's test script, to which
we have inserted a few lines of code to save the values to a file.

In the plots, the performance for the bus class appears abnormally high. We
hypothesize that the reason behind it is the presence of a few
easily-detectable, large buses in numerous images from the DETRAC-UA dataset
within the testing set. Additionally, the low performance of models in detecting
vehicles belonging to the \textit{unknown} category should not be a major
concern, as the diversity of the vehicles in this category makes it difficult to
accurately detect and classify them.

As could be expected, the YOLOv8-medium model demonstrated superior performance,
achieving the highest precision and recall values. Furthermore, the performance
of the YOLOv8 MobileNetV2 model is comparable to that of the YOLOv8-nano,
although slightly superior. In contrast, the YOLOv8-femto with an input
resolution of $352 \times 192$ could be an example of a model with considerably
lower performance.

\begin{figure}[H]
    \begin{framed}
        \centering
        \includegraphics[width=\textwidth]{pr_curve.pdf}
        % \caption{upozorniť že nano je s 512x288. A že som IoU thresholdy priemeroval (to by som mohol písať tam kde budem písať o skripte)}
        \caption{Precision-Recall curves are displayed for four major models,
        including YOLOv8-femto with the smallest input resolution of $352 \times 192$. It
        is important to note that, in this figure, the YOLOv8-nano model chosen for
        comparison has an input resolution of $512 \times 288$, although most experiments
        used the model with an input resolution of $640 \times 384$. This decision was
        made to simplify the comparison between YOLOv8-nano and YOLOv8 MobileNetV2.}
        \label{PRCurveMajor}
    \end{framed}
\end{figure}






\section{Effect of Model's Input Resolution on Inference Speed}
\label{InputResolutionVSFPSExperiment}

Smaller models\,--\,YOLOv8-nano, YOLOv8-pico and YOLOv8-femto were trained with
different input resolutions to compare how the image resolution affects
the model's performance on our test dataset.

We expect that the reduction in the number of pixels in a model's input resolution
should be directly proportional to the increase in its inference speed (FPS).
For example, we anticipate that a detector with the input resolution of $640 \times
320$ would be twice as fast compared to a detector with the input resolution $640
\times 640$. However, this hypothesis needed to be tested to bIn this experiment, we measured the precision and recall values of four of the
trained models on the test dataset to plot the precision-recall (PR) curves in
\autoref{PRCurveMajor}. We could not show the curves of all the models because
the figure would simply not fit on a single page, so instead, only the four most
representative models were selected. A more complete figure with PR curves for
the rest of the models can be seen in \autoref{PRCurvesAdditional}.e proven correct
In \autoref{InputResolutionVSFPS}, we compare the inference speed of models with
different input resolutions. The biggest input resolution is selected as the base to which
other input resolutions will be compared to, and expected FPS is calculated using
this simple equation:
\begin{equation}
    \text{Expected FPS} = \text{Base FPS} \times \frac{\text{Base input width} \times \text{Base input height}}{\text{Compared input width} \times \text{Compared input height}}
\end{equation}

While our experiments showed that the relationship between input resolution and FPS
cannot be exactly captured by the above equation, the results generally
supported our hypothesis. Following this experiment, we will suppose that the
input resolution reduction is indeed directly proportional to the inference speed
increase, and in some of the following experiments, we will only compare the
biggest input resolutions of each model to make the results more informative and
concise.

\begin{figure}[t]
    \begin{framed}
        \centering
        \includegraphics[width=\textwidth]{input_resolution_vs_fps.pdf}
        % \caption{nano tensorrt, dynamic, batch16}
        \caption{Inference speed (FPS) comparison of YOLOv8-nano, YOLOv8-pico, and
        YOLOv8-femto models with different input resolutions. The results generally
        support our hypothesis that the reduction in model's input resolution is directly
        proportional to the increase in inference speed, although with some
        deviations. These results were obtained by running tests on NVIDIA Jetson
        Nano using the TensorRT backend. The tested models were deployed with
        dynamic shape in the batch size dimension to run the tests with a batch size
        of 16 to most accurately measure the speed of the smallest models.}
        \label{InputResolutionVSFPS}
    \end{framed}
\end{figure}



\section{Inference Speeds: ONNX Runtime vs. TensorRT}

In this experiment, we compare how the inference speed is increased by using the
TensorRT backend for inference on NVIDIA Jetson devices instead of the ONNX
Runtime backend. The inference speeds (FPS) measured by our test script are
compared on both the lowest and the highest-performing NVIDIA Jetson devices
with different batch sizes to be able to draw general conclusions from the
results. For each model architecture, only the biggest input resolution is
selected for testing, following the insights gained by the experiment in
\autoref{InputResolutionVSFPSExperiment}. Additionally, all tested models were
dynamic in shape.

The results in \autoref{OrtVsTrtFPS} clearly show the superiority of the
TensorRT inference backend on NVIDIA Jetson devices for all tested models and
different batch sizes. Naturally, the choice of the inference backend has no
effect on the mean Average Precision of the tested models. Therefore, none of
the following experiments will feature the ONNX Runtime backend on NVIDIA Jetson
devices.

% Please note that the on these devices, inference speeds of tiny models, such as
% the YOLOv8-pico and YOLOv8-femto, might not be accurate due to them being so
% small.

\begin{figure}[t]
    \begin{framed}
        \centering
        \includegraphics[width=\textwidth]{onnx_vs_tensorrt_comparison.pdf}
        \caption{Comparison of inference speeds of all trained YOLOv8 model
        architectures between the TensorRT and the ONNX Runtime inference backends
        (of course, both utilizing the devices' GPU). Tests were conducted on NVIDIA
        Jetson Nano with a batch size of 1 and on NVIDIA Jetson AGX Xavier with a
        batch size of 32. Please note that for each model architecture, only the
        model with the largest input resolution was tested and all tested models were
        deployed as dynamic in the batch size dimension.}
        \label{OrtVsTrtFPS}
    \end{framed}
\end{figure}




\section{Effect of Model's Shape on Inference Speed}

While a model with a static shape has a fixed input resolution and a
fixed batch size, it can be set to accept input images of different resolutions
and different batch sizes. In this paper, we used the MMDeploy library to deploy
the trained models into both static and dynamic shapes. However, the deployment
process was configured so that dynamic models would only be flexible in the
batch size dimension while the model's input resolution stays static. Anyways, we
will call these models dynamic.

In this experiment, we compare inference speeds achieved by static models and
dynamic models to learn whether deploying a model to a dynamic shape (in the
batch size dimension) decreases the inference speed. Only the most
representative models were selected for comparison to keep the plots concise.
Naturally, all tests were conducted with a batch size of a single image. Tests
were conducted on the NVIDIA Jetson Nano with the TensorRT inference backend and
on the Raspberry PI 4B with ONNX Runtime to make the measurements as accurate as
possible by using the least-performing devices for both inference backends. The
comparison of inference speeds on all model architectures is presented in
\autoref{StaticDynamicModelShape}.

Because the inference speed is not consistently faster for static models, we
conclude that it does not significantly decrease if a dynamic shape is used for
the model's input in the batch size dimension. Therefore, we will only evaluate
dynamic models in future experiments.

\begin{figure}[h]
    \begin{framed}
        \centering
        \includegraphics[width=\textwidth]{static_dynamic_comparison.pdf}
        % \caption{na konci pripomenúť že dynamic je iba batch size dimension}
        \caption{Comparison of inference speeds between models with static and
        dynamic shapes in the batch size dimension. Only the most representative
        models were selected for comparison, as the rest of the data do not
        provide additional insights.}
        \label{StaticDynamicModelShape}
    \end{framed}
\end{figure}







\section{Effect of Batch Size on Inference Speed}
\label{BatchSizeComparisonSection}

An object detector can achieve higher inference speeds (FPS) if a larger batch
size is used\,--\,if the detector is given more than one frame in a single
input. Although rarely suitable for real-time object detection, a larger batch
size enables more efficient utilization of GPU resources at the cost of higher
memory usage. In this experiment, we will discuss how the inference speed
increases with larger batch sizes used when performing inference with different
models and on different devices.

In \autoref{BatchSizeComparison} (please note the logarithmic horizontal axis),
results of tests on Raspberry PI 4B, NVIDIA Jetson Nano and NVIDIA Jetson AGX
Xavier are shown. Although tests were run with six different batch sizes, to
make the figure more compact, only batch sizes 1, 2, 8 and 32 were selected for
comparison.

The results show that using a larger batch size for inference does indeed often
increase the inference speed, but the increase is generally only significant
when performing inference on smaller models, where the inference speed is
higher. This is well illustrated by the NVIDIA Jetson AGX Xavier, where the
increase in inference speed with larger batch sizes is relatively subtle for the
largest detector, YOLOv8-medium $640 \times 384$. In contrast, for the smallest
model, YOLOv8-femto $352 \times 192$, raising the batch size from 1 to 32
results in an almost ten-fold increase in inference speed. Therefore, using
larger batch sizes is generally only suitable in cases where high inference
speeds (higher than real-time) are desired, for example in a multi-camera
real-time vehicle detection system.

% Napísať že je v poriadku že sme skončili pri 32 lebo z grafu vidno že už ten
% rozdiel medzi 16 a 32 nie je taký veľký ako medzi 8 a 16? A teda batch 64 by
% pravdepodobne ani na AGX s najmenším femtom už príliš nepomohol

\begin{figure}[t]
    \begin{framed}
        \centering
        \includegraphics[width=\textwidth]{batch_size_comparison.pdf}
        \caption{Inference speeds achieved by using different batch sizes. Tests
        were conducted on Raspberry PI 4B, NVIDIA Jetson Nano and NVIDIA Jetson
        AGX Xavier and only the most representative models were selected for
        comparison to keep the figure compact. Please note that the horizontal
        axes are logarithmic to better present the differences in inference
        speeds. Additionally, inference speed of YOLOv8-medium $640 \times 384$
        with the batch size of 32 on NVIDIA Jetson Nano is missing because the
        model did not fit into the memory.}
        \label{BatchSizeComparison}
    \end{framed}
\end{figure}





% \section{Mean Average Precision and Inference Speed of All Models}
% \section{Benchmark of Mean Average Precision and Inference Speed}
\section{Mean Average Precision and Inference Speed Benchmark}
\label{FPSvsmAPComparison}

In this experiment, we compare the trained and models in terms of their mAP
metric (mean Average Precision) and their inference speed in FPS.

For the tests, we wanted to choose two least-performing devices, with the
Raspberry PI 4B being an obvious choice. Additionally, the NVIDIA Jetson Nano
would be a great choice, but since the device is not optimized for \texttt{INT8}
precision and the models were therefore not quantized on this device, NVIDIA
Jetson Xavier NX was selected instead. For the tests to be as representative as
possible even for the smallest models, tests on Jetson Xavier NX were run with a
large batch size of 32, while on the Raspberry PI, where the batch size doesn't
really matter, a batch size of 1 was used instead. Naturally, ONNX Runtime
backend was used on the Raspberry PI and TensorRT on the Jetson Xavier NX.

We therefore present two plots, comparing the mAP and FPS of all models. On the
Raspberry PI, ONNX Runtime backend was used to test the models on the CPU. For
the NVIDIA Jetson Xavier NX with TensorRT backend, the plot also features
results of models quantized to both \texttt{FP16} and \texttt{INT8} precisions
and individual models with different precisions were connected in the plot. The
mAP and FPS comparison for the Raspberry PI can be found in
\autoref{FPSvsmAPComparisonRPI}, while the corresponding results for the NVIDIA
Jetson Xavier NX are shown in \autoref{FPSvsmAPComparisonNX}. Please note that
in both plots, a logarithmic scale was chosen for the X-axis (FPS) to include
all models (large and tiny) in the figure. For a complete benchmark of all
trained and quantized models on all devices, see \autoref{mAPvsFPS}.

An important thing to notice is the effect of quantization on the mAP and FPS.
Although quantization to \texttt{INT8} precision seems to rarely result in
better performance than training a smaller model, it is clear that quantizing to
\texttt{FP32} results in a significantly higher inference speed with a little or
no decrease in the mAP.

The performance of the YOLOv8 MobileNetV2 model can also be discussed here, as
tests on the Raspberry PI show the superiority of the YOLOv8 model with the
MobileNetV2 backbone compared to the YOLOv8-nano models. However, tests
conducted on the NVIDIA Jetson Xavier NX with the TensorRT backend show that the
opposite is true and the MobileNetV2 brings little to no advantage when compared
to YOLOv8-small or YOLOv8-nano models on this device. To draw further
conclusions about this matter, the reader is encouraged to look at the tables in
\autoref{mAPvsFPS}, which present the complete mAP and FPS values. Additionally,
all 6 COCO mAP metrics of different models are compared in the next experiment.

In both plots resulting from this experiment, a strange phenomenon can be seen
in which the mean Average Precision of a model with a higher input
resolution\,--\,YOLOv8-nano $512 \times 288$ is lower than the one of a model
with a lower input resolution of $448 \times 256$. The same seems to apply for
YOLOv8-pico models, where the mAP with input resolution $448 \times 256$ is
lower (although not as significantly) than with $384 \times 224$, instead of
being higher. However, this problem doesn't seem to affect the YOLOv8-femto
models, of which the mAP and FPS values are as expected. We tried examining the
problem, mainly by double-checking the results and training configurations of
these models, but could not find the root cause and did not have enough
resources to conduct further experiments to provide more insights into this
phenomenon.
% We suppose the reason behind it is the use of bilinear interpolation when resizing?

\begin{figure}[H]
    \begin{framed}
        \centering
        \includegraphics[width=\textwidth]{fps_vs_map_comparison_log_rpi.pdf}
        \caption{Performance comparison of all YOLOv8 models in terms of mAP
        (mean Average Precision) and inference speed (FPS) on a Raspberry PI 4B
        with ONNX Runtime inference backend and the batch size of 1. Please note
        that the X-axis is logarithmic to be able to display all models in a
        single plot for easier analysis. We also remind the reader that only a
        single CPU thread was used when testing models on this device.}
        \label{FPSvsmAPComparisonRPI}
    \end{framed}
\end{figure}

\begin{figure}[H]
    \begin{framed}
        \centering
        \includegraphics[width=\textwidth]{fps_vs_map_comparison_log_nx.pdf}
        \caption{Performance comparison of all YOLOv8 models, including
        quantized ones, in terms of mAP (mean Average Precision) and inference
        speed (FPS) on NVIDIA Jetson Xavier NX with TensorRT inference backend
        and the batch size of 32. Please note that the X-axis is logarithmic to
        be able to display all models in a single plot for easier analysis.}
        \label{FPSvsmAPComparisonNX}
    \end{framed}
\end{figure}








\section{Evaluating Models Across Multiple Mean Average Precision Metrics}

In other experiments, we compare the performance of models just by
using a single metric\,--\,the mean Average Precision (mAP) for objects of all
sizes and over ten IoU thresholds ranging from \num{0.50} to \num{0.95}.
However, models can also be compared using five additional, more specific mAP
metrics as explained in \autoref{EvaluationMetrics}\,--\,mAP with the IoU
threshold equal to \num{0.50}, with IoU threshold \num{0.75}, and mAP for small,
medium-sized or large objects.

In \autoref{mAPTableSmall}, we display these metrics for all trained models.
However, we omit the quantized models from the table because the mAP values tend
to stay the same after reducing precision to \texttt{FP16} and quantizing to
\texttt{INT8} is rarely beneficial, as illustrated by the experiment in
\autoref{FPSvsmAPComparison}. For the complete table featuring all precisions of
all models, see \autoref{mAPTableBig}.

In the results, we can observe that smaller models detect large and even
medium-sized objects reasonably well. However, when it comes to detecting small
objects (of area lower than $32 \times 32$ px), the performance of these models
drops significantly, even at higher input resolutions. However, the smaller
models might still be highly suitable for cases in which detecting small objects
is not a priority, as they generally perform well for larger objects while being
dramatically faster.

\begin{table}
    \centering
    \small
    \begin{tabular}{|c|c|rrrrrr|}
        \hline
        Model & Input & \multicolumn{6}{c|}{mAP} \\
        \cline{3-8}
        (YOLOv8)                     & resolution                       & mAP   & IoU:50& IoU:75& small & medium& large \\
        \hline
        \hline
        \multirow{1}{*}{medium}      & \multirow{1}{*}{$640\times384$}  & 0.603 & 0.805 & 0.695 & 0.316 & 0.606 & 0.770 \\
        \hline                                                          
        \multirow{1}{*}{small}       & \multirow{1}{*}{$640\times384$}  & 0.578 & 0.786 & 0.660 & 0.273 & 0.585 & 0.744 \\
        \hline
        \multirow{1}{*}{MobileNetV2} & \multirow{1}{*}{$512\times288$}  & 0.548 & 0.759 & 0.618 & 0.235 & 0.550 & 0.715 \\
        \hline                      
        \multirow{3}{*}{nano}        & \multirow{1}{*}{$640\times384$}  & 0.530 & 0.749 & 0.608 & 0.229 & 0.529 & 0.685 \\
        \cline{2-2}
                                     & \multirow{1}{*}{$512\times288$}  & 0.492 & 0.699 & 0.569 & 0.210 & 0.483 & 0.673 \\
        \cline{2-2}
                                     & \multirow{1}{*}{$448\times256$}  & 0.511 & 0.723 & 0.592 & 0.206 & 0.508 & 0.681 \\
        \hline
        \multirow{3}{*}{pico}        & \multirow{1}{*}{$512\times288$}  & 0.454 & 0.664 & 0.502 & 0.151 & 0.448 & 0.614 \\
        \cline{2-2}
                                     & \multirow{1}{*}{$448\times256$}  & 0.415 & 0.621 & 0.468 & 0.136 & 0.408 & 0.552 \\
        \cline{2-2}
                                     & \multirow{1}{*}{$384\times224$}  & 0.417 & 0.615 & 0.465 & 0.128 & 0.405 & 0.589 \\
        \hline
        \multirow{4}{*}{femto}       & \multirow{1}{*}{$512\times288$}  & 0.318 & 0.488 & 0.351 & 0.072 & 0.298 & 0.416 \\
        \cline{2-2}
                                     & \multirow{1}{*}{$448\times256$}  & 0.300 & 0.477 & 0.318 & 0.070 & 0.259 & 0.429 \\
        \cline{2-2}
                                     & \multirow{1}{*}{$384\times224$}  & 0.281 & 0.452 & 0.302 & 0.070 & 0.263 & 0.386 \\
        \cline{2-2}
                                     & \multirow{1}{*}{$352\times192$}  & 0.259 & 0.430 & 0.275 & 0.054 & 0.227 & 0.370 \\
        \hline
    \end{tabular}
    % \caption{Displaying multiple mean Average Precision metrics for each trained
    % model and input resolution. The base mAP is measured over ten IoU
    % (Intersection over Union) thresholds ranging from \num{0.50} to \num{0.95}
    % with step \num{0.05}.  The IoU:50 and IoU:75 metrics are calculated
    % similarly, but only over one IoU threshold (\num{0.50} and \num{0.75}
    % respectively). Additionally, mAP for small, medium-sized and large objects
    % is simply calculated for objects of the respective sizes, where a small
    % object is a one with area lower than $32 \times 32$, a medium-sized object
    % is with area higher than $32 \times 32$ but lower than $96 \times 96$, while
    % a large object is a one with an area higher than $96 \times 96$.}
    \caption{Comparison of multiple mean Average Precision metrics for each
    trained model and input resolution. The mAP metrics displayed in this table
    are explained in \autoref{EvaluationMetrics}.}
    \label{mAPTableSmall}
\end{table}






\section{Inference Speeds on Different Devices}


% IF: Two separate figures
% In this experiment, we benchmark all available devices by measuring the
% inference speeds of all model architectures. For each architecture, only one
% input resolution was selected, however, differently than in the other
% experiments: YOLOv8-nano $512 \times 288$ was chosen to better compare with
% YOLOv8 MobileNetV2 and YOLOv8-femto $352 \times 192$ to include the smallest
% model. This plot can be seen in \autoref{DevicesBenchmark}. A second set of
% measurements was created to benchmark models quantized to \texttt{FP16}
% precision solely on the NVIDIA Jetson devices and is displayed in a separate
% plot in \autoref{DevicesBenchmarkFP16}. Please note that the X-axis is in a
% logarithmic scale in both plots because of the performance differences between
% individual devices.

% ELSE: Both plots in one figure:
In this experiment, we benchmark all available devices by measuring the
inference speeds of all model architectures. For each architecture, only one
input resolution was selected, however, differently than in the other
experiments: YOLOv8-nano $512 \times 288$ was chosen to better compare with
YOLOv8 MobileNetV2 and YOLOv8-femto $352 \times 192$ to include the smallest
model. A second set of measurements was created to only benchmark models
quantized to \texttt{FP16} precision on the NVIDIA Jetson devices. Both plots
can be seen in \autoref{DevicesBenchmark}. Please note that the X-axis is in a
logarithmic scale in both plots because of the performance differences between
individual devices.

We decided to measure the inference speed with higher batch sizes to get the
highest possible inference speeds possible, but since larger models do not fit
into memory on lower-performing devices when a large batch size was
used\footnote{List of tests that failed because the model could not fit into
the memory of a device when performing inference with a specific batch size:
YOLOv8-medium on NVIDIA Jetson Nano with batch size 32, YOLOv8-medium on NVIDIA
MX150 with batch sizes 16 and 32, YOLOv8-small on NVIDIA MX150 with batch size
32.}, we could not benchmark these models with the largest batch size of 32.
However, thanks to insights provided by \autoref{BatchSizeComparisonSection}, we
consider it safe to use a smaller batch size of 8 for YOLOv8-medium,
YOLOv8-small and YOLOv8 MobileNetV2 on all devices. For the rest of the models,
a batch size of 32 was used. This applies to both plots resulting from this
experiment.

What's interesting to see from the first plot is that the NVIDIA Jetson Nano is
comparable to the NVIDIA MX150, while consuming just a fraction of the power.
Additionally, there is a significant difference between the performance of the
Jetson Nano compared to Jetson Xavier NX, while the inference speed measured on
Jetson AGX Xavier is only slightly higher than on Jetson Xavier NX.

Another notable difference lies in how weight quantization to \texttt{FP16}
increases the inference speeds on NVIDIA Jetson devices. The highest difference
in FPS between precisions \texttt{FP32} and \texttt{FP16} on Jetson nano was for
the YOLOv8-small model, where the inference speed after quantization increased
by \num{50.13} \% from \num{11.19} FPS to \num{16.79} FPS. On the other hand,
the inference speed for the same model on Jetson Xavier NX increased by
\num{165.84} \% from \num{44.12} FPS to \num{117.29} FPS. Furthermore, the
difference is even higher for YOLOv8-medium, where the quantization to
\texttt{FP16} resulted in a \num{228.33} \% increase in inference speed.
However, it's important to note that for smaller models, these gains are lower
and more comparable between the two devices.

For a complete table of inference speeds of all models (including all quantized
models) on all devices with a batch size of 1, please see \autoref{mAPvsFPS1} in
\autoref{mAPvsFPS}.

% IF: Two separate figures
% \begin{figure}[H]
%     \begin{framed}
%         \centering
%         \includegraphics[width=\textwidth]{devices_benchmark.pdf}
%         \caption{Benchmark of inference speeds (FPS) on different devices
%         featuring all model architectures. Please note the logarithmic X-axis
%         and that for comparison, YOLOv8-nano model with $512 \times 288$ input
%         resolution and YOLOv8-femto model with $352 \times 192$ input resolution
%         were selected.  A batch size of 8 was selected for tests featuring
%         larger models\,--\,YOLOv8-medium, YOLOv8-small and YOLOv8 MobileNetV2,
%         and for the rest of the models, a batch size of 32 was used. The
%         TensorRT inference backend was utilized when testing on NVIDIA Jetson
%         devices, while the ONNX Runtime backend was used on the rest of the
%         devices.}
%         \label{DevicesBenchmark}
%     \end{framed}
% \end{figure}
% \begin{figure}[H]
%     \begin{framed}
%         \centering
%         \includegraphics[width=\textwidth]{devices_benchmark_FP16.pdf}
%         \caption{Benchmark of inference speeds (FPS) on NVIDIA Jetson devices
%         featuring all model architectures. Models were quantized to
%         \texttt{FP16} precision. Please note the logarithmic X-axis and that for
%         comparison, YOLOv8-nano model with $512 \times 288$ input resolution and
%         YOLOv8-femto model with $352 \times 192$ input resolution were selected.
%         A batch size of 8 was selected for tests featuring larger
%         models\,--\,YOLOv8-medium, YOLOv8-small and YOLOv8 MobileNetV2, and for
%         the rest of the models, a batch size of 32 was used. For these tests,
%         the TensorRT inference backend was utilized.}
%         \label{DevicesBenchmarkFP16}
%     \end{framed}
% \end{figure}

% ELSE: Both plots in one figure
\begin{figure}[H]
    \begin{framed}
        \centering
        \includegraphics[width=\textwidth]{devices_benchmark.pdf}
        \includegraphics[width=\textwidth]{devices_benchmark_FP16.pdf}
        \caption{Benchmark of inference speeds (FPS) on different devices
        featuring all model architectures. The second plot compares models
        quantized to \texttt{FP16} precision on NVIDIA Jetson devices. Please
        note that in both subplots, the X-axis is logarithmic and that for
        comparison, YOLOv8-nano model with $512 \times 288$ input resolution and
        YOLOv8-femto model with $352 \times 192$ input resolution were selected.
        A batch size of 8 was selected for tests featuring larger
        models\,--\,YOLOv8-medium, YOLOv8-small and YOLOv8 MobileNetV2, and for
        the rest of the models, a batch size of 32 was used. The TensorRT
        inference backend was utilized when testing on NVIDIA Jetson devices,
        while the ONNX Runtime backend was used on the rest of the devices.
        Additionally, we remind the reader that for the tests on CPUs, only a
        single thread was used, utilizing just one of the CPU's cores.}
        \label{DevicesBenchmark}
    \end{framed}
\end{figure}


% \section{Porovnať CSPDarknet vs MobileNetV2?}






\section{TODO Confusion matrix?}

Asi netreba keď mám PR krivky. Možno v prílohách ak bude čas




\section{TODO DETRAC vs MIO-TCD?}

NEDÔLEŽITÉ!

porovnať mAP niektorých, možno všetkých detektorov, medzi dvoma test subsetmi. Nedôležité!

Že detrac je ľahký do istej miery ale pri rohoch je anotovaný inak ako je
detektor schopný sa naučiť; mio-tcd má veľa objektov náročných

ako graf, vľavo modely a resolution, x os mAP a pre každý model tri bary - all, detrac a mio-tcd







\chapter{Future Work}

TODO TODO TODO

knowledge distillation, pruning,... kvantizácia onnx

Porovnať onnxruntime a tensorRT na bežnej GPU

NCNN alebo ARM NN pre RPI

SSD porovnať tiež

TFLite



% Tu nemá byť nič nové - iba zhrnutie
\chapter{Conclusion}

TODO TODO TODO











\begin{appendices}


\chapter{Additional Precision-Recall Curves}
\label{PRCurvesAdditional}
\begin{figure}[H]
    \begin{framed}
        \centering
        \includegraphics[width=\textwidth]{pr_curve_additional.pdf}
        \caption{Precision-Recall curves for the rest of the models (which were
        not included in \autoref{PRCurveMajor}).}
    \end{framed}
\end{figure}






\chapter{Complete Benchmark of All Models on All Devices}
\label{mAPvsFPS}

\todo{pripomenúť že CPU len na jednom threade}

% Batch 1
\begin{table}[h]
    \footnotesize
    \centering
    \begin{tabular}{|c|c|c|c|rrrrrr|}
        \hline
        \multirow{4}{*}{\parbox{1.6cm}{\centering Model\\(YOLOv8)}} &
          \multirow{4}{*}{\parbox{1.5cm}{\centering Input\\Resolution}} &
            \multirow{4}{*}{\parbox{0.9cm}{\centering Preci-\\sion}} &
              \multirow{4}{*}{mAP} &
                \multicolumn{6}{c|}{Inference Speed with a batch size of 1 (FPS)} \\
        \cline{5-10}
        & & & & Rasp- & Intel   &\multirow{3}{*}{\parbox{1.2cm}{\raggedleft NVIDIA\\MX150}} & \multicolumn{3}{|c|}{NVIDIA Jetson} \\
        \cline{8-10}
        & & & & berry & Core i7 &                                                           & \multicolumn{1}{|c}{\multirow{2}{*}{Nano}} & Xavier & AGX \\
        & & & & PI    & 9850H   &                                                           & \multicolumn{1}{|c}{}                      & NX     & Xavier \\
        \hline
        \hline
        \multirow{3}{*}{medium}                                   & \multirow{3}{*}{$640 \times 384$} & FP32 & 0.603 &  0.12 &   1.62 &  4.04 &  4.15 & 16.36 &  17.48 \\
                                                                  &                                   & FP16 & 0.603 &     - &      - &     - &  6.82 & 47.49 &  49.29 \\
                                                                  &                                   & INT8 & 0.471 &     - &      - &     - &     - & 48.62 &  53.37 \\
        \hline
        \multirow{3}{*}{small}                                    & \multirow{3}{*}{$640 \times 384$} & FP32 & 0.578 &  0.34 &   5.12 &  9.54 & 10.91 & 39.48 &  46.18 \\
                                                                  &                                   & FP16 & 0.578 &     - &      - &     - & 16.27 & 52.01 &  57.44 \\
                                                                  &                                   & INT8 & 0.544 &     - &      - &     - &     - & 70.68 &  64.25 \\
        \hline
        \multirow{3}{*}{\parbox{1.6cm}{\centering MobileNet\\V2}} & \multirow{3}{*}{$512 \times 288$} & FP32 & 0.548 &  1.19 &  15.82 & 15.34 & 18.55 & 51.59 &  66.72 \\
                                                                  &                                   & FP16 & 0.548 &     - &      - &     - & 19.93 & 53.30 &  58.66 \\
                                                                  &                                   & INT8 & 0.459 &     - &      - &     - &     - & 69.95 &  55.24 \\
        \hline
        \multirow{9}{*}{nano}                                     & \multirow{3}{*}{$640 \times 384$} & FP32 & 0.530 &  1.02 &  14.78 & 20.40 & 25.08 & 54.30 &  57.50 \\
                                                                  &                                   & FP16 & 0.530 &     - &      - &     - & 31.90 & 63.13 &  55.33 \\
                                                                  &                                   & INT8 & 0.430 &     - &      - &     - &     - & 59.78 &  64.21 \\
        \cline{2-10}
                                                                  & \multirow{3}{*}{$512 \times 288$} & FP32 & 0.492 &  1.70 &  23.67 & 30.58 & 37.56 & 57.25 &  63.53 \\
                                                                  &                                   & FP16 & 0.492 &     - &      - &     - & 48.29 & 70.01 &  71.38 \\
                                                                  &                                   & INT8 & 0.356 &     - &      - &     - &     - & 81.91 &  83.87 \\
        \cline{2-10}
                                                                  & \multirow{3}{*}{$448 \times 256$} & FP32 & 0.511 &  2.18 &  29.55 & 36.21 & 49.25 & 67.73 &  58.17 \\
                                                                  &                                   & FP16 & 0.511 &     - &      - &     - & 54.04 & 77.80 &  83.35 \\
                                                                  &                                   & INT8 & 0.372 &     - &      - &     - &     - & 61.81 &  91.60 \\
        \hline
        \multirow{9}{*}{pico}                                     & \multirow{3}{*}{$512 \times 288$} & FP32 & 0.454 &  4.23 &  50.48 & 43.96 & 54.02 & 69.78 &  71.11 \\
                                                                  &                                   & FP16 & 0.454 &     - &      - &     - & 57.14 & 59.80 &  87.29 \\
                                                                  &                                   & INT8 & 0.297 &     - &      - &     - &     - & 64.16 &  94.32 \\
        \cline{2-10}
                                                                  & \multirow{3}{*}{$448 \times 256$} & FP32 & 0.415 &  5.39 &  61.38 & 52.89 & 57.37 & 78.47 &  81.29 \\
                                                                  &                                   & FP16 & 0.415 &     - &      - &     - & 59.12 & 66.16 &  98.80 \\
                                                                  &                                   & INT8 & 0.356 &     - &      - &     - &     - & 71.28 & 101.98 \\
        \cline{2-10}
                                                                  & \multirow{3}{*}{$384 \times 224$} & FP32 & 0.417 &  7.07 &  77.38 & 59.04 & 60.39 & 62.16 &  88.68 \\
                                                                  &                                   & FP16 & 0.418 &     - &      - &     - & 61.85 & 77.94 & 108.55 \\
                                                                  &                                   & INT8 & 0.325 &     - &      - &     - &     - & 80.42 & 113.24 \\
        \hline
        \multirow{12}{*}{femto}                                   & \multirow{3}{*}{$512 \times 288$} & FP32 & 0.318 &  7.11 &  62.40 & 55.16 & 58.78 & 55.81 &  84.13 \\
                                                                  &                                   & FP16 & 0.318 &     - &      - &     - & 60.28 & 91.62 &  93.83 \\
                                                                  &                                   & INT8 & 0.300 &     - &      - &     - &     - & 65.07 &  97.91 \\
        \cline{2-10}
                                                                  & \multirow{3}{*}{$448 \times 256$} & FP32 & 0.300 &  9.03 &  81.19 & 63.29 & 61.44 & 62.33 &  95.11 \\
                                                                  &                                   & FP16 & 0.300 &     - &      - &     - & 58.12 & 70.06 & 104.58 \\
                                                                  &                                   & INT8 & 0.264 &     - &      - &     - &     - & 72.38 & 107.26 \\
        \cline{2-10}
                                                                  & \multirow{3}{*}{$384 \times 224$} & FP32 & 0.281 & 11.87 & 103.49 & 76.33 & 66.01 & 72.29 & 103.47 \\
                                                                  &                                   & FP16 & 0.281 &     - &      - &     - & 57.66 & 82.98 & 113.06 \\
                                                                  &                                   & INT8 & 0.235 &     - &      - &     - &     - & 83.54 & 115.56 \\
        \cline{2-10}
                                                                  & \multirow{3}{*}{$352 \times 192$} & FP32 & 0.259 & 14.82 & 121.83 & 84.24 & 64.84 & 78.18 & 109.02 \\
                                                                  &                                   & FP16 & 0.259 &     - &      - &     - & 65.91 & 89.41 & 120.63 \\
                                                                  &                                   & INT8 & 0.214 &     - &      - &     - &     - & 88.15 & 123.08 \\
        \hline
    \end{tabular}
    \caption{Benchmark of all trained and quantized YOLOv8 models on all
    available devices. For these tests, a batch size of 1 was used and all
    models were of a dynamic shape in the batch size dimension. Additionally,
    the TensorRT inference backend was used on NVIDIA Jetson devices, while the
    ONNX Runtime backend was used for tests on the rest of the devices. Models
    were only quantized to \texttt{FP16} precision on NVIDIA Jetson devices and
    \texttt{INT8} precision was only used on Jetson Xavier NX and Jetson AGX
    Xavier.}
    \label{mAPvsFPS1}
    \normalsize
\end{table}

% Batch 32
\begin{table}
    \footnotesize
    \centering
    \begin{tabular}{|c|c|c|c|rrrrrr|}
        \hline
        \multirow{4}{*}{\parbox{1.6cm}{\centering Model\\(YOLOv8)}} &
          \multirow{4}{*}{\parbox{1.5cm}{\centering Input\\Resolution}} &
            \multirow{4}{*}{\parbox{0.9cm}{\centering Preci-\\sion}} &
              \multirow{4}{*}{mAP} &
                \multicolumn{6}{c|}{Inference Speed with a batch size of 32* (FPS)} \\
        \cline{5-10}
        & & & & Rasp- & Intel   &\multirow{3}{*}{\parbox{1.2cm}{\raggedleft NVIDIA\\MX150}} & \multicolumn{3}{|c|}{NVIDIA Jetson} \\
        \cline{8-10}
        & & & & berry & Core i7 &                                                           & \multicolumn{1}{|c}{\multirow{2}{*}{Nano}} & Xavier & AGX \\
        & & & & PI    & 9850H   &                                                           & \multicolumn{1}{|c}{}                      & NX     & Xavier \\
        \hline
        \hline
        \multirow{3}{*}{medium}                                   & \multirow{3}{*}{$640 \times 384$} & FP32 & 0.603 & 0.12  &   1.57 &  4.68* &  4.15* &   16.97 &   19.11 \\
                                                                  &                                   & FP16 & 0.603 &    -  &      - &      - &   6.96 &   58.20 &   63.91 \\
                                                                  &                                   & INT8 & 0.471 &    -  &      - &      - &      - &   86.30 &  102.17 \\
        \hline
        \multirow{3}{*}{small}                                    & \multirow{3}{*}{$640 \times 384$} & FP32 & 0.578 & 0.34  &   4.98 & 10.70* &  11.23 &   44.73 &   54.78 \\
                                                                  &                                   & FP16 & 0.578 &    -  &      - &      - &  16.79 &  121.36 &  140.62 \\
                                                                  &                                   & INT8 & 0.544 &    -  &      - &      - &      - &  191.18 &  206.82 \\
        \hline
        \multirow{3}{*}{\parbox{1.6cm}{\centering MobileNet\\V2}} & \multirow{3}{*}{$512 \times 288$} & FP32 & 0.548 &  1.19 &  15.46 &  17.17 &  20.36 &   89.98 &  108.48 \\
                                                                  &                                   & FP16 & 0.548 &     - &      - &      - &  22.00 &  159.10 &  185.10 \\
                                                                  &                                   & INT8 & 0.459 &     - &      - &      - &      - &  261.10 &  290.35 \\
        \hline
        \multirow{9}{*}{nano}                                     & \multirow{3}{*}{$640 \times 384$} & FP32 & 0.530 &  1.03 &  14.38 &  21.22 &  26.30 &  115.28 &  139.49 \\
                                                                  &                                   & FP16 & 0.530 &     - &      - &      - &  33.69 &  205.36 &  251.42 \\
                                                                  &                                   & INT8 & 0.430 &     - &      - &      - &      - &  264.79 &  305.43 \\
        \cline{2-10}
                                                                  & \multirow{3}{*}{$512 \times 288$} & FP32 & 0.492 &  1.70 &  23.71 &  40.13 &  41.86 &  192.31 &  219.17 \\
                                                                  &                                   & FP16 & 0.492 &     - &      - &      - &  54.16 &  328.51 &  390.02 \\
                                                                  &                                   & INT8 & 0.356 &     - &      - &      - &      - &  416.75 &  466.65 \\
        \cline{2-10}
                                                                  & \multirow{3}{*}{$448 \times 256$} & FP32 & 0.511 &  2.21 &  30.16 &  51.74 &  55.73 &  246.03 &  283.35 \\
                                                                  &                                   & FP16 & 0.511 &     - &      - &      - &  71.49 &  409.57 &  483.80 \\
                                                                  &                                   & INT8 & 0.372 &     - &      - &      - &      - &  524.52 &  575.99 \\
        \hline
        \multirow{9}{*}{pico}                                     & \multirow{3}{*}{$512 \times 288$} & FP32 & 0.454 &  4.30 &  53.92 &  67.29 &  70.51 &  338.69 &  378.36 \\
                                                                  &                                   & FP16 & 0.454 &     - &      - &      - &  83.02 &  457.63 &  523.29 \\
                                                                  &                                   & INT8 & 0.297 &     - &      - &      - &      - &  511.57 &  596.65 \\
        \cline{2-10}
                                                                  & \multirow{3}{*}{$448 \times 256$} & FP32 & 0.415 &  5.48 &  68.66 &  83.70 &  92.82 &  432.56 &  484.74 \\
                                                                  &                                   & FP16 & 0.415 &     - &      - &      - & 107.79 &  563.96 &  641.45 \\
                                                                  &                                   & INT8 & 0.356 &     - &      - &      - &      - &  631.91 &  740.18 \\
        \cline{2-10}
                                                                  & \multirow{3}{*}{$384 \times 224$} & FP32 & 0.417 &  7.18 &  89.89 & 105.58 & 118.47 &  564.88 &  648.23 \\
                                                                  &                                   & FP16 & 0.418 &     - &      - &      - & 138.83 &  763.25 &  888.08 \\
                                                                  &                                   & INT8 & 0.325 &     - &      - &      - &      - &  878.58 & 1016.77 \\
        \hline
        \multirow{12}{*}{femto}                                   & \multirow{3}{*}{$512 \times 288$} & FP32 & 0.318 &  7.10 &  70.84 &  85.43 &  87.42 &  440.21 &  474.64 \\
                                                                  &                                   & FP16 & 0.318 &     - &      - &      - &  98.72 &  527.16 &  579.90 \\
                                                                  &                                   & INT8 & 0.300 &     - &      - &      - &      - &  582.56 &  589.16 \\
        \cline{2-10}
                                                                  & \multirow{3}{*}{$448 \times 256$} & FP32 & 0.300 &  9.04 &  93.97 & 106.62 & 113.09 &  556.71 &  600.81 \\
                                                                  &                                   & FP16 & 0.300 &    -  &      - &      - & 127.27 &  656.74 &  682.84 \\
                                                                  &                                   & INT8 & 0.264 &    -  &      - &      - &      - &  679.27 &  726.82 \\
        \cline{2-10}
                                                                  & \multirow{3}{*}{$384 \times 224$} & FP32 & 0.281 & 11.92 & 125.96 & 133.62 & 145.79 &  732.24 &  813.68 \\
                                                                  &                                   & FP16 & 0.281 &    -  &      - &      - & 158.38 &  843.47 & 1004.03 \\
                                                                  &                                   & INT8 & 0.235 &    -  &      - &      - &      - & 1009.65 & 1034.69 \\
        \cline{2-10}
                                                                  & \multirow{3}{*}{$352 \times 192$} & FP32 & 0.259 & 14.80 & 145.94 & 158.72 & 175.98 &  878.53 &  968.72 \\
                                                                  &                                   & FP16 & 0.259 &    -  &      - &      - & 197.29 & 1040.57 & 1152.97 \\
                                                                  &                                   & INT8 & 0.214 &    -  &      - &      - &      - & 1117.36 & 1237.66 \\
        \hline
        \end{tabular}
    % \caption{s hviezdičami sú s batch 8 lebo sa nevošli do memory}
    \caption{Benchmark of all trained and quantized YOLOv8 models on all
    available devices. For these tests, a batch size of 32 was used. However,
    for special cases marked by \texttt{*}, where the model did not fit into the
    device's memory with the large batch size, a batch size of 8 was used
    instead. Models tested were of a dynamic shape in the batch size dimension.
    Additionally, the TensorRT inference backend was used on NVIDIA Jetson
    devices, while the ONNX Runtime backend was used for tests on the rest of
    the devices. Models were only quantized to \texttt{FP16} precision on NVIDIA
    Jetson devices and \texttt{INT8} precision was only used on Jetson Xavier NX
    and Jetson AGX Xavier.}
    \label{mAPvsFPS32}
    \normalsize
\end{table}




\chapter{Complete Mean Average Precision Metrics for YOLOv8 Models}
\label{mAPTableBig}
\begin{table}[t]
    \centering
    \small
    \begin{tabular}{|c|c|c|rrrrrr|}
        \hline
        Model & Input & \multirow{2}{*}{Precision} &  \multicolumn{6}{c|}{mAP} \\
        \cline{4-9}
        (YOLOv8)                     & resolution                      &      & mAP   & IoU:50& IoU:75& small & medium& large \\
        \hline
        \hline
        \multirow{3}{*}{medium}     & \multirow{3}{*}{$640\times384$}  & FP32 & 0.603 & 0.805 & 0.695 & 0.316 & 0.606 & 0.770 \\
                                    &                                  & FP16 & 0.603 & 0.805 & 0.693 & 0.316 & 0.607 & 0.770 \\
                                    &                                  & INT8 & 0.471 & 0.672 & 0.556 & 0.164 & 0.476 & 0.663 \\
        \hline                                                           
        \multirow{3}{*}{small}      & \multirow{3}{*}{$640\times384$}  & FP32 & 0.578 & 0.786 & 0.660 & 0.273 & 0.585 & 0.744 \\
                                    &                                  & FP16 & 0.578 & 0.786 & 0.660 & 0.274 & 0.585 & 0.742 \\
                                    &                                  & INT8 & 0.544 & 0.763 & 0.625 & 0.245 & 0.549 & 0.703 \\
        \hline
        \multirow{3}{*}{MobileNetV2}& \multirow{3}{*}{$512\times288$}  & FP32 & 0.548 & 0.759 & 0.618 & 0.235 & 0.550 & 0.715 \\
                                    &                                  & FP16 & 0.548 & 0.759 & 0.619 & 0.236 & 0.549 & 0.718 \\
                                    &                                  & INT8 & 0.459 & 0.655 & 0.523 & 0.212 & 0.453 & 0.607 \\
        \hline                      
        \multirow{9}{*}{nano}       & \multirow{3}{*}{$640\times384$}  & FP32 & 0.530 & 0.749 & 0.608 & 0.229 & 0.529 & 0.685 \\
                                    &                                  & FP16 & 0.530 & 0.749 & 0.607 & 0.230 & 0.529 & 0.683 \\
                                    &                                  & INT8 & 0.430 & 0.650 & 0.484 & 0.182 & 0.453 & 0.530 \\
        \cline{2-9}
                                    & \multirow{3}{*}{$512\times288$}  & FP32 & 0.492 & 0.699 & 0.569 & 0.210 & 0.483 & 0.673 \\
                                    &                                  & FP16 & 0.492 & 0.698 & 0.569 & 0.209 & 0.482 & 0.669 \\
                                    &                                  & INT8 & 0.356 & 0.535 & 0.398 & 0.150 & 0.359 & 0.500 \\
        \cline{2-9}
                                    & \multirow{3}{*}{$448\times256$}  & FP32 & 0.511 & 0.723 & 0.592 & 0.206 & 0.508 & 0.681 \\
                                    &                                  & FP16 & 0.511 & 0.723 & 0.592 & 0.205 & 0.508 & 0.683 \\
                                    &                                  & INT8 & 0.372 & 0.555 & 0.417 & 0.140 & 0.371 & 0.516 \\
        \hline
        \multirow{9}{*}{pico}       & \multirow{3}{*}{$512\times288$}  & FP32 & 0.454 & 0.664 & 0.502 & 0.151 & 0.448 & 0.614 \\
                                    &                                  & FP16 & 0.454 & 0.664 & 0.500 & 0.151 & 0.448 & 0.612 \\
                                    &                                  & INT8 & 0.297 & 0.446 & 0.329 & 0.065 & 0.260 & 0.497 \\
        \cline{2-9}
                                    & \multirow{3}{*}{$448\times256$}  & FP32 & 0.415 & 0.621 & 0.468 & 0.136 & 0.408 & 0.552 \\
                                    &                                  & FP16 & 0.415 & 0.621 & 0.468 & 0.136 & 0.409 & 0.551 \\
                                    &                                  & INT8 & 0.356 & 0.552 & 0.396 & 0.122 & 0.355 & 0.474 \\
        \cline{2-9}
                                    & \multirow{3}{*}{$384\times224$}  & FP32 & 0.417 & 0.615 & 0.465 & 0.128 & 0.405 & 0.589 \\
                                    &                                  & FP16 & 0.418 & 0.615 & 0.465 & 0.128 & 0.405 & 0.588 \\
                                    &                                  & INT8 & 0.325 & 0.490 & 0.361 & 0.101 & 0.308 & 0.486 \\
        \hline
        \multirow{12}{*}{femto}     & \multirow{3}{*}{$512\times288$}  & FP32 & 0.318 & 0.488 & 0.351 & 0.072 & 0.298 & 0.416 \\
                                    &                                  & FP16 & 0.318 & 0.488 & 0.350 & 0.072 & 0.298 & 0.417 \\
                                    &                                  & INT8 & 0.300 & 0.466 & 0.326 & 0.071 & 0.270 & 0.420 \\
        \cline{2-9}
                                    & \multirow{3}{*}{$448\times256$}  & FP32 & 0.300 & 0.477 & 0.318 & 0.070 & 0.259 & 0.429 \\
                                    &                                  & FP16 & 0.300 & 0.477 & 0.319 & 0.070 & 0.259 & 0.430 \\
                                    &                                  & INT8 & 0.264 & 0.413 & 0.287 & 0.061 & 0.219 & 0.404 \\
        \cline{2-9}
                                    & \multirow{3}{*}{$384\times224$}  & FP32 & 0.281 & 0.452 & 0.302 & 0.070 & 0.263 & 0.386 \\
                                    &                                  & FP16 & 0.281 & 0.453 & 0.302 & 0.070 & 0.264 & 0.387 \\
                                    &                                  & INT8 & 0.235 & 0.409 & 0.242 & 0.059 & 0.211 & 0.331 \\
        \cline{2-9}
                                    & \multirow{3}{*}{$352\times192$}  & FP32 & 0.259 & 0.430 & 0.275 & 0.054 & 0.227 & 0.370 \\
                                    &                                  & FP16 & 0.259 & 0.430 & 0.275 & 0.054 & 0.226 & 0.370 \\
                                    &                                  & INT8 & 0.214 & 0.348 & 0.235 & 0.044 & 0.182 & 0.323 \\
        \hline
    \end{tabular}
    \normalsize
    \caption{Comparison of multiple mean Average Precision metrics for each
    trained model and input resolution, including models quantized to
    \texttt{FP16} and \texttt{INT8}.}
\end{table}







\end{appendices}

% For compilation piecewise (see projekt.tex), it is necessary to uncomment it
% \end{document}