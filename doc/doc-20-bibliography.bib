    
    @MISC{TODO,
      author = "john doe",
      title = "todo",
      publisher = "publisher",
      year = "1970"
    }

@article{MagneticSensors,
  author = {Bugdol, Marcin and Miodonska, Zuzanna and Krecichwost, Michal and KASPEREK, Paweł},
  title = {Vehicle detection system using magnetic sensors},
  year = {2014},
  month = {03},
  volume = {9},
  journal = {Transport Problems}
}

@article{UltrasonicSensors,
  author = {Roni Stiawan and Adhi Kusumadjati and Nina Siti Aminah and Mitra Djamal and Sparisoma Viridi},
  title = {An Ultrasonic Sensor System for Vehicle Detection Application},
  doi = {10.1088/1742-6596/1204/1/012017},
  url = {https://dx.doi.org/10.1088/1742-6596/1204/1/012017},
  year = {2019},
  month = {apr},
  publisher = {IOP Publishing},
  volume = {1204},
  number = {1},
  pages = {012017},
  journal = {Journal of Physics: Conference Series},
}

@INPROCEEDINGS{RadarSensors,
  author={Fang, Jianxin and Meng, Huadong and Zhang, Hao and Wang, Xiqin},
  title={A Low-cost Vehicle Detection and Classification System based on Unmodulated Continuous-wave Radar}, 
  booktitle={2007 IEEE Intelligent Transportation Systems Conference}, 
  year={2007},
  volume={},
  number={},
  pages={715-720},
  doi={10.1109/ITSC.2007.4357739}
}

@article{ImageProcessingOverview,
  author = {Wu, Kun and Xu, Tianmao and Zhang, Haiying and Song, Ju},
  title = {Overview of video-based vehicle detection technologies},
  year = {2011},
  month = {08},
  pages = {},
  journal = {ICCSE 2011 - 6th International Conference on Computer Science and Education, Final Program and Proceedings},
  doi = {10.1109/ICCSE.2011.6028764}
}

@article{ShadowRemoval,
  author = {Murali, Saritha and V K, Govindan},
  title = {Shadow Detection and Removal from a Single Image Using LAB Color Space},
  year = {2013},
  month = {03},
  pages = {},
  volume = {13},
  journal = {Cybernetics and Information Technologies},
  doi = {10.2478/cait-2013-0009}
}


@Article{OShea2015,
  author        = {O'Shea, Keiron and Nash, Ryan},
  title         = {An Introduction to Convolutional Neural Networks},
  year          = {2015},
  month         = nov,
  abstract      = {The field of machine learning has taken a dramatic twist in recent times, with the rise of the Artificial Neural Network (ANN). These biologically inspired computational models are able to far exceed the performance of previous forms of artificial intelligence in common machine learning tasks. One of the most impressive forms of ANN architecture is that of the Convolutional Neural Network (CNN). CNNs are primarily used to solve difficult image-driven pattern recognition tasks and with their precise yet simple architecture, offers a simplified method of getting started with ANNs. This document provides a brief introduction to CNNs, discussing recently published papers and newly formed techniques in developing these brilliantly fantastic image recognition models. This introduction assumes you are familiar with the fundamentals of ANNs and machine learning.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.1511.08458},
  eprint        = {1511.08458},
  file          = {:http\://arxiv.org/pdf/1511.08458v2:PDF},
  groups        = {Detectors and CNNs},
  keywords      = {Neural and Evolutionary Computing (cs.NE), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), FOS: Computer and information sciences},
  primaryclass  = {cs.NE},
  publisher     = {arXiv},
}


@Article{Li2022,
  author    = {Zewen Li and Fan Liu and Wenjie Yang and Shouheng Peng and Jun Zhou},
  journal   = {{IEEE} Transactions on Neural Networks and Learning Systems},
  title     = {A Survey of Convolutional Neural Networks: Analysis, Applications, and Prospects},
  year      = {2022},
  month     = {dec},
  number    = {12},
  pages     = {6999--7019},
  volume    = {33},
  doi       = {10.1109/tnnls.2021.3084827},
  groups    = {Detectors and CNNs},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
}




@Article{Lecun1998,
  author    = {Y. Lecun and L. Bottou and Y. Bengio and P. Haffner},
  journal   = {Proceedings of the {IEEE}},
  title     = {Gradient-based learning applied to document recognition},
  year      = {1998},
  number    = {11},
  pages     = {2278--2324},
  volume    = {86},
  doi       = {10.1109/5.726791},
  groups    = {Detectors and CNNs},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
}

@InProceedings{NIPS2012,
  author    = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {ImageNet Classification with Deep Convolutional Neural Networks},
  year      = {2012},
  editor    = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},
  publisher = {Curran Associates, Inc.},
  volume    = {25},
  groups    = {Detectors and CNNs},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf},
}

@Article{Simonyan2014,
  author        = {Simonyan, Karen and Zisserman, Andrew},
  title         = {Very Deep Convolutional Networks for Large-Scale Image Recognition},
  year          = {2014},
  month         = sep,
  abstract      = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.1409.1556},
  eprint        = {1409.1556},
  file          = {:http\://arxiv.org/pdf/1409.1556v6:PDF},
  groups        = {Detectors and CNNs},
  keywords      = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences},
  primaryclass  = {cs.CV},
  publisher     = {arXiv},
}

@Article{He2015,
  author        = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  title         = {Deep Residual Learning for Image Recognition},
  year          = {2015},
  month         = dec,
  abstract      = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.1512.03385},
  eprint        = {1512.03385},
  file          = {:http\://arxiv.org/pdf/1512.03385v1:PDF},
  groups        = {Detectors and CNNs},
  keywords      = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences},
  primaryclass  = {cs.CV},
  publisher     = {arXiv},
}

@Article{Howard2017,
  author        = {Howard, Andrew G. and Zhu, Menglong and Chen, Bo and Kalenichenko, Dmitry and Wang, Weijun and Weyand, Tobias and Andreetto, Marco and Adam, Hartwig},
  title         = {MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications},
  year          = {2017},
  month         = apr,
  abstract      = {We present a class of efficient models called MobileNets for mobile and embedded vision applications. MobileNets are based on a streamlined architecture that uses depth-wise separable convolutions to build light weight deep neural networks. We introduce two simple global hyper-parameters that efficiently trade off between latency and accuracy. These hyper-parameters allow the model builder to choose the right sized model for their application based on the constraints of the problem. We present extensive experiments on resource and accuracy tradeoffs and show strong performance compared to other popular models on ImageNet classification. We then demonstrate the effectiveness of MobileNets across a wide range of applications and use cases including object detection, finegrain classification, face attributes and large scale geo-localization.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.1704.04861},
  eprint        = {1704.04861},
  file          = {:http\://arxiv.org/pdf/1704.04861v1:PDF},
  groups        = {Detectors and CNNs},
  keywords      = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences},
  primaryclass  = {cs.CV},
  publisher     = {arXiv},
}

@Article{Girshick2013,
  author        = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
  title         = {Rich feature hierarchies for accurate object detection and semantic segmentation},
  year          = {2013},
  month         = nov,
  abstract      = {Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30% relative to the previous best result on VOC 2012---achieving a mAP of 53.3%. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. We also compare R-CNN to OverFeat, a recently proposed sliding-window detector based on a similar CNN architecture. We find that R-CNN outperforms OverFeat by a large margin on the 200-class ILSVRC2013 detection dataset. Source code for the complete system is available at http://www.cs.berkeley.edu/~rbg/rcnn.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.1311.2524},
  eprint        = {1311.2524},
  file          = {:http\://arxiv.org/pdf/1311.2524v5:PDF},
  groups        = {Detectors and CNNs},
  keywords      = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences},
  primaryclass  = {cs.CV},
  publisher     = {arXiv},
}

@Article{Choudhary2020,
  author    = {Tejalal Choudhary and Vipul Mishra and Anurag Goswami and Jagannathan Sarangapani},
  journal   = {Artificial Intelligence Review},
  title     = {A comprehensive survey on model compression and acceleration},
  year      = {2020},
  month     = {feb},
  number    = {7},
  pages     = {5113--5155},
  volume    = {53},
  doi       = {10.1007/s10462-020-09816-7},
  groups    = {Optimization},
  publisher = {Springer Science and Business Media {LLC}},
}

@Article{Tang2022,
  author        = {Tang, Chen and Ouyang, Kai and Wang, Zhi and Zhu, Yifei and Wang, Yaowei and Ji, Wen and Zhu, Wenwu},
  title         = {Mixed-Precision Neural Network Quantization via Learned Layer-wise Importance},
  year          = {2022},
  month         = mar,
  abstract      = {The exponentially large discrete search space in mixed-precision quantization (MPQ) makes it hard to determine the optimal bit-width for each layer. Previous works usually resort to iterative search methods on the training set, which consume hundreds or even thousands of GPU-hours. In this study, we reveal that some unique learnable parameters in quantization, namely the scale factors in the quantizer, can serve as importance indicators of a layer, reflecting the contribution of that layer to the final accuracy at certain bit-widths. These importance indicators naturally perceive the numerical transformation during quantization-aware training, which can precisely provide quantization sensitivity metrics of layers. However, a deep network always contains hundreds of such indicators, and training them one by one would lead to an excessive time cost. To overcome this issue, we propose a joint training scheme that can obtain all indicators at once. It considerably speeds up the indicators training process by parallelizing the original sequential training processes. With these learned importance indicators, we formulate the MPQ search problem as a one-time integer linear programming (ILP) problem. That avoids the iterative search and significantly reduces search time without limiting the bit-width search space. For example, MPQ search on ResNet18 with our indicators takes only 0.06 s, which improves time efficiency exponentially compared to iterative search methods. Also, extensive experiments show our approach can achieve SOTA accuracy on ImageNet for far-ranging models with various constraints (e.g., BitOps, compress rate). Code is available on https://github.com/1hunters/LIMPQ.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.2203.08368},
  eprint        = {2203.08368},
  file          = {:http\://arxiv.org/pdf/2203.08368v5:PDF},
  groups        = {Optimization},
  keywords      = {Machine Learning (cs.LG), Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences},
  primaryclass  = {cs.LG},
  publisher     = {arXiv},
}

@misc{mAP,
  title = {Mean Average Precision ({mAP}) Explained: Everything You Need to Know},
  author = {Shah, Deval},
  year = {2022},
  url = {https://www.v7labs.com/blog/mean-average-precision},
  howpublished = {Online},
}

@Article{Wang2019,
  author     = {Hai Wang and YIjie Yu and Yingfeng Cai and Xiaobo Chen and Long Chen and Qingchao Liu},
  journal    = {{IEEE} Intelligent Transportation Systems Magazine},
  title      = {A Comparative Study of State-of-the-Art Deep Learning Algorithms for Vehicle Detection},
  year       = {2019},
  number     = {2},
  pages      = {82--95},
  volume     = {11},
  comment    = {porovnáva rcnn, rfc, ssd, yolov3 a retinanet na kitti.},
  doi        = {10.1109/mits.2019.2903518},
  groups     = {Vehicle detection},
  priority   = {prio1},
  publisher  = {Institute of Electrical and Electronics Engineers ({IEEE})},
  ranking    = {rank3},
  readstatus = {read},
}



@InProceedings{Maity2021,
  author    = {Madhusri Maity and Sriparna Banerjee and Sheli Sinha Chaudhuri},
  booktitle = {2021 5th International Conference on Computing Methodologies and Communication ({ICCMC})},
  title     = {Faster R-{CNN} and {YOLO} based Vehicle detection: A Survey},
  year      = {2021},
  month     = {apr},
  publisher = {{IEEE}},
  doi       = {10.1109/iccmc51019.2021.9418274},
  groups    = {Vehicle detection},
  priority  = {prio2},
}

@InProceedings{Wu2021,
  author    = {Tian-Hao Wu and Tong-Wen Wang and Ya-Qi Liu},
  booktitle = {2021 3rd World Symposium on Artificial Intelligence ({WSAI})},
  title     = {Real-Time Vehicle and Distance Detection Based on Improved Yolo v5 Network},
  year      = {2021},
  month     = {jun},
  publisher = {{IEEE}},
  doi       = {10.1109/wsai51899.2021.9486316},
  groups    = {Vehicle detection, Light-weight detectors},
  priority  = {prio2},
  ranking   = {rank3},
}






@misc{LabelBox,
  title = {Labelbox},
  author = {Labelbox},
  year = {2022},
  url = {https://labelbox.com},
  howpublished = {Online},
}

@article{PyTorch,
  title={Automatic differentiation in PyTorch},
  author={Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and DeVito, Zachary and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Lerer, Adam},
  year={2017}
}

@misc{MMDetection,
  author  = {MMDetection Contributors},
  comment = {If you use this software, please cite it as below.},
  license = {Apache-2.0},
  month   = aug,
  title   = {OpenMMLab Detection Toolbox and Benchmark},
  url     = {https://github.com/open-mmlab/mmdetection},
  year    = {2018},
}

@misc{MMDeploy,
  author  = {MMDeploy Contributors},
  comment = {If you use this software, please cite it as below.},
  license = {Apache-2.0},
  month   = dec,
  title   = {OpenMMLab's Model deployment toolbox},
  url     = {https://github.com/open-mmlab/mmdeploy},
  year    = {2021},
}

@Misc{MMYOLO,
  author       = {MMYOLO Contributors},
  howpublished = {\url{https://github.com/open-mmlab/mmyolo}},
  title        = {{MMYOLO: OpenMMLab YOLO} series toolbox and benchmark},
  year         = {2022},
}

@misc{ONNX,
  title={ONNX: Open Neural Network Exchange},
  author={Bai, Junjie and Lu, Fang and Zhang, Ke and others},
  year={2019},
  howpublished={\url{https://github.com/onnx/onnx}},
}

@misc{ONNXRuntime,
  title={ONNX Runtime},
  author={ONNX Runtime developers},
  year={2021},
  howpublished={\url{https://onnxruntime.ai/}},
}









@article{detrac,
  author        = {Wen, Longyin and Du, Dawei and Cai, Zhaowei and Lei, Zhen and Chang, Ming-Ching and Qi, Honggang and Lim, Jongwoo and Yang, Ming-Hsuan and Lyu, Siwei},
  title         = {UA-DETRAC: A New Benchmark and Protocol for Multi-Object Detection and Tracking},
  year          = {2015},
  month         = nov,
  abstract      = {In recent years, numerous effective multi-object tracking (MOT) methods are developed because of the wide range of applications. Existing performance evaluations of MOT methods usually separate the object tracking step from the object detection step by using the same fixed object detection results for comparisons. In this work, we perform a comprehensive quantitative study on the effects of object detection accuracy to the overall MOT performance, using the new large-scale University at Albany DETection and tRACking (UA-DETRAC) benchmark dataset. The UA-DETRAC benchmark dataset consists of 100 challenging video sequences captured from real-world traffic scenes (over 140,000 frames with rich annotations, including occlusion, weather, vehicle category, truncation, and vehicle bounding boxes) for object detection, object tracking and MOT system. We evaluate complete MOT systems constructed from combinations of state-of-the-art object detection and object tracking methods. Our analysis shows the complex effects of object detection accuracy on MOT system performance. Based on these observations, we propose new evaluation tools and metrics for MOT systems that consider both object detection and object tracking for comprehensive analysis.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.1511.04136},
  eprint        = {1511.04136},
  file          = {:http\://arxiv.org/pdf/1511.04136v4:PDF},
  groups        = {Datasets},
  keywords      = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences},
  primaryclass  = {cs.CV},
  publisher     = {arXiv},
}
@Article{MIO2018,
  journal   = {{IEEE} Transactions on Image Processing},
  title     = {{MIO}-{TCD}: A New Benchmark Dataset for Vehicle Classification and Localization},
  year      = {2018},
  month     = {oct},
  number    = {10},
  pages     = {5129--5141},
  volume    = {27},
  doi       = {10.1109/tip.2018.2848705},
  groups    = {Datasets},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
}
@Article{Bahnsen2019,
  author    = {Chris H. Bahnsen and Thomas B. Moeslund},
  journal   = {{IEEE} Transactions on Intelligent Transportation Systems},
  title     = {Rain Removal in Traffic Surveillance: Does it Matter?},
  year      = {2019},
  month     = {aug},
  number    = {8},
  pages     = {2802--2819},
  volume    = {20},
  doi       = {10.1109/tits.2018.2872502},
  groups    = {Datasets},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
}
@InProceedings{Jensen2020,
  author    = {Morten B. Jensen and Andreas Mogelmose and Thomas B. Moeslund},
  booktitle = {2020 {IEEE} 23rd International Conference on Intelligent Transportation Systems ({ITSC})},
  title     = {Presenting the Multi-view Traffic Intersection Dataset ({MTID}): A Detailed Traffic-Surveillance Dataset},
  year      = {2020},
  month     = {sep},
  publisher = {{IEEE}},
  doi       = {10.1109/itsc45102.2020.9294694},
  groups    = {Datasets},
}
@Misc{Luca2022,
  author    = {Luca, Ciampi and Carlos, Santiago and Paulo, Costeira Joao and Claudio, Gennaro and Giuseppe, Amato},
  title     = {Night and Day Instance Segmented Park (NDISPark) Dataset: a Collection of Images taken by Day and by Night for Vehicle Detection, Segmentation and Counting in Parking Areas},
  year      = {2022},
  copyright = {Open Data Commons Attribution License v1.0},
  doi       = {10.5281/ZENODO.6560822},
  groups    = {Datasets},
  keywords  = {object detection, vehicle detection, vehicle segmentation, vehicle counting, domain shift},
  language  = {en},
  publisher = {Zenodo},
}
@Article{Zhu2022,
  author    = {Pengfei Zhu and Longyin Wen and Dawei Du and Xiao Bian and Heng Fan and Qinghua Hu and Haibin Ling},
  journal   = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence},
  title     = {Detection and Tracking Meet Drones Challenge},
  year      = {2022},
  month     = {nov},
  number    = {11},
  pages     = {7380--7399},
  volume    = {44},
  doi       = {10.1109/tpami.2021.3119563},
  groups    = {Datasets},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
}
@InCollection{LargeBatch,
  author    = {Tong Wang and Yousong Zhu and Chaoyang Zhao and Wei Zeng and Yaowei Wang and Jinqiao Wang and Ming Tang},
  booktitle = {Computer Vision {\textendash} {ECCV} 2020},
  publisher = {Springer International Publishing},
  title     = {Large Batch Optimization for Object Detection: Training {COCO} in 12~minutes},
  year      = {2020},
  pages     = {481--496},
  doi       = {10.1007/978-3-030-58589-1\_29},
  groups    = {Datasets},
}




@Article{Albumentations,
  author  = {A. Buslaev and A. Parinov and E. Khvedchenya and V.~I. Iglovikov and A.~A. Kalinin},
  journal = {ArXiv e-prints},
  title   = {Albumentations: fast and flexible image augmentations},
  year    = {2018},
  eprint  = {1809.06839},
}





    
    @BOOK{Pravidla,
      author =       "Zdeněk Hlavsa and others",
      title =        "Pravidla českého pravopisu",
      publisher =    "Academia",
      edition =      "2",
      year =         "2009",
      isbn =         "80-200-1327-X",
    }
    
    @BOOK{Knuth,
      author =       "Donald E. Knuth",
      title =        "The {\TeX}book",
      publisher =    "Addison-Wesley Publishing Company",
      year =         "1996",
      isbn =         "0-201-13447-0",
    }
    
    @BOOK{Rybicka,
      author =       "Jiří Rybička",
      title =        "\LaTeX~pro začátečníky",
      publisher =    "Konvoj",
      edition =      "3",
      year =         "2003",
      isbn =         "80-7302-049-1",
    }

    @BOOK{Olsak,
      author =       "Petr Olšák",
      title =        "{\TeX}book naruby",
      publisher =    "Konvoj",
      year =         "2001",
      isbn =         "80-7302-0007-6",
    }

    @BOOK{Eikh,
      author =       "Victor Eikhout",
      title =        "{\TeX}by Topics, a {\TeX}nitian's Reference",
      publisher =    "Addison-Wesley UK",
      year =         "2001",
      isbn =         "0-201-56882-9",
    }

    @website{cstugWeb,
      author =       "Petr Sojka and others",
      title =        "Československé sdružení uživatelů {\TeX}u",
      howpublished = "online",
      url =          "http://www.cstug.cz",
      cited =        "2019-10-02"
    }

    @webpage{fitWeb,
      author =       "Zdena Rábová and Petr Hanáček and Petr Peringer and Petr Přikryl and Bohuslav Křena",
      title =        "Užitečné rady pro psaní práce",
      howpublished = "online",
      publisher =    "FIT VUT v~Brně",
      year =         "2008",
      month =        11,
      url =          "https://www.fit.vut.cz/study/theses/theses-advice/",
      cited =        "2019-10-02"
    }

    @webpage{citace,
      author =       "Zdena Rábová and Petr Hanáček and Petr Peringer and Petr Přikryl and Bohuslav Křena",
      title =        "Pravidla pro bibliografické citace",
      howpublished = "online",
      publisher =    "FIT VUT v Brně",
      year =         "2019",
      month =        3,
      url =          "https://www.fit.vut.cz/study/theses/citations/",
      cited =        "2019-10-02"
    }
   
    @MISC{biblio,
      author =       "Olga Biernátová and Jan Skůpa",
      title =        "Bibliografické odkazy a citace dokumentů",
      howpublished = "online",
      url =          "http://www.citace.com/download/CSN-ISO-690.pdf",
      publisher =    "Citace.com",
      month =        9,
      address =      "Brno",
      year =         "2011",
      cited =        "2019-10-02"
    }
   
    @webpage{formalniBP,
      author =       "Zdena Rábová and Petr Přikryl and Petr Peringer and Petr Lampa and Bohuslav Křena and jaroslav Dytrych and Petr Veigend",
      title =        "Pokyny k~bakalářským pracím a SZZ",
      howpublished = "online",
      year =         "2019",
      url =          "https://www.fit.vut.cz/study/theses/bachelor-theses/",
      cited =        "2019-10-02"
    }
   
    @webpage{formalniDP,
      author =       "Zdena Rábová and Petr Přikryl and Petr Peringer and Petr Lampa and Bohuslav Křena and jaroslav Dytrych and Petr Veigend",
      title =        "Pokyny k~diplomovým pracím a SZZ",
      howpublished = "online",
      year =         "2019",
      url =          "https://www.fit.vut.cz/study/theses/master-theses/",
      cited =        "2019-10-02"
    }
   
    @MISC{smernice,
      key =          "Szz",
      author =       "Petr Štěpánek",
      title =        "Směrnice rektora č. 72/2017 -- Úprava, odevzdávání a zveřejňování závěrečných prací",
      howpublished = "online",
      year =         "2017",
      url =          "https://www.vutbr.cz/uredni-deska/vnitrni-predpisy-a-dokumenty/smernice-c-72-2017-uprava-odevzdavani-a-zverejnovani-zaverecnych-praci-d161410",
      cited =        "2019-10-02"
    }
   
   @MISC{smerniceFIT,
      author =       "Pavel Zemčík",
      title =        "Směrnice děkana č. 7/2018 -- Úprava, odevzdávání a zveřejňování závěrečných prací na FIT VUT v Brně",
      howpublished = "online",
      year =         "2018",
      url =          "https://www.fit.vut.cz/fit/info/smernice/sm2018-07.pdf",
      cited =        "2019-10-02"
    }
   
    @website{prirucka,
      key =          "Cerna",
      author =       "Anna Černá and Jan Chromý and Hana Konečná and others",
      title =        "Internetová jazyková příručka -- Ústav pro jazyk český Akademie věd ČR, v. v. i.",
      howpublished = "online",
      publisher =    "Centrum zpracování přirozeného jazyka FI MU",
      year =         "2019",
      url =          "http://prirucka.ujc.cas.cz/",
      cited =        "2019-10-02"
    }
   
    @website{Herout,
      author =       "Adam Herout",
      title =        {herout.net -- Poznámky učitele, kouče, čtenáře.},
      howpublished = "online",
      year =         "2018",
      url =          "http://www.herout.net/",
      cited =        "2019-10-02"
    }
   
    @webpage{chyby,
      key =          "Szoke",
      author =       "Igor Szöke",
      title =        "Textová část BP/DP -- Lessons learned",
      howpublished = "online",
      year =         "2012",
      url =          "http://blog.igor.szoke.cz/2012/01/textova-cast-bpdp-lessons-learned-1.html",
      cited =        "2019-10-02"
    }
   
    @webpage{rady,
      key =          "Szoke",
      author =       "Igor Szöke",
      title =        "Píšete diplomku? 15+1 rad, které vám pomohou.",
      howpublished = "online",
      year =         "2011",
      url =          "http://blog.igor.szoke.cz/2011/12/pisete-diplomku-151-rad-ktere-vam.html",
      cited =        "2019-10-02"
    }
   
    @webpage{Beran,
      author =       "Vítězslav Beran",
      title =        "Beran-BP DP Projekty",
      howpublished = "online",
      year =         "2017",
      url =          "http://merlin.fit.vutbr.cz/wiki/index.php/Beran-BP_DP_Projekty",
      cited =        "2019-10-02"
    }

    @MISC{BeranPDF,
      author =       "Vítězslav Beran",
      title =        "Jak psát technickou zprávu",
      howpublished = "online",
      year =         "2013",
      url =          "http://www.fit.vutbr.cz/~beranv/podpora/Jak\%20psat\%20technickou\%20zpravu.pdf",
      cited =        "2019-10-00"
    }
   
    @webpage{Cernocky,
      key =          "Cernocky",
      author =       "Jan Černocký",
      title =        "Černocký BP DP -- obecně",
      howpublished = "online",
      year =         "2016",
      url =          "https://merlin.fit.vutbr.cz/wiki/index.php/Cernocky_BP_DP-obecne",
      publisher = "Wiki FIT VUT v Brně",
      cited =        "2019-10-02"
    }
   
    @webpage{CernockyEnglish,
      key =          "Cernocky",
      author =       "Jan Černocký",
      title =        "English SOS",
      howpublished = "online",
      year =         "2016",
      url =          "https://merlin.fit.vutbr.cz/wiki/index.php/English_SOS",
      cited =        "2019-10-02"
    }
   
    @MISC{Zemcik,
      author =       "Pavel Zemčík",
      title =        "Všeobecné pokyny pro studenty a vůbec",
      howpublished = "online",
      year =         "2012",
      url =          "http://www.fit.vutbr.cz/~zemcik/Pokyny.pdf",
      cited =        "2019-10-02"
    }
   
    @BOOK{Lebrun2011,
      author =       "Jean-Luc Lebrun",
      title =        "Scientific Writing 2.0: a reader and writer's guide",
      publisher =    "World Scientific Publishing",
      year =         "2011",
      edition =      "1",
      isbn =         "9814350605",
      owner =        "herout",
      timestamp =    "2015.01.20"
    }
  
    @MASTERSTHESIS{Pysny,
      author = {Radek Py{\v{s}}n{\'{y}}},
      type = {Bakal{\'{a}}{\v{r}}sk{\'{a}} pr{\'{a}}ce},
      title = {BiBTeX styl pro {\v{C}}SN ISO 690 a {\v{C}}SN ISO	690-2},
      school = {Vysok{\'{e}} u{\v{c}}en{\'{i}} technick{\'{e}} v Brn{\v{e}}, Fakulta informa{\v{c}}n{\'{i}}ch technologi{\'{i}}},
      year = {2009},
      location = {Brno, CZ},
      language = {czech},
      url = {https://www.fit.vut.cz/study/thesis/7848/}
    }
